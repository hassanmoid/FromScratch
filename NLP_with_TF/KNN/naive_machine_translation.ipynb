{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "\n",
    "from utils import (cosine_similarity, get_dict,\n",
    "                   process_tweet)\n",
    "from os import getcwd\n",
    "\n",
    "import unittest as w4_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "filePath = f\"{getcwd()}/tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('./data/en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./data/en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torpedo': 'torpilles',\n",
       " 'giovanni': 'giovanni',\n",
       " 'chat': 'chat',\n",
       " 'catholics': 'catholiques',\n",
       " 'herald': 'herald',\n",
       " 'chuck': 'chuck',\n",
       " 'pit': 'fosse',\n",
       " 'supplied': 'fournie',\n",
       " 'optional': 'facultatives',\n",
       " 'garrison': 'garnison',\n",
       " 'sprint': 'sprint',\n",
       " 'exile': 'exilés',\n",
       " 'surprised': 'étonnée',\n",
       " 'achievements': 'réalisations',\n",
       " 'biblical': 'bibliques',\n",
       " 'rebels': 'rebelles',\n",
       " 'denis': 'denis',\n",
       " 'geographical': 'géographique',\n",
       " 'sit': 'sit',\n",
       " 'alpine': 'alpine',\n",
       " 'bills': 'factures',\n",
       " 'glacier': 'glaciers',\n",
       " 'binding': 'reliure',\n",
       " 'indicating': 'indiquant',\n",
       " 'estonia': 'estonie',\n",
       " 'eating': 'manger',\n",
       " 'saving': 'économiser',\n",
       " 'chi': 'chi',\n",
       " 'developer': 'développeurs',\n",
       " 'indie': 'indie',\n",
       " 'difficulties': 'difficultés',\n",
       " 'doctrine': 'doctrine',\n",
       " 'worn': 'porté',\n",
       " 'fork': 'fourches',\n",
       " 'simpson': 'simpson',\n",
       " 'maintaining': 'maintenir',\n",
       " 'theological': 'théologique',\n",
       " 'upcoming': 'prochaines',\n",
       " 'temporarily': 'momentanément',\n",
       " 'hotels': 'hôtels',\n",
       " 'edmonton': 'edmonton',\n",
       " 'developments': 'développements',\n",
       " 'literacy': 'alphabétisation',\n",
       " 'currency': 'devises',\n",
       " 'missionary': 'missionnaires',\n",
       " 'arrives': 'arrive',\n",
       " 'hammer': 'maillet',\n",
       " 'dollar': 'dollar',\n",
       " 'ambassadors': 'ambassadeurs',\n",
       " 'twitter': 'twitter',\n",
       " 'centres': 'centres',\n",
       " 'solomon': 'solomon',\n",
       " 'recommend': 'recommande',\n",
       " 'descendants': 'descendants',\n",
       " 'ruth': 'ruth',\n",
       " 'handling': 'manutention',\n",
       " 'customs': 'douanier',\n",
       " 'collect': 'collectionner',\n",
       " 'grid': 'grille',\n",
       " 'secured': 'sécurisé',\n",
       " 'certificate': 'certificat',\n",
       " 'destination': 'destination',\n",
       " 'albania': 'albanie',\n",
       " 'euro': 'euro',\n",
       " 'consumption': 'consommation',\n",
       " 'feat': 'prouesse',\n",
       " 'pushing': 'poussant',\n",
       " 'constantly': 'constamment',\n",
       " 'survivors': 'survivant',\n",
       " 'mansion': 'manoir',\n",
       " 'cardiff': 'cardiff',\n",
       " 'temples': 'temples',\n",
       " 'blake': 'blake',\n",
       " 'sheet': 'feuillet',\n",
       " 'lift': 'élévateur',\n",
       " 'confidence': 'confiance',\n",
       " 'cuisine': 'gastronomie',\n",
       " 'frankfurt': 'francfort',\n",
       " 'galaxy': 'galaxy',\n",
       " 'ecuador': 'equateur',\n",
       " 'breeding': 'reproduction',\n",
       " 'outbreak': 'éclosion',\n",
       " 'legendary': 'mythique',\n",
       " 'handball': 'handball',\n",
       " 'georgian': 'géorgienne',\n",
       " 'copenhagen': 'copenhague',\n",
       " 'trek': 'trek',\n",
       " 'ignored': 'ignorés',\n",
       " 'arch': 'arch',\n",
       " 'keys': 'clefs',\n",
       " 'proceedings': 'procédures',\n",
       " 'enjoy': 'profitez',\n",
       " 'quartet': 'quatuor',\n",
       " 'aims': 'finalités',\n",
       " 'propaganda': 'propagande',\n",
       " 'disk': 'disquette',\n",
       " 'realized': 'réalisé',\n",
       " 'neat': 'soigné',\n",
       " 'funny': 'amusant',\n",
       " 'punishment': 'sanction',\n",
       " 'accuracy': 'justesse',\n",
       " 'meter': 'mètre',\n",
       " 'theoretical': 'théorique',\n",
       " 'suspension': 'suspensions',\n",
       " 'seeds': 'graines',\n",
       " 'lighting': 'luminaires',\n",
       " 'jennifer': 'jennifer',\n",
       " 'smooth': 'lisse',\n",
       " 'customer': 'client',\n",
       " 'armstrong': 'armstrong',\n",
       " 'involve': 'impliquer',\n",
       " 'philosophical': 'philosophiques',\n",
       " 'escaped': 'échappés',\n",
       " 'powell': 'powell',\n",
       " 'kills': 'tue',\n",
       " 'taste': 'goût',\n",
       " 'allmusic': 'allmusic',\n",
       " 'requiring': 'nécessitant',\n",
       " 'bros': 'bros',\n",
       " 'assertion': 'assertion',\n",
       " 'boulevard': 'boulevard',\n",
       " 'brooks': 'brooks',\n",
       " 'sending': 'envoi',\n",
       " 'atomic': 'atomique',\n",
       " 'antarctica': 'antarctique',\n",
       " 'strikes': 'grèves',\n",
       " 'reconstruction': 'reconstruction',\n",
       " 'chronicle': 'chronique',\n",
       " 'traveling': 'voyageant',\n",
       " 'leslie': 'leslie',\n",
       " 'ellis': 'ellis',\n",
       " 'devon': 'devon',\n",
       " 'ghana': 'ghana',\n",
       " 'gen': 'gen',\n",
       " 'rebel': 'rebelle',\n",
       " 'duncan': 'duncan',\n",
       " 'pianist': 'pianiste',\n",
       " 'canon': 'canon',\n",
       " 'reformed': 'réformée',\n",
       " 'pack': 'pack',\n",
       " 'iceland': 'islande',\n",
       " 'solve': 'résout',\n",
       " 'cyclists': 'cyclistes',\n",
       " 'payment': 'paiement',\n",
       " 'suburbs': 'banlieues',\n",
       " 'militia': 'milices',\n",
       " 'pronounced': 'prononcés',\n",
       " 'exhibit': 'exposition',\n",
       " 'mph': 'mph',\n",
       " 'glen': 'vallon',\n",
       " 'eugene': 'eugene',\n",
       " 'compromise': 'compromettre',\n",
       " 'tactical': 'tactiques',\n",
       " 'discovers': 'découvre',\n",
       " 'switched': 'basculé',\n",
       " 'uganda': 'ouganda',\n",
       " 'jail': 'prison',\n",
       " 'yeah': 'ouai',\n",
       " 'withdraw': 'retirez',\n",
       " 'holmes': 'holmes',\n",
       " 'promise': 'promets',\n",
       " 'convert': 'convertit',\n",
       " 'dos': 'dos',\n",
       " 'noting': 'constatant',\n",
       " 'recall': 'rappel',\n",
       " 'arrive': 'arrivez',\n",
       " 'warrior': 'guerrier',\n",
       " 'mammals': 'mammifère',\n",
       " 'dimensions': 'dimension',\n",
       " 'surrey': 'surrey',\n",
       " 'gaming': 'jeu',\n",
       " 'lutheran': 'luthérienne',\n",
       " 'ports': 'ports',\n",
       " 'amy': 'amy',\n",
       " 'survival': 'survivre',\n",
       " 'responses': 'réponses',\n",
       " 'collegiate': 'collégial',\n",
       " 'scandal': 'scandale',\n",
       " 'widow': 'veuf',\n",
       " 'swing': 'swing',\n",
       " 'nights': 'soirs',\n",
       " 'polo': 'polo',\n",
       " 'linda': 'linda',\n",
       " 'adr': 'adr',\n",
       " 'probability': 'vraisemblance',\n",
       " 'farms': 'exploitations',\n",
       " 'conferences': 'colloques',\n",
       " 'zhang': 'zhang',\n",
       " 'crazy': 'folle',\n",
       " 'witness': 'témoin',\n",
       " 'nephew': 'neveu',\n",
       " 'sensitive': 'sensibles',\n",
       " 'mutual': 'mutuelles',\n",
       " 'diet': 'régime',\n",
       " 'clients': 'clients',\n",
       " 'fringe': 'franges',\n",
       " 'passion': 'passion',\n",
       " 'rings': 'anneaux',\n",
       " 'millions': 'millions',\n",
       " 'dialect': 'dialecte',\n",
       " 'orlando': 'orlando',\n",
       " 'relay': 'relais',\n",
       " 'wet': 'humides',\n",
       " 'cruise': 'croisières',\n",
       " 'henri': 'henri',\n",
       " 'publish': 'publiez',\n",
       " 'joy': 'joie',\n",
       " 'julia': 'julia',\n",
       " 'kitchen': 'cuisine',\n",
       " 'abstract': 'résumé',\n",
       " 'snake': 'serpent',\n",
       " 'comedian': 'humoriste',\n",
       " 'motorcycle': 'moto',\n",
       " 'nadu': 'nadu',\n",
       " 'arsenal': 'arsenal',\n",
       " 'millennium': 'millennium',\n",
       " 'assists': 'assiste',\n",
       " 'bow': 'bow',\n",
       " 'andré': 'andré',\n",
       " 'serie': 'serie',\n",
       " 'dimensional': 'dimensionnelle',\n",
       " 'travelled': 'parcouru',\n",
       " 'eurovision': 'eurovision',\n",
       " 'suite': 'suite',\n",
       " 'doug': 'doug',\n",
       " 'gravity': 'gravité',\n",
       " 'stored': 'stockées',\n",
       " 'departed': 'défunts',\n",
       " 'optical': 'optiques',\n",
       " 'frontier': 'frontier',\n",
       " 'evaluation': 'evaluation',\n",
       " 'graph': 'graphe',\n",
       " 'hybrid': 'hybride',\n",
       " 'oslo': 'oslo',\n",
       " 'earn': 'mériter',\n",
       " 'metre': 'mètre',\n",
       " 'keyboard': 'claviers',\n",
       " 'jamie': 'jamie',\n",
       " 'decorated': 'décoré',\n",
       " 'complicated': 'compliquée',\n",
       " 'nathan': 'nathan',\n",
       " 'slavery': 'esclavage',\n",
       " 'circular': 'circulaires',\n",
       " 'operators': 'opérateurs',\n",
       " 'armor': 'armure',\n",
       " 'mechanics': 'mécaniques',\n",
       " 'bradford': 'bradford',\n",
       " 'leon': 'leon',\n",
       " 'rachel': 'rachel',\n",
       " 'strings': 'cordes',\n",
       " 'header': 'entête',\n",
       " 'hood': 'capuchon',\n",
       " 'inspector': 'inspecteur',\n",
       " 'warnings': 'avertissements',\n",
       " 'plains': 'plaines',\n",
       " 'defended': 'défendue',\n",
       " 'wheels': 'roues',\n",
       " 'criterion': 'critère',\n",
       " 'ace': 'ace',\n",
       " 'arrangements': 'arrangements',\n",
       " 'penn': 'penn',\n",
       " 'approached': 'approché',\n",
       " 'joke': 'plaisanterie',\n",
       " 'sailed': 'navigué',\n",
       " 'religions': 'cultes',\n",
       " 'grants': 'subventions',\n",
       " 'andrews': 'andrews',\n",
       " 'moderate': 'modérés',\n",
       " 'stolen': 'dérobée',\n",
       " 'tributary': 'affluent',\n",
       " 'pin': 'épingler',\n",
       " 'carol': 'carole',\n",
       " 'owns': 'possède',\n",
       " 'prototype': 'prototypes',\n",
       " 'copied': 'copiée',\n",
       " 'canterbury': 'canterbury',\n",
       " 'midnight': 'midnight',\n",
       " 'quarterback': 'quaterback',\n",
       " 'duchy': 'duché',\n",
       " 'bailey': 'bailey',\n",
       " 'arbitrators': 'arbitres',\n",
       " 'performers': 'interprètes',\n",
       " 'handled': 'manipulées',\n",
       " 'exploration': 'exploration',\n",
       " 'diversity': 'diversité',\n",
       " 'sixteen': 'seize',\n",
       " 'findings': 'conclusions',\n",
       " 'repeat': 'répétez',\n",
       " 'brussels': 'bruxelles',\n",
       " 'planets': 'planètes',\n",
       " 'theatrical': 'théâtral',\n",
       " 'reconnaissance': 'reconnaissance',\n",
       " 'shots': 'coups',\n",
       " 'complaint': 'plaintes',\n",
       " 'batman': 'batman',\n",
       " 'exhibited': 'exposées',\n",
       " 'espn': 'espn',\n",
       " 'investigate': 'enquêter',\n",
       " 'verify': 'vérifiez',\n",
       " 'discontinued': 'interrompu',\n",
       " 'absent': 'absente',\n",
       " 'girlfriend': 'copine',\n",
       " 'resignation': 'démission',\n",
       " 'fossil': 'fossiles',\n",
       " 'explaining': 'expliquer',\n",
       " 'tang': 'tang',\n",
       " 'inches': 'centimètres',\n",
       " 'proven': 'prouvé',\n",
       " 'franco': 'franco',\n",
       " 'dying': 'mourant',\n",
       " 'tribal': 'tribale',\n",
       " 'tyler': 'tyler',\n",
       " 'surrender': 'reddition',\n",
       " 'glenn': 'glenn',\n",
       " 'substance': 'substance',\n",
       " 'focusing': 'focalisation',\n",
       " 'luxembourg': 'luxembourg',\n",
       " 'colored': 'colorée',\n",
       " 'scholarly': 'érudit',\n",
       " 'administered': 'administré',\n",
       " 'explosion': 'explosions',\n",
       " 'pushed': 'poussés',\n",
       " 'generations': 'générations',\n",
       " 'duck': 'duck',\n",
       " 'porter': 'porter',\n",
       " 'permanently': 'définitivement',\n",
       " 'memphis': 'memphis',\n",
       " 'salvador': 'salvador',\n",
       " 'emma': 'emma',\n",
       " 'mit': 'mit',\n",
       " 'zoo': 'zoo',\n",
       " 'gibson': 'gibson',\n",
       " 'wording': 'libellé',\n",
       " 'emerging': 'émergente',\n",
       " 'portions': 'portions',\n",
       " 'macedonia': 'macédoine',\n",
       " 'ethics': 'éthique',\n",
       " 'depot': 'depot',\n",
       " 'curtis': 'curtis',\n",
       " 'rescued': 'sauvé',\n",
       " 'gaelic': 'gaélique',\n",
       " 'slovakia': 'slovaquie',\n",
       " 'elevated': 'élevé',\n",
       " 'jeremy': 'jérémy',\n",
       " 'listen': 'écoute',\n",
       " 'impressive': 'impressionnants',\n",
       " 'bradley': 'bradley',\n",
       " 'surely': 'surement',\n",
       " 'egg': 'oeufs',\n",
       " 'conquest': 'conquêtes',\n",
       " 'rod': 'tige',\n",
       " 'cdp': 'cdp',\n",
       " 'algorithm': 'algorithme',\n",
       " 'burn': 'brûlent',\n",
       " 'thesis': 'thèse',\n",
       " 'lover': 'amant',\n",
       " 'capitol': 'capitol',\n",
       " 'ferdinand': 'ferdinand',\n",
       " 'marshal': 'maréchal',\n",
       " 'judaism': 'judaïsme',\n",
       " 'balls': 'billes',\n",
       " 'nacional': 'nacional',\n",
       " 'wrestlers': 'lutteurs',\n",
       " 'ahmed': 'ahmed',\n",
       " 'sin': 'péchés',\n",
       " 'holocaust': 'holocauste',\n",
       " 'edgar': 'edgar',\n",
       " 'saxophone': 'saxophone',\n",
       " 'retain': 'retenir',\n",
       " 'wishes': 'vœux',\n",
       " 'prepare': 'préparez',\n",
       " 'ruins': 'ruine',\n",
       " 'ibm': 'ibm',\n",
       " 'rochester': 'rochester',\n",
       " 'nigerian': 'nigérian',\n",
       " 'jesse': 'jesse',\n",
       " 'malaysian': 'malaisienne',\n",
       " 'atlas': 'atlas',\n",
       " 'telegraph': 'télégraphique',\n",
       " 'performer': 'interprète',\n",
       " 'cannon': 'canons',\n",
       " 'encounter': 'rencontre',\n",
       " 'emily': 'emily',\n",
       " 'dissolved': 'dissous',\n",
       " 'catalogue': 'catalogue',\n",
       " 'discrimination': 'discriminations',\n",
       " 'myspace': 'myspace',\n",
       " 'reveal': 'révèlent',\n",
       " 'wizard': 'magicien',\n",
       " 'teen': 'ado',\n",
       " 'spots': 'taches',\n",
       " 'bomber': 'poseur',\n",
       " 'foods': 'aliments',\n",
       " 'quest': 'quête',\n",
       " 'connor': 'connor',\n",
       " 'screenplay': 'scénario',\n",
       " 'motors': 'moteurs',\n",
       " 'minimal': 'minimal',\n",
       " 'muscle': 'musclé',\n",
       " 'prestigious': 'prestigieuses',\n",
       " 'sustainable': 'soutenable',\n",
       " 'chelsea': 'chelsea',\n",
       " 'strict': 'stricte',\n",
       " 'kingston': 'kingston',\n",
       " 'sheep': 'ovins',\n",
       " 'andrea': 'andrea',\n",
       " 'complaints': 'réclamations',\n",
       " 'connects': 'connecte',\n",
       " 'nursing': 'infirmières',\n",
       " 'defenders': 'défenseurs',\n",
       " 'richardson': 'richardson',\n",
       " 'triangle': 'triangles',\n",
       " 'nato': 'otan',\n",
       " 'teeth': 'dents',\n",
       " 'occasional': 'occasionnel',\n",
       " 'strictly': 'strictement',\n",
       " 'harper': 'harper',\n",
       " 'fluid': 'fluide',\n",
       " 'fed': 'nourris',\n",
       " 'newfoundland': 'neuve',\n",
       " 'disbanded': 'démantelé',\n",
       " 'comparable': 'comparable',\n",
       " 'documentation': 'documentation',\n",
       " 'brien': 'brien',\n",
       " 'compounds': 'composés',\n",
       " 'pointing': 'pointant',\n",
       " 'edmund': 'edmond',\n",
       " 'naturally': 'naturellement',\n",
       " 'forcing': 'forcer',\n",
       " 'ussr': 'urss',\n",
       " 'laser': 'lasers',\n",
       " 'lat': 'lats',\n",
       " 'sculptor': 'sculpteur',\n",
       " 'guild': 'guilde',\n",
       " 'observer': 'observatrice',\n",
       " 'worlds': 'mondes',\n",
       " 'imprisoned': 'emprisonné',\n",
       " 'wrestler': 'catcheur',\n",
       " 'praise': 'éloges',\n",
       " 'parishes': 'paroisses',\n",
       " 'bones': 'os',\n",
       " 'css': 'css',\n",
       " 'cox': 'cox',\n",
       " 'contracts': 'contrats',\n",
       " 'consequences': 'conséquences',\n",
       " 'provisions': 'provisions',\n",
       " 'circulation': 'circulation',\n",
       " 'butterfly': 'papillon',\n",
       " 'hugo': 'hugo',\n",
       " 'abolished': 'aboli',\n",
       " 'algeria': 'algérie',\n",
       " 'edu': 'edu',\n",
       " 'sufficiently': 'suffisamment',\n",
       " 'armies': 'armées',\n",
       " 'separation': 'séparation',\n",
       " 'spy': 'espionnage',\n",
       " 'cliff': 'cliff',\n",
       " 'technically': 'techniquement',\n",
       " 'reactions': 'réactions',\n",
       " 'lithuanian': 'lituanienne',\n",
       " 'trick': 'astuce',\n",
       " 'curve': 'courbe',\n",
       " 'accidents': 'accidents',\n",
       " 'horizontal': 'horizontale',\n",
       " 'uploader': 'uploader',\n",
       " 'legends': 'légendes',\n",
       " 'enzyme': 'enzymatique',\n",
       " 'freight': 'fret',\n",
       " 'hydrogen': 'hydrogène',\n",
       " 'broadcasts': 'diffusions',\n",
       " 'viii': 'viii',\n",
       " 'caroline': 'caroline',\n",
       " 'pull': 'tirez',\n",
       " 'plymouth': 'plymouth',\n",
       " 'twentieth': 'xxe',\n",
       " 'cuts': 'coupes',\n",
       " 'mediation': 'médiation',\n",
       " 'airfield': 'aérodrome',\n",
       " 'catalog': 'catalogue',\n",
       " 'dale': 'dale',\n",
       " 'synthesis': 'synthèse',\n",
       " 'rape': 'violer',\n",
       " 'seoul': 'séoul',\n",
       " 'engagement': 'fiançailles',\n",
       " 'coin': 'pièce',\n",
       " 'lucy': 'lucy',\n",
       " 'platinum': 'platinum',\n",
       " 'twins': 'jumeaux',\n",
       " 'memories': 'souvenirs',\n",
       " 'robertson': 'robertson',\n",
       " 'verified': 'vérifié',\n",
       " 'anthology': 'anthologie',\n",
       " 'milton': 'milton',\n",
       " 'geological': 'géologique',\n",
       " 'defining': 'définissant',\n",
       " 'dinner': 'diner',\n",
       " 'hosting': 'hébergement',\n",
       " 'thriller': 'thriller',\n",
       " 'retreat': 'retraite',\n",
       " 'albany': 'albany',\n",
       " 'abdul': 'abdel',\n",
       " 'ignore': 'ignorer',\n",
       " 'migration': 'migrations',\n",
       " 'carefully': 'attentivement',\n",
       " 'magnitude': 'grandeur',\n",
       " 'sudan': 'soudan',\n",
       " 'manages': 'gère',\n",
       " 'duration': 'durée',\n",
       " 'henderson': 'henderson',\n",
       " 'explorer': 'explorateur',\n",
       " 'marco': 'marco',\n",
       " 'fusion': 'fusion',\n",
       " 'aids': 'aides',\n",
       " 'gathered': 'recueillies',\n",
       " 'reflected': 'réfléchie',\n",
       " 'afraid': 'effrayé',\n",
       " 'presbyterian': 'presbytérienne',\n",
       " 'automobile': 'automobile',\n",
       " 'fault': 'faille',\n",
       " 'pound': 'livre',\n",
       " 'allegedly': 'prétendument',\n",
       " 'delay': 'délai',\n",
       " 'developers': 'développeurs',\n",
       " 'belfast': 'belfast',\n",
       " 'arctic': 'arctique',\n",
       " 'kurt': 'kurt',\n",
       " 'mayors': 'maires',\n",
       " 'windsor': 'windsor',\n",
       " 'assumption': 'hypothèse',\n",
       " 'plates': 'plaques',\n",
       " 'fourteen': 'quatorze',\n",
       " 'nominee': 'nominé',\n",
       " 'disruption': 'perturbation',\n",
       " 'monroe': 'monroe',\n",
       " 'hearts': 'coeur',\n",
       " 'belgrade': 'belgrade',\n",
       " 'victories': 'victoires',\n",
       " 'extending': 'étendre',\n",
       " 'pale': 'pâle',\n",
       " 'pursuit': 'poursuite',\n",
       " 'glory': 'glory',\n",
       " 'destroyer': 'destroyer',\n",
       " 'deeply': 'profondément',\n",
       " 'lectures': 'conférences',\n",
       " 'affiliate': 'affiliés',\n",
       " 'preston': 'preston',\n",
       " 'deceased': 'décédé',\n",
       " 'speaks': 'parle',\n",
       " 'gathering': 'cueillette',\n",
       " 'angry': 'fâché',\n",
       " 'incomplete': 'incomplètes',\n",
       " 'enrolled': 'inscrit',\n",
       " 'configuration': 'configuration',\n",
       " 'brad': 'brad',\n",
       " 'skill': 'compétences',\n",
       " 'intense': 'intenses',\n",
       " 'tasmania': 'tasmanie',\n",
       " 'commitment': 'engagement',\n",
       " 'loved': 'aimés',\n",
       " 'reforms': 'réformes',\n",
       " 'rulers': 'dirigeants',\n",
       " 'uruguay': 'uruguay',\n",
       " 'sustained': 'soutenu',\n",
       " 'napoleon': 'napoléon',\n",
       " 'confirm': 'confirmer',\n",
       " 'breed': 'race',\n",
       " 'auxiliary': 'auxiliaires',\n",
       " 'enabled': 'activé',\n",
       " 'discography': 'discographie',\n",
       " 'licence': 'licence',\n",
       " 'refugees': 'réfugiés',\n",
       " 'adrian': 'adrian',\n",
       " 'pipe': 'tuyau',\n",
       " 'karen': 'karen',\n",
       " 'altered': 'altérée',\n",
       " 'budapest': 'budapest',\n",
       " 'designers': 'dessinateurs',\n",
       " 'heir': 'héritière',\n",
       " 'advisor': 'conseillère',\n",
       " 'illustrate': 'illustrent',\n",
       " 'authorized': 'autorisée',\n",
       " 'hide': 'cacher',\n",
       " 'announcement': 'annonce',\n",
       " 'compact': 'compacte',\n",
       " 'particles': 'particules',\n",
       " 'refuses': 'refuse',\n",
       " 'receiver': 'receveur',\n",
       " 'civilians': 'civiles',\n",
       " 'marsh': 'marsh',\n",
       " 'vinyl': 'vinyl',\n",
       " 'delayed': 'différé',\n",
       " 'encountered': 'rencontrés',\n",
       " 'wednesday': 'mercredi',\n",
       " 'chilean': 'chiliens',\n",
       " 'hey': 'hey',\n",
       " 'chambers': 'chambers',\n",
       " 'demo': 'démos',\n",
       " 'ahmad': 'ahmad',\n",
       " 'santos': 'santos',\n",
       " 'paying': 'payer',\n",
       " 'interpreted': 'interprétée',\n",
       " 'submit': 'soumettre',\n",
       " 'desired': 'souhaité',\n",
       " 'followers': 'abonnés',\n",
       " 'observatory': 'observatoire',\n",
       " 'problematic': 'problématiques',\n",
       " 'springfield': 'springfield',\n",
       " 'kit': 'kit',\n",
       " 'remarks': 'observations',\n",
       " 'burton': 'burton',\n",
       " 'coached': 'entraînée',\n",
       " 'monarch': 'monarque',\n",
       " 'observations': 'observations',\n",
       " 'beetle': 'scarabée',\n",
       " 'promised': 'promis',\n",
       " 'palomar': 'palomar',\n",
       " 'cream': 'cream',\n",
       " 'presenter': 'présentateur',\n",
       " 'potter': 'potter',\n",
       " 'copyvio': 'copyvio',\n",
       " 'favourite': 'préférés',\n",
       " 'transformation': 'transformation',\n",
       " 'mcdonald': 'mcdonald',\n",
       " 'bavaria': 'bavière',\n",
       " 'kumar': 'kumar',\n",
       " 'nineteenth': 'nineteenth',\n",
       " 'severely': 'sévèrement',\n",
       " 'mixture': 'mélange',\n",
       " 'browser': 'navigateur',\n",
       " 'endangered': 'menacé',\n",
       " 'mate': 'mate',\n",
       " 'lyon': 'lyon',\n",
       " 'illustration': 'illustration',\n",
       " 'kyle': 'kyle',\n",
       " 'afl': 'afl',\n",
       " 'brook': 'ruisseau',\n",
       " 'geometry': 'géométrie',\n",
       " 'ping': 'ping',\n",
       " 'extends': 'étend',\n",
       " 'aggregate': 'agrégats',\n",
       " 'variants': 'variantes',\n",
       " 'baroque': 'baroques',\n",
       " 'iso': 'iso',\n",
       " 'collapsed': 'effondrée',\n",
       " 'integral': 'intégrales',\n",
       " 'jake': 'jake',\n",
       " 'hopes': 'espoirs',\n",
       " 'cornell': 'cornell',\n",
       " 'modes': 'modes',\n",
       " 'servant': 'servante',\n",
       " 'kenny': 'kenny',\n",
       " 'hurt': 'blesser',\n",
       " 'inline': 'inline',\n",
       " 'carlo': 'carlo',\n",
       " 'lynn': 'lynn',\n",
       " 'stability': 'stabilité',\n",
       " 'hoping': 'espérer',\n",
       " 'imposed': 'imposé',\n",
       " 'confusing': 'déroutant',\n",
       " 'summaries': 'sommaires',\n",
       " 'beetles': 'coléoptères',\n",
       " 'joel': 'joel',\n",
       " 'jets': 'jets',\n",
       " 'logos': 'logos',\n",
       " 'vital': 'vitaux',\n",
       " 'malcolm': 'malcolm',\n",
       " 'winnipeg': 'winnipeg',\n",
       " 'kilometers': 'kilométrage',\n",
       " 'songwriters': 'compositeurs',\n",
       " 'buddhism': 'bouddhisme',\n",
       " 'nose': 'nez',\n",
       " 'respected': 'respectées',\n",
       " 'pace': 'rythme',\n",
       " 'thunder': 'tonnerre',\n",
       " 'centered': 'centrées',\n",
       " 'physicians': 'médecins',\n",
       " 'bolivia': 'bolivie',\n",
       " 'forget': 'oubliez',\n",
       " 'implies': 'implique',\n",
       " 'crops': 'récoltes',\n",
       " 'halifax': 'halifax',\n",
       " 'toll': 'péages',\n",
       " 'monk': 'moine',\n",
       " 'extraordinary': 'extraordinaires',\n",
       " 'lessons': 'enseignements',\n",
       " 'pub': 'pub',\n",
       " 'paralympics': 'paralympiques',\n",
       " 'monte': 'monte',\n",
       " 'maría': 'maría',\n",
       " 'segments': 'segments',\n",
       " 'deer': 'chevreuil',\n",
       " 'wireless': 'wireless',\n",
       " 'commenced': 'commencée',\n",
       " 'mysterious': 'mystérieuse',\n",
       " 'consultant': 'consultante',\n",
       " 'fraser': 'fraser',\n",
       " 'formats': 'formats',\n",
       " 'jam': 'confitures',\n",
       " 'chicken': 'poulets',\n",
       " 'enable': 'activer',\n",
       " 'idol': 'idole',\n",
       " 'reid': 'reid',\n",
       " 'births': 'accouchements',\n",
       " 'amazing': 'étonnant',\n",
       " 'pet': 'animal',\n",
       " 'upset': 'contrarié',\n",
       " 'loves': 'adore',\n",
       " 'stretch': 'étirement',\n",
       " 'nominate': 'désigner',\n",
       " 'striking': 'frappant',\n",
       " 'striker': 'attaquant',\n",
       " 'accidentally': 'accidentellement',\n",
       " 'louisville': 'louisville',\n",
       " 'hopkins': 'hopkins',\n",
       " 'eds': 'eds',\n",
       " 'goddess': 'déesse',\n",
       " 'resumed': 'reprise',\n",
       " 'satisfy': 'satisfaire',\n",
       " 'notion': 'notion',\n",
       " 'voltage': 'voltage',\n",
       " 'betty': 'betty',\n",
       " 'marion': 'marion',\n",
       " 'geology': 'géologie',\n",
       " 'cyclone': 'cyclone',\n",
       " 'export': 'exportation',\n",
       " 'lightning': 'foudre',\n",
       " 'impressed': 'impressionnée',\n",
       " 'maintains': 'entretient',\n",
       " 'logical': 'logiques',\n",
       " 'aggressive': 'agressivité',\n",
       " 'jin': 'jin',\n",
       " 'julie': 'julie',\n",
       " 'fbi': 'fbi',\n",
       " 'yankees': 'yankees',\n",
       " 'ludwig': 'ludwig',\n",
       " 'pond': 'étang',\n",
       " 'suburban': 'banlieue',\n",
       " 'enlisted': 'engagé',\n",
       " 'moments': 'instants',\n",
       " 'conjunction': 'conjonction',\n",
       " 'interim': 'intérim',\n",
       " 'lucky': 'veinard',\n",
       " 'targeted': 'ciblées',\n",
       " 'lon': 'lon',\n",
       " 'speedway': 'speedway',\n",
       " 'regiments': 'régiments',\n",
       " 'picks': 'pics',\n",
       " 'prevented': 'empêché',\n",
       " 'toy': 'jouets',\n",
       " 'bicycle': 'bicyclette',\n",
       " 'purely': 'purement',\n",
       " 'interactions': 'interactions',\n",
       " 'fraud': 'imposteur',\n",
       " 'lang': 'lang',\n",
       " 'arcade': 'arcade',\n",
       " 'lecture': 'conférence',\n",
       " 'sanctuary': 'sanctuaire',\n",
       " 'dragons': 'dragons',\n",
       " 'copa': 'copa',\n",
       " 'careful': 'prudents',\n",
       " 'nurse': 'infirmière',\n",
       " 'rivals': 'rivaux',\n",
       " 'module': 'module',\n",
       " 'supplement': 'supplément',\n",
       " 'lens': 'lentilles',\n",
       " 'patron': 'patronne',\n",
       " 'commands': 'commandes',\n",
       " 'trend': 'tendance',\n",
       " 'superintendent': 'surintendant',\n",
       " 'gerald': 'gérald',\n",
       " 'rap': 'rap',\n",
       " 'geneva': 'genève',\n",
       " 'ash': 'cendre',\n",
       " 'blade': 'lame',\n",
       " 'disappeared': 'disparus',\n",
       " 'patrolling': 'patrouilles',\n",
       " 'predominantly': 'majoritairement',\n",
       " 'committees': 'comités',\n",
       " 'boom': 'boom',\n",
       " 'sailors': 'matelots',\n",
       " 'beaten': 'battus',\n",
       " 'smoke': 'fumées',\n",
       " 'assassination': 'assassinat',\n",
       " 'lancaster': 'lancaster',\n",
       " 'reynolds': 'reynolds',\n",
       " 'divorce': 'divorces',\n",
       " 'dust': 'poussières',\n",
       " 'saxon': 'saxonne',\n",
       " 'separately': 'séparément',\n",
       " 'grain': 'grains',\n",
       " 'executives': 'dirigeants',\n",
       " 'translations': 'traduction',\n",
       " 'zimbabwe': 'zimbabwe',\n",
       " 'thrown': 'jeté',\n",
       " 'cohen': 'cohen',\n",
       " 'diving': 'plongée',\n",
       " 'neighbouring': 'voisin',\n",
       " 'carroll': 'carroll',\n",
       " 'accounting': 'comptabilité',\n",
       " 'mesa': 'missa',\n",
       " 'prussia': 'prusse',\n",
       " 'intelligent': 'intelligentes',\n",
       " 'cherry': 'cerises',\n",
       " 'tobacco': 'tabagisme',\n",
       " 'cleaned': 'nettoyés',\n",
       " 'varieties': 'variétés',\n",
       " 'bench': 'banc',\n",
       " 'directions': 'directions',\n",
       " 'ellen': 'ellen',\n",
       " 'padding': 'rembourrage',\n",
       " 'measurement': 'mesure',\n",
       " 'paradise': 'paradise',\n",
       " 'alexandria': 'alexandrie',\n",
       " 'complement': 'complément',\n",
       " 'witch': 'sorcières',\n",
       " 'attraction': 'attrait',\n",
       " 'diana': 'diana',\n",
       " 'personalities': 'personnalités',\n",
       " 'colleagues': 'collègues',\n",
       " 'busy': 'occupée',\n",
       " 'cia': 'cia',\n",
       " 'screenwriter': 'scénariste',\n",
       " 'rankings': 'classements',\n",
       " 'aboriginal': 'autochtone',\n",
       " 'commanders': 'commandants',\n",
       " 'salem': 'salem',\n",
       " 'wagner': 'wagner',\n",
       " 'sanctions': 'sanctions',\n",
       " 'americas': 'amériques',\n",
       " 'endings': 'fins',\n",
       " 'instructor': 'instructeur',\n",
       " 'nobility': 'noblesse',\n",
       " 'divorced': 'divorcés',\n",
       " 'varies': 'varient',\n",
       " 'tomorrow': 'tomorrow',\n",
       " 'manuscripts': 'manuscrits',\n",
       " 'unified': 'unifiés',\n",
       " 'clarify': 'clarifier',\n",
       " 'scouts': 'scouts',\n",
       " 'investigations': 'enquêtes',\n",
       " 'silva': 'silva',\n",
       " 'derek': 'derek',\n",
       " 'agenda': 'agenda',\n",
       " 'provision': 'disposition',\n",
       " 'humanity': 'humanité',\n",
       " 'admit': 'admet',\n",
       " 'terror': 'terrorisme',\n",
       " 'contestants': 'concurrents',\n",
       " 'trinidad': 'trinidad',\n",
       " 'distant': 'lointain',\n",
       " 'burke': 'burke',\n",
       " 'circles': 'cercles',\n",
       " 'assignment': 'assignation',\n",
       " 'recalled': 'rappelés',\n",
       " 'shrine': 'autel',\n",
       " 'sail': 'naviguer',\n",
       " 'willie': 'willie',\n",
       " 'karnataka': 'karnataka',\n",
       " 'celebrate': 'célébrer',\n",
       " 'ranch': 'ranch',\n",
       " 'collaborated': 'collaboré',\n",
       " 'vampire': 'vampires',\n",
       " 'playwright': 'dramaturge',\n",
       " 'sick': 'malade',\n",
       " 'associates': 'associés',\n",
       " 'heinrich': 'heinrich',\n",
       " 'ethiopia': 'ethiopie',\n",
       " 'flags': 'drapeaux',\n",
       " 'tel': 'tel',\n",
       " 'drove': 'conduisait',\n",
       " 'learns': 'apprend',\n",
       " 'shorts': 'shorts',\n",
       " 'accomplished': 'accomplie',\n",
       " 'autobiography': 'autobiographie',\n",
       " 'recruited': 'recrutés',\n",
       " 'uprising': 'soulèvement',\n",
       " 'edwin': 'edwin',\n",
       " 'velocity': 'vitesse',\n",
       " 'terminology': 'terminologie',\n",
       " 'raiders': 'raiders',\n",
       " 'coordinates': 'coordonnés',\n",
       " 'brighton': 'brighton',\n",
       " 'viola': 'viola',\n",
       " 'para': 'para',\n",
       " 'morrison': 'morrison',\n",
       " 'propulsion': 'propulsion',\n",
       " 'boxer': 'boxeur',\n",
       " 'finale': 'finale',\n",
       " 'shoulder': 'épaule',\n",
       " 'disabled': 'désactivé',\n",
       " 'joins': 'rejoint',\n",
       " 'div': 'div',\n",
       " 'tactics': 'tactiques',\n",
       " 'ernst': 'ernst',\n",
       " 'innocent': 'innocent',\n",
       " 'rapper': 'rappeur',\n",
       " 'privacy': 'confidentialité',\n",
       " 'boeing': 'boeing',\n",
       " 'cites': 'cites',\n",
       " 'emmy': 'emmy',\n",
       " 'indo': 'indo',\n",
       " 'distinguish': 'distinguer',\n",
       " 'rosa': 'rosa',\n",
       " 'thermal': 'thermiques',\n",
       " 'flute': 'flûtes',\n",
       " 'marines': 'marines',\n",
       " 'feminist': 'féministes',\n",
       " 'trustees': 'mandataires',\n",
       " 'sculptures': 'sculptures',\n",
       " 'bacteria': 'bactérie',\n",
       " 'introduce': 'introduire',\n",
       " 'landmarks': 'repères',\n",
       " 'disorders': 'troubles',\n",
       " 'rivalry': 'rivalités',\n",
       " 'prevention': 'prévention',\n",
       " 'honored': 'honoré',\n",
       " 'healthy': 'saine',\n",
       " 'circus': 'cirques',\n",
       " 'speculation': 'spéculation',\n",
       " 'burma': 'birmanie',\n",
       " 'sec': 'sec',\n",
       " 'quiet': 'silencieux',\n",
       " 'knee': 'genou',\n",
       " 'deliver': 'livrer',\n",
       " 'hypothesis': 'hypothèse',\n",
       " 'referendum': 'référendum',\n",
       " 'travelling': 'voyager',\n",
       " 'estonian': 'estoniens',\n",
       " 'pastor': 'pasteur',\n",
       " 'sofia': 'sofia',\n",
       " 'tribune': 'tribune',\n",
       " 'permit': 'permis',\n",
       " 'priority': 'prioritaires',\n",
       " 'cent': 'cent',\n",
       " 'consequence': 'conséquence',\n",
       " 'rica': 'rica',\n",
       " 'furniture': 'meuble',\n",
       " 'macdonald': 'macdonald',\n",
       " 'honest': 'honnêtes',\n",
       " 'innovative': 'innovateur',\n",
       " 'estimate': 'estimations',\n",
       " 'atp': 'atp',\n",
       " 'rotation': 'rotation',\n",
       " 'syracuse': 'syracuse',\n",
       " 'lecturer': 'conférencier',\n",
       " 'automated': 'automatisé',\n",
       " 'obscure': 'obscur',\n",
       " 'kosovo': 'kosovo',\n",
       " 'classics': 'classiques',\n",
       " 'julius': 'jules',\n",
       " 'appreciated': 'appréciées',\n",
       " 'naples': 'naples',\n",
       " 'sebastian': 'sebastien',\n",
       " 'activated': 'activé',\n",
       " 'varied': 'variés',\n",
       " 'offense': 'offense',\n",
       " 'advised': 'conseillé',\n",
       " 'barnes': 'barnes',\n",
       " 'acknowledged': 'reconnus',\n",
       " 'exceptions': 'dérogations',\n",
       " 'martha': 'marthe',\n",
       " 'quarters': 'trimestres',\n",
       " 'drawings': 'dessins',\n",
       " 'refuge': 'refuge',\n",
       " 'maharashtra': 'maharashtra',\n",
       " 'conventions': 'conventions',\n",
       " 'elliott': 'elliot',\n",
       " 'diplomat': 'diplomate',\n",
       " 'unused': 'inutilisé',\n",
       " 'searches': 'recherches',\n",
       " 'brigadier': 'brigadier',\n",
       " 'particle': 'particules',\n",
       " 'malayalam': 'malayalam',\n",
       " 'thursday': 'jeudi',\n",
       " 'icon': 'icône',\n",
       " 'ulster': 'ulster',\n",
       " 'genes': 'gènes',\n",
       " 'infinite': 'infini',\n",
       " 'considerably': 'considérablement',\n",
       " 'vale': 'vale',\n",
       " 'portraits': 'portraits',\n",
       " 'paste': 'pâte',\n",
       " 'randy': 'randy',\n",
       " 'saxony': 'saxe',\n",
       " 'convoy': 'convoi',\n",
       " 'annie': 'annie',\n",
       " 'excessive': 'excessive',\n",
       " 'believing': 'croire',\n",
       " 'rhine': 'rhin',\n",
       " 'mineral': 'minérales',\n",
       " 'implement': 'implémenter',\n",
       " 'surgeon': 'chirurgien',\n",
       " 'badge': 'badge',\n",
       " 'charleston': 'charleston',\n",
       " 'clause': 'clause',\n",
       " 'infection': 'infection',\n",
       " 'electron': 'électron',\n",
       " 'walt': 'walt',\n",
       " 'cnn': 'cnn',\n",
       " 'likewise': 'pareillement',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = set(english_vecs.keys())\n",
    "\n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.vstack(Y_l)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# getting the training set:\n",
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_get_matrices(get_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "        \n",
    "    # diff is XR - Y    \n",
    "    diff = np.dot(X,R) - Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference    \n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i is the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared/m\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for an experiment with random matrices: 8.1866\n"
     ]
    }
   ],
   "source": [
    "# Testing your implementation.\n",
    "np.random.seed(123)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = np.random.rand(n, n)\n",
    "print(f\"Expected loss for an experiment with random matrices: {compute_loss(X, Y, R):.4f}\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m    \n",
    "    gradient = np.dot(X.T, (np.dot(X,R)-Y)) * (2/m)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the gradient matrix: [1.3498175  1.11264981 0.69626762 0.98468499 1.33828969]\n"
     ]
    }
   ],
   "source": [
    "# Testing your implementation.\n",
    "np.random.seed(123)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = np.random.rand(n, n)\n",
    "gradient = compute_gradient(X, Y, R)\n",
    "print(f\"First row of the gradient matrix: {gradient[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_compute_gradient(compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True, compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        ### START CODE HERE ###\n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate*gradient\n",
    "        ### END CODE HERE ###\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Testing your implementation.\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_align_embeddings(align_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "\n",
    "    # sort the similarity list and get the indices of the sorted list    \n",
    "    sorted_ids = np.argsort(similarity_l)  \n",
    "    \n",
    "    # Reverse the order of the sorted_ids array\n",
    "    sorted_ids = sorted_ids[::-1]\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[:k]\n",
    "    ### END CODE HERE ###\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1]\n",
      " [1 0 5]\n",
      " [9 9 9]]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Test your implementation:\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_nearest_neighbor(nearest_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y, 1)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct/X.shape[0]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.unittest_test_vocabulary(test_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C12 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # process the document into a list of words (process the tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        doc_embedding = doc_embedding + en_embeddings.get(word,0)\n",
    "    ### END CODE HERE ###\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNQ_C13 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# testing your function\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_get_document_embedding(get_document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C14 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_document_vecs(all_docs, en_embeddings, get_document_embedding=get_document_embedding):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding = get_document_embedding(all_docs[i],en_embeddings)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C15 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function. This cell may take some seconds to run.\n",
    "w4_unittest.test_get_document_vecs(get_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n",
    "tweet_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@hanbined sad pray for me :(((\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# this gives you a similar tweet as your input.\n",
    "# this implementation is vectorized...\n",
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(256) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C17 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)    \n",
    "    dot_product = np.dot(v, planes)\n",
    "        \n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "\n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n",
    "    h = np.where(sign_of_dot_product >= 0, True, False)\n",
    "\n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i        \n",
    "        hash_value += 2**i * h[i]\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 768\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_hash_value_of_vector(hash_value_of_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets = 10\n",
    "hash_table = {i:[] for i in range(num_buckets)}\n",
    "hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C19 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# This is the code used to create a hash table: feel free to read over it\n",
    "def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)    \n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 3 document vectors\n",
      "The id table at key 0 has 3\n",
      "The first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_make_hash_table(make_hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "# Creating the hashtables\n",
    "def create_hash_id_tables(n_universes):\n",
    "    hash_tables = []\n",
    "    id_tables = []\n",
    "    for universe_id in range(n_universes):  # there are 25 hashes\n",
    "        print('working on hash universe #:', universe_id)\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "        hash_tables.append(hash_table)\n",
    "        id_tables.append(id_table)\n",
    "    \n",
    "    return hash_tables, id_tables\n",
    "\n",
    "hash_tables, id_tables = create_hash_id_tables(N_UNIVERSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C21 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# This is the code used to do the fast nearest neighbor search. Feel free to go over it\n",
    "def approximate_knn(doc_id, v, planes_l, hash_tables, id_tables, k=1, num_universes_to_use=25, hash_value_of_vector=hash_value_of_vector):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    #assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vectors that will be checked as possible nearest neighbor\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # list of document IDs\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        # get the set of planes from the planes_l list, for this particular universe_id\n",
    "        planes = planes_l[universe_id]\n",
    "\n",
    "        # get the hash value of the vector for this set of planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # get the hash table for this particular universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "            \n",
    "            if doc_id == new_id:\n",
    "                continue\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 77 vecs\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Sample\n",
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n",
      "Nearest neighbor at document id 2478\n",
      "document contents: #ShareTheLove @oymgroup @musicartisthere for being top HighValue members this week :) @nataliavas http://t.co/IWSDMtcayt\n",
      "Nearest neighbor at document id 105\n",
      "document contents: #FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 77 vecs\n",
      "Fast considering 153 vecs\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w4_unittest.test_approximate_knn(approximate_knn, hash_tables, id_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cs224n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7ade109fcaf6ff10adabf032c9e3d03d273292ea2827ed946949d6cb5f69c2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
