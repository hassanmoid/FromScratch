{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed5aed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Program Files\\Eclipse Adoptium\\jdk-21.0.8.9-hotspot\n",
      "PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.9\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.9\\libnvvp;;C:\\Program Files\\Microsoft SDKs\\Azure\\CLI2\\wbin;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\LINQPad8;C:\\Program Files\\dotnet\\;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\Program Files\\Microsoft Service Fabric\\bin\\Fabric\\Fabric.Code;C:\\Program Files\\Microsoft SDKs\\Service Fabric\\Tools\\ServiceFabricLocalClusterManager;C:\\Program Files\\nodejs\\;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2025.3.0\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\CMake\\bin;C:\\Users\\moidhassan\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\moidhassan\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\Library\\bin;C:\\Users\\moidhassan\\AppData\\Local\\anaconda3;C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\Library;C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\Scripts;C:\\Users\\moidhassan\\.dotnet\\tools;C:\\Users\\moidhassan\\AppData\\Roaming\\npm;C:\\Program Files\\Microsoft SDKs\\Azure\\CLI2\\wbin;;C:\\Program Files\\Eclipse Adoptium\\jdk-21.0.8.9-hotspot\\bin\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "try:  # optional dependency for grammar checking\n",
    "    from language_tool_python import LanguageTool  # type: ignore\n",
    "except ImportError:  # pragma: no cover - optional tool\n",
    "    LanguageTool = None  # type: ignore\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Eclipse Adoptium\\\\jdk-21.0.8.9-hotspot\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "print(\"PATH:\", os.environ.get(\"PATH\"))\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"rl_finetune_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ae2a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reward_weights = {\n",
    "        \"length\": 1.2,\n",
    "        \"politeness\": 1.2,\n",
    "        \"sentiment\": 0.7,\n",
    "        \"clarity\": 0.6,\n",
    "        \"cta\": 1.4,\n",
    "        \"personalization\": 0.7,\n",
    "        \"grammar\": 0.8,\n",
    "        \"value\": 1.1,\n",
    "        \"spam\": 0.8,\n",
    "        \"structure\": 0.8,\n",
    "        \"instructional_tone\": 0.8,\n",
    "        \"lexical_coherence\": 1.2,\n",
    "    }\n",
    "\n",
    "PROMPT_TEMPLATES = [\n",
    "    \"Write a concise, professional sales email introducing this idea: \\\"{snippet}\\\"\",\n",
    "    \"Compose a friendly B2B sales email based on this concept: \\\"{snippet}\\\"\",\n",
    "    \"Generate a product outreach email using this information: \\\"{snippet}\\\"\",\n",
    "    \"Write an enterprise sales email centered on: \\\"{snippet}\\\"\",\n",
    "    \"Craft a formal product introduction email about: \\\"{snippet}\\\"\",\n",
    "]\n",
    "\n",
    "data_path = \"data/seller_emails_v3.json\"\n",
    "output_dir = \"\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer_name = \"gpt2\"\n",
    "num_epochs = 2\n",
    "batch_size = 2\n",
    "max_new_tokens = 768\n",
    "top_p = 1.0\n",
    "temperature = 0.85\n",
    "learning_rate = 1.41e-5\n",
    "seed = 42\n",
    "max_length = 1024\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "010c4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seller_emails(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load seller email corpus stored as JSON list of strings.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Seller email JSON not found: {file_path}\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        raw = json.load(fp)\n",
    "\n",
    "    if not isinstance(raw, list) or not all(isinstance(entry, str) for entry in raw):\n",
    "        raise ValueError(\"Expected the JSON file to contain a list of strings (email bodies).\")\n",
    "\n",
    "    df = pd.DataFrame({\"email_text\": raw})\n",
    "    # adding email length feature\n",
    "    df[\"len_email_text\"] = df[\"email_text\"].str.len()\n",
    "    # clean text by removing extra whitespace\n",
    "    df[\"email_text\"] = df[\"email_text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    # remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"email_text\"]).reset_index(drop=True)\n",
    "    logger.info(\"Loaded %d seller emails from %s\", len(df), file_path)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9aa0f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_text</th>\n",
       "      <th>len_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biodegradable packaging: lower footprint, same...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear HR Director, I'm Jennifer from HealthFirs...</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning platform lifts test scores 18%. Pilot?</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, I work with boutique hotels to enhance ...</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cut cloud spend 30–40%. 15‑min chat next week?...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          email_text  len_email_text\n",
       "0  Biodegradable packaging: lower footprint, same...              58\n",
       "1  Dear HR Director, I'm Jennifer from HealthFirs...             478\n",
       "2    Learning platform lifts test scores 18%. Pilot?              47\n",
       "3  Hello, I work with boutique hotels to enhance ...             432\n",
       "4  Cut cloud spend 30–40%. 15‑min chat next week?...              57"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_seller_emails(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4adad5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts_from_emails(df: pd.DataFrame, *, text_column: str = \"email_text\") -> List[str]:\n",
    "    texts = df[text_column].dropna().tolist()\n",
    "    prompts: List[str] = []\n",
    "    for email in texts:\n",
    "        email = email.strip()\n",
    "        sentences = nltk.sent_tokenize(email)\n",
    "        core = [\n",
    "            s for s in sentences if not re.match(r\"^(hi|hello|dear|regards|thank|best)\", s.strip().lower())\n",
    "        ]\n",
    "        snippet = \"\"\n",
    "        for sentence in core:\n",
    "            if len(sentence.split()) >= 6:\n",
    "                snippet = sentence.strip()\n",
    "                break\n",
    "        if not snippet:\n",
    "            snippet = email[:120]\n",
    "        template = random.choice(PROMPT_TEMPLATES)\n",
    "        prompts.append(template.format(snippet=snippet))\n",
    "    logger.info(\"Generated %d prompts from dataset\", len(prompts))\n",
    "    return prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c91a86f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Compose a friendly B2B sales email based on this concept: \"Biodegradable packaging: lower footprint, same cost.\"',\n",
       " 'Generate a product outreach email using this information: \"Our corporate wellness programs have helped over 200 companies reduce healthcare costs by an average of $450 per employee annually while boosting morale.\"',\n",
       " 'Write an enterprise sales email centered on: \"Learning platform lifts test scores 18%.\"',\n",
       " 'Write an enterprise sales email centered on: \"Properties using our platform have seen an average 23% increase in positive reviews and a significant boost in repeat bookings.\"',\n",
       " 'Write an enterprise sales email centered on: \"Cut cloud spend 30–40%. 15‑min chat next week? – DataSync\"',\n",
       " 'Craft a formal product introduction email about: \"Our clients typically see 15-20% increase in conversion rates within 90 days.\"',\n",
       " 'Craft a formal product introduction email about: \"Logistics costs down 20–30%. Explore fulfillment?\"',\n",
       " 'Compose a friendly B2B sales email based on this concept: \"We specialize in cloud infrastructure optimization that reduces costs by 30-40% while improving performance.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"Dental no‑shows down 40%, throughput up 15%.\"',\n",
       " 'Compose a friendly B2B sales email based on this concept: \"Good morning, As consumers increasingly prefer eco-friendly brands, I wanted to introduce you to our biodegradable packaging solutions that don\\'t compromise on quality or cost.\"',\n",
       " 'Generate a product outreach email using this information: \"Wellness saves $450/employee yearly. Quick demo?\"',\n",
       " 'Generate a product outreach email using this information: \"Given your fleet size, I estimate we could save your organization approximately $85K annually.\"',\n",
       " 'Write an enterprise sales email centered on: \"Zero‑capex solar: ROI 4–5 yrs, big carbon cuts.\"',\n",
       " 'Craft a formal product introduction email about: \"Clients see 20-30% logistics cost reduction through our warehouse network and carrier partnerships.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"Organic produce <24h from harvest. Sample box?\"',\n",
       " 'Write an enterprise sales email centered on: \"Three major builders in your region use us.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"Predictive maintenance: 25% less downtime in 6 months.\"',\n",
       " 'Write an enterprise sales email centered on: \"Our AI ad platform delivers 3-4x ROAS for e-commerce brands in your space.\"',\n",
       " 'Generate a product outreach email using this information: \"Influencer matching: 5× engagement at 60% lower cost.\"',\n",
       " 'Write an enterprise sales email centered on: \"ContractIQ helps law firms and legal departments reduce contract review time by 60% using AI-assisted analysis.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"Hotel tech boosts reviews +23%. 15‑min intro?\"',\n",
       " 'Compose a friendly B2B sales email based on this concept: \"Given your facility size, this could be valuable.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"AI ads: 3–4x ROAS for brands like yours.\"',\n",
       " 'Craft a formal product introduction email about: \"Good morning, Our interactive learning platform helped 500+ schools improve student engagement and test scores by 18%.\"',\n",
       " 'Compose a friendly B2B sales email based on this concept: \"Free ransomware security audit for financial orgs.\"',\n",
       " 'Craft a formal product introduction email about: \"Our automated reminder system and smart booking platform have helped practices like yours reduce no-shows by 40% and increase patient throughput by 15%.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"Contract review 60% faster with AI.\"',\n",
       " 'Craft a formal product introduction email about: \"Good afternoon, Our regulatory compliance software streamlines FDA submissions for pharma companies.\"',\n",
       " 'Write an enterprise sales email centered on: \"Retail foot traffic analytics lift conversions 15–20%.\"',\n",
       " 'Write an enterprise sales email centered on: \"Commercial real estate firms love it.\"',\n",
       " 'Generate a product outreach email using this information: \"Quote time: hours → minutes for agencies.\"',\n",
       " 'Craft a formal product introduction email about: \"Tenant portal halves maintenance response time.\"',\n",
       " 'Write an enterprise sales email centered on: \"We\\'ve helped consumer brands achieve 5x better engagement rates at 60% lower cost than traditional influencer campaigns.\"',\n",
       " 'Write an enterprise sales email centered on: \"Fleet telematics can save ≈$85K annually.\"',\n",
       " 'Write an enterprise sales email centered on: \"Compliance software trims FDA timelines 20%.\"',\n",
       " 'Write an enterprise sales email centered on: \"BuildPro cuts delays 35% via real‑time scheduling.\"',\n",
       " 'Write an enterprise sales email centered on: \"SecureNet has protected over 150 financial institutions from breaches, and I believe we could provide valuable insights into your current security posture.\"',\n",
       " 'Write a concise, professional sales email introducing this idea: \"Clients achieve ROI in 4-5 years while cutting carbon footprint.\"']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = generate_prompts_from_emails(df)\n",
    "print(len(prompts))\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e76b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(tokenizer_name, model_name, device):\n",
    "    logger.info(\"Loading tokenizer: %s\", tokenizer_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    print(f\"Tokenizer pad token before adjustment: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set pad token to eos token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n",
    "\n",
    "    logger.info(\"Loading base model for PPO value head: %s\", model_name)\n",
    "    ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "    ppo_model = ppo_model.to(device)\n",
    "    logger.info(\"Model loaded on %s\", device)\n",
    "    return ppo_model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d9acebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\envs\\rl_ft_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token before adjustment: None (id=None)\n",
      "Set pad token to eos token: <|endoftext|> (id=50256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\envs\\rl_ft_gpu\\Lib\\site-packages\\trl\\models\\modeling_base.py:331: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(tokenizer_name, model_name, \"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29de5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ppo_trainer(\n",
    "    model: AutoModelForCausalLMWithValueHead,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    learning_rate: float,\n",
    "    batch_size: int\n",
    ") -> PPOTrainer:\n",
    "    logger.info(\"Initialising PPO trainer\")\n",
    "    ppo_config = PPOConfig(\n",
    "        model_name=None,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        mini_batch_size=batch_size,\n",
    "        optimize_cuda_cache=True,\n",
    "    )\n",
    "    trainer = PPOTrainer(config=ppo_config, model=model, tokenizer=tokenizer)\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46aea73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.trainer.ppo_trainer.PPOTrainer at 0x2b02bd383d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = create_ppo_trainer(model, tokenizer, learning_rate, batch_size)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "994b382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_analyzer(use_gpu: bool = True):\n",
    "    device = 0 if use_gpu and torch.cuda.is_available() else -1\n",
    "    logger.info(\"Loading sentiment analyzer (%s)...\", \"GPU\" if device == 0 else \"CPU\")\n",
    "    return pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e092c920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "sentiment_analyzer = load_sentiment_analyzer(use_gpu=device == \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12dad3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_language_tool():\n",
    "    if LanguageTool is None:\n",
    "        logger.warning(\"language_tool_python not installed; grammar rewards disabled.\")\n",
    "        return None\n",
    "    try:  # pragma: no cover - performs network access on first run\n",
    "        tool = LanguageTool(\"en-US\")\n",
    "        logger.info(\"Initialized LanguageTool for grammar checking.\")\n",
    "        return tool\n",
    "    except Exception as exc:  # pragma: no cover\n",
    "        logger.warning(\"Failed to initialize LanguageTool: %s\", exc)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea681a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool latest: 100%|██████████| 254M/254M [00:31<00:00, 7.97MB/s] \n",
      "Unzipping C:\\Users\\MOIDHA~1\\AppData\\Local\\Temp\\tmp9qa7lnvx.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloading LanguageTool latest: 100%|██████████| 254M/254M [00:18<00:00, 13.5MB/s] \n",
      "Unzipping C:\\Users\\MOIDHA~1\\AppData\\Local\\Temp\\tmp60inuwer.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloading LanguageTool latest: 100%|██████████| 254M/254M [00:19<00:00, 13.3MB/s] \n",
      "Unzipping C:\\Users\\MOIDHA~1\\AppData\\Local\\Temp\\tmpwwcfqi6v.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloading LanguageTool latest: 100%|██████████| 254M/254M [00:17<00:00, 14.7MB/s] \n",
      "Unzipping C:\\Users\\MOIDHA~1\\AppData\\Local\\Temp\\tmpx6mkzt_g.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n"
     ]
    }
   ],
   "source": [
    "grammar_tool = init_language_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5467db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CTA_PHRASES = [\n",
    "    \"call\",\n",
    "    \"reply\",\n",
    "    \"schedule\",\n",
    "    \"meet\",\n",
    "    \"connect\",\n",
    "    \"reach out\",\n",
    "    \"get in touch\",\n",
    "    \"book a demo\",\n",
    "    \"set up a meeting\",\n",
    "    \"schedule a call\",\n",
    "    \"contact us\",\n",
    "]\n",
    "\n",
    "POLITE_TERMS = [\n",
    "    \"thank\",\n",
    "    \"appreciate\",\n",
    "    \"please\",\n",
    "    \"hope\",\n",
    "    \"kindly\",\n",
    "    \"regards\",\n",
    "    \"grateful\",\n",
    "    \"welcome\",\n",
    "    \"would you\",\n",
    "    \"could you\",\n",
    "]\n",
    "\n",
    "UNCLEAR_TERMS = [\n",
    "    \"utilize\",\n",
    "    \"leverage\",\n",
    "    \"synergy\",\n",
    "    \"paradigm\",\n",
    "    \"bandwidth\",\n",
    "    \"ecosystem\",\n",
    "    \"turnkey\",\n",
    "    \"disruptive\",\n",
    "]\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "VALUE_TERMS_PATTERN = re.compile(\n",
    "    r\"\\b(save|reduce|increase|boost|improve|growth|roi|cost|revenue|profit)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "SPAM_TERMS = [\n",
    "    \"free\",\n",
    "    \"winner\",\n",
    "    \"click here\",\n",
    "    \"urgent\",\n",
    "    \"act now\",\n",
    "    \"limited time\",\n",
    "    \"guarantee\",\n",
    "]\n",
    "\n",
    "INSTRUCTIONAL_PATTERN = re.compile(\n",
    "    r\"(write (an|a) email|here'?s how|this guide|please follow)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def has_cta_phrase(text: str) -> Tuple[bool, List[str]]:\n",
    "    lower = text.lower()\n",
    "    matches = []\n",
    "    for phrase in CTA_PHRASES:\n",
    "        if \" \" in phrase:\n",
    "            if phrase in lower:\n",
    "                matches.append(phrase)\n",
    "        elif re.search(rf\"\\b{re.escape(phrase)}\\b\", lower):\n",
    "            matches.append(phrase)\n",
    "    return len(matches) > 0, matches\n",
    "\n",
    "\n",
    "def evaluate_structure_and_intent(text: str) -> Tuple[float, str]:\n",
    "    lower = text.lower()\n",
    "    matched: List[str] = []\n",
    "    greeting_match = re.findall(r\"\\b(hi|hello|dear)\\b\", lower)\n",
    "    closing_match = re.findall(r\"\\b(regards|sincerely|thanks|best)\\b\", lower)\n",
    "    product_match = re.findall(r\"\\b(product|solution|platform|offer|launch|introduc)\\w*\", lower)\n",
    "    if greeting_match:\n",
    "        matched.extend(greeting_match)\n",
    "    if closing_match:\n",
    "        matched.extend(closing_match)\n",
    "    if product_match:\n",
    "        matched.extend(product_match)\n",
    "    has_greeting = bool(greeting_match)\n",
    "    has_closing = bool(closing_match)\n",
    "    has_product = bool(product_match)\n",
    "    if has_greeting and has_closing and has_product:\n",
    "        return 1.0, f\"Proper structure detected: {matched}\"\n",
    "    if has_greeting or has_product:\n",
    "        return 0.5, f\"Partial email structure: {matched}\"\n",
    "    return -0.5, \"Missing greeting/closing/product context\"\n",
    "\n",
    "\n",
    "def penalize_instructional_tone(text: str) -> Tuple[float, str]:\n",
    "    matches = INSTRUCTIONAL_PATTERN.findall(text.lower())\n",
    "    if matches:\n",
    "        flat = [m[0] if isinstance(m, tuple) else m for m in matches]\n",
    "        return -1.0, f\"Instructional tone detected: {flat}\"\n",
    "    return 0.0, \"No instructional tone detected\"\n",
    "\n",
    "\n",
    "def lexical_coherence(text: str) -> Tuple[float, str]:\n",
    "    sentences = [s.strip().lower() for s in re.split(r\"[.!?]\", text) if s.strip()]\n",
    "    if len(sentences) < 2:\n",
    "        return 0.5, \"Too short to assess coherence\"\n",
    "    overlaps: List[float] = []\n",
    "    for idx in range(len(sentences) - 1):\n",
    "        s1 = {w for w in re.findall(r\"\\b[a-z]+\\b\", sentences[idx]) if w not in STOP_WORDS}\n",
    "        s2 = {w for w in re.findall(r\"\\b[a-z]+\\b\", sentences[idx + 1]) if w not in STOP_WORDS}\n",
    "        if not s1 or not s2:\n",
    "            continue\n",
    "        overlap = len(s1 & s2) / len(s1 | s2)\n",
    "        overlaps.append(overlap)\n",
    "    if not overlaps:\n",
    "        return 0.3, \"Insufficient overlapping vocabulary\"\n",
    "    avg_overlap = float(np.mean(overlaps))\n",
    "    if avg_overlap > 0.4:\n",
    "        return 1.0, f\"Good coherence (avg overlap={avg_overlap:.2f})\"\n",
    "    if avg_overlap > 0.2:\n",
    "        return 0.5, f\"Partial coherence (avg overlap={avg_overlap:.2f})\"\n",
    "    return -0.5, f\"Low coherence (avg overlap={avg_overlap:.2f})\"\n",
    "\n",
    "\n",
    "def compute_reward(\n",
    "    text: str,\n",
    "    *,\n",
    "    sentiment_analyzer=None,\n",
    "    tool=None,\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "    detailed: bool = False,\n",
    ") -> Tuple[float, Dict[str, float]] | float:\n",
    "    \"\"\"Compute composite reward for a generated email.\"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    lower = text.lower()\n",
    "    w = weights or default_reward_weights()\n",
    "\n",
    "    # Length\n",
    "    length = len(text)\n",
    "    if 100 <= length <= 300:\n",
    "        length_r = 1.0\n",
    "        length_reason = f\"Ideal length ({length} chars)\"\n",
    "    elif 300 < length <= 450:\n",
    "        length_r = 0.2\n",
    "        length_reason = f\"Slightly long ({length} chars)\"\n",
    "    elif length > 450:\n",
    "        length_r = -0.7\n",
    "        length_reason = f\"Too long ({length} chars)\"\n",
    "    else:\n",
    "        length_r = -0.7\n",
    "        length_reason = f\"Too short ({length} chars)\"\n",
    "\n",
    "    # Politeness\n",
    "    polite_hits = [term for term in POLITE_TERMS if re.search(rf\"\\b{re.escape(term)}\\b\", lower)]\n",
    "    polite_r = 0.5 * len(polite_hits)\n",
    "    polite_reason = (\n",
    "        f\"Polite terms: {', '.join(polite_hits)}\"\n",
    "        if polite_hits\n",
    "        else \"No polite phrasing detected\"\n",
    "    )\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment_r = 0.0\n",
    "    sentiment_reason = \"Sentiment analysis skipped\"\n",
    "    if sentiment_analyzer is not None:\n",
    "        try:\n",
    "            output = sentiment_analyzer(text[:512])\n",
    "            if output and isinstance(output, list):\n",
    "                label = output[0].get(\"label\", \"\").upper()\n",
    "                score = output[0].get(\"score\", 0.0)\n",
    "                if label.startswith(\"POS\"):\n",
    "                    sentiment_r = 1.0\n",
    "                    sentiment_reason = f\"Positive tone (score={score:.2f})\"\n",
    "                elif label.startswith(\"NEG\"):\n",
    "                    sentiment_r = -0.3\n",
    "                    sentiment_reason = f\"Negative tone (score={score:.2f})\"\n",
    "                else:\n",
    "                    sentiment_reason = f\"Neutral tone (score={score:.2f})\"\n",
    "        except Exception as exc:  # pragma: no cover\n",
    "            sentiment_reason = f\"Sentiment analysis failed: {exc}\"\n",
    "\n",
    "    # Clarity\n",
    "    unclear_hits = [term for term in UNCLEAR_TERMS if re.search(rf\"\\b{term}\\b\", lower)]\n",
    "    clarity_r = -0.3 * len(unclear_hits) if unclear_hits else 0.7\n",
    "    clarity_reason = (\n",
    "        f\"Unclear buzzwords: {', '.join(unclear_hits)}\"\n",
    "        if unclear_hits\n",
    "        else \"Clear, accessible language\"\n",
    "    )\n",
    "\n",
    "    # CTA\n",
    "    has_cta, cta_hits = has_cta_phrase(text)\n",
    "    cta_r = 1.0 if has_cta else -0.8\n",
    "    cta_reason = (\n",
    "        f\"CTA detected: {', '.join(cta_hits)}\"\n",
    "        if has_cta\n",
    "        else \"Missing explicit call-to-action\"\n",
    "    )\n",
    "\n",
    "    # Personalization\n",
    "    personalization_terms = [\"you\", \"your team\", \"your company\", \"dear\", \"hello\"]\n",
    "    personalized = any(term in lower for term in personalization_terms)\n",
    "    personalization_r = 0.7 if personalized else -0.2\n",
    "    personalization_reason = (\n",
    "        \"Personalized tone\"\n",
    "        if personalized\n",
    "        else \"No personalization markers\"\n",
    "    )\n",
    "\n",
    "    # Grammar (optional)\n",
    "    grammar_r = 0.0\n",
    "    grammar_reason = \"Grammar check skipped\"\n",
    "    if tool is not None:\n",
    "        try:\n",
    "            matches = tool.check(text)\n",
    "            n_errors = len(matches)\n",
    "            if n_errors == 0:\n",
    "                grammar_r = 1.0\n",
    "                grammar_reason = \"No grammar issues\"\n",
    "            elif n_errors < 4:\n",
    "                grammar_r = 0.5\n",
    "                grammar_reason = f\"Minor grammar issues ({n_errors})\"\n",
    "            else:\n",
    "                grammar_r = -0.3\n",
    "                grammar_reason = f\"Significant grammar issues ({n_errors})\"\n",
    "        except Exception as exc:  # pragma: no cover\n",
    "            grammar_reason = f\"Grammar check failed: {exc}\"\n",
    "\n",
    "    # Value proposition\n",
    "    value_hits = VALUE_TERMS_PATTERN.findall(text)\n",
    "    value_r = min(len(value_hits) * 0.5, 1.0)\n",
    "    value_reason = (\n",
    "        f\"Value props detected: {', '.join(set(map(str.lower, value_hits)))}\"\n",
    "        if value_hits\n",
    "        else \"No value proposition terms\"\n",
    "    )\n",
    "\n",
    "    # Spam avoidance\n",
    "    spam_hits = [term for term in SPAM_TERMS if re.search(rf\"\\b{term}\\b\", lower)]\n",
    "    spam_r = -0.8 * len(spam_hits) if spam_hits else 0.5\n",
    "    spam_reason = (\n",
    "        f\"Spammy terms present: {', '.join(spam_hits)}\"\n",
    "        if spam_hits\n",
    "        else \"No spam terms detected\"\n",
    "    )\n",
    "\n",
    "    # Structure\n",
    "    structure_r, structure_reason = evaluate_structure_and_intent(text)\n",
    "\n",
    "    # Instructional tone penalty\n",
    "    instruction_r, instruction_reason = penalize_instructional_tone(text)\n",
    "\n",
    "    # Lexical coherence\n",
    "    coherence_r, coherence_reason = lexical_coherence(text)\n",
    "\n",
    "    total_reward = (\n",
    "        w[\"length\"] * length_r\n",
    "        + w[\"politeness\"] * polite_r\n",
    "        + w[\"sentiment\"] * sentiment_r\n",
    "        + w[\"clarity\"] * clarity_r\n",
    "        + w[\"cta\"] * cta_r\n",
    "        + w[\"personalization\"] * personalization_r\n",
    "        + w[\"grammar\"] * grammar_r\n",
    "        + w[\"value\"] * value_r\n",
    "        + w[\"spam\"] * spam_r\n",
    "        + w[\"structure\"] * structure_r\n",
    "        + w[\"instructional_tone\"] * instruction_r\n",
    "        + w[\"lexical_coherence\"] * coherence_r\n",
    "    )\n",
    "    total_reward = float(np.clip(total_reward, -4.0, 6.0))\n",
    "\n",
    "    if not detailed:\n",
    "        return total_reward\n",
    "\n",
    "    breakdown = {\n",
    "        \"length\": length_r,\n",
    "        \"politeness\": polite_r,\n",
    "        \"sentiment\": sentiment_r,\n",
    "        \"clarity\": clarity_r,\n",
    "        \"cta\": cta_r,\n",
    "        \"personalization\": personalization_r,\n",
    "        \"grammar\": grammar_r,\n",
    "        \"value\": value_r,\n",
    "        \"spam\": spam_r,\n",
    "        \"structure\": structure_r,\n",
    "        \"instructional_tone\": instruction_r,\n",
    "        \"lexical_coherence\": coherence_r,\n",
    "        \"reasons\": {\n",
    "            \"length\": length_reason,\n",
    "            \"politeness\": polite_reason,\n",
    "            \"sentiment\": sentiment_reason,\n",
    "            \"clarity\": clarity_reason,\n",
    "            \"cta\": cta_reason,\n",
    "            \"personalization\": personalization_reason,\n",
    "            \"grammar\": grammar_reason,\n",
    "            \"value\": value_reason,\n",
    "            \"spam\": spam_reason,\n",
    "            \"structure\": structure_reason,\n",
    "            \"instructional_tone\": instruction_reason,\n",
    "            \"lexical_coherence\": coherence_reason,\n",
    "        },\n",
    "    }\n",
    "    return total_reward, breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "12d5fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_queries(input_ids: torch.Tensor, attention_mask: torch.Tensor) -> List[torch.Tensor]:\n",
    "    prompt_lengths = attention_mask.sum(dim=1)\n",
    "    queries: List[torch.Tensor] = []\n",
    "    for row, length in zip(input_ids, prompt_lengths):\n",
    "        length_int = int(length.item())\n",
    "        start_idx = row.size(0) - length_int  # account for left padding\n",
    "        queries.append(row[start_idx:].clone())\n",
    "    return queries\n",
    "\n",
    "def slice_responses(\n",
    "    generated_ids: torch.Tensor,\n",
    "    prompt_token_count: int,\n",
    "    prompt_lengths: Sequence[int],\n",
    ") -> List[torch.Tensor]:\n",
    "    responses: List[torch.Tensor] = []\n",
    "    for idx in range(generated_ids.size(0)):\n",
    "        start = prompt_token_count\n",
    "        resp_ids = generated_ids[idx, start:]\n",
    "        if resp_ids.numel() == 0:\n",
    "            resp_ids = generated_ids.new_tensor([generated_ids[idx, -1].item()])\n",
    "        responses.append(resp_ids)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def run_ppo_training(\n",
    "    device: str,\n",
    "    top_p: float,\n",
    "    temperature: float,\n",
    "    max_new_tokens: int,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    max_length: int,\n",
    "    reward_weights: Dict[str, float],\n",
    "    save_model: bool,\n",
    "    output_dir: Path,\n",
    "    model: AutoModelForCausalLMWithValueHead,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    trainer: PPOTrainer,\n",
    "    prompts: Sequence[str],\n",
    "    sentiment_analyzer,\n",
    "    grammar_tool,\n",
    ") -> None:\n",
    "    device = torch.device(device)\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=pad_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting PPO fine-tuning for %d epochs\", num_epochs)\n",
    "    logging.info(\"Using batch size: %d\", batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(\"Epoch %d/%d\", epoch + 1, num_epochs)\n",
    "        progress = tqdm(range(0, len(prompts), batch_size), desc=f\"epoch {epoch+1}\")\n",
    "        for idx in progress:\n",
    "            batch_prompts = list(prompts[idx : idx + batch_size])\n",
    "            print(f\"number of prompts in a batch - {len(batch_prompts)}\")\n",
    "            enc = tokenizer(\n",
    "                batch_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            input_ids = enc.input_ids.to(device)\n",
    "            print(f\"shape of input_ids - {input_ids.shape}\")\n",
    "            attention_mask = enc.attention_mask.to(device)\n",
    "            print(f\"shape of attention_mask - {attention_mask.shape}\")\n",
    "\n",
    "            queries = prepare_queries(input_ids, attention_mask)\n",
    "            print(f\"queries shape: {[query.shape for query in queries]}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **gen_kwargs,\n",
    "                )\n",
    "            print(f\"shape of generated: {generated.shape}\")\n",
    "            prompt_token_count = input_ids.size(1)\n",
    "            print(f\"prompt_token_count: {prompt_token_count}\")\n",
    "            prompt_lengths = attention_mask.sum(dim=1).tolist()\n",
    "            print(f\"prompt_lengths: {prompt_lengths}\")\n",
    "            responses = slice_responses(generated, prompt_token_count, prompt_lengths)\n",
    "            #print(f\"responses: {responses}\")\n",
    "            print(f\"shape of responses: {[resp.shape for resp in responses]}\")\n",
    "            response_texts = [\n",
    "                tokenizer.decode(resp, skip_special_tokens=True) for resp in responses\n",
    "            ]\n",
    "            print(f\"response_texts: {response_texts}\")\n",
    "\n",
    "            rewards: List[torch.Tensor] = []\n",
    "            for text, resp_ids in zip(response_texts, responses):\n",
    "                total_reward = compute_reward(\n",
    "                    text,\n",
    "                    sentiment_analyzer=sentiment_analyzer,\n",
    "                    tool=grammar_tool,\n",
    "                    weights=reward_weights,\n",
    "                )\n",
    "                reward_tensor = torch.tensor(\n",
    "                    float(total_reward),\n",
    "                    device=device,\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "                rewards.append(reward_tensor)\n",
    "\n",
    "            print({\"query\": batch_prompts, \"response\": response_texts})\n",
    "            print(f\"queries shape: {[query.shape for query in queries]}\")\n",
    "            print(f\"responses shape: {[resp.shape for resp in responses]}\")\n",
    "            print(f\"rewards shape: {[reward.shape for reward in rewards]}\")\n",
    "            stats = trainer.step(queries, responses, rewards)\n",
    "\n",
    "            rewards_for_log = torch.stack(rewards).to(device)\n",
    "            batch_log = {\"query\": batch_prompts, \"response\": response_texts}\n",
    "            print({\"query\": batch_prompts, \"response\": response_texts})\n",
    "            trainer.log_stats(stats, batch_log, rewards_for_log)\n",
    "\n",
    "        logger.info(\"Completed epoch %d\", epoch + 1)\n",
    "\n",
    "    if save_model:\n",
    "        save_dir = output_dir / \"ppo_gpt2_epoch2_batch8\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(\"Saving fine-tuned model to %s\", save_dir)\n",
    "        trainer.model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fdb3b618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b0cb1e919244dba66bfeb5043e3180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of prompts in a batch - 8\n",
      "shape of input_ids - torch.Size([8, 36])\n",
      "shape of attention_mask - torch.Size([8, 36])\n",
      "queries shape: [torch.Size([27]), torch.Size([36]), torch.Size([17]), torch.Size([33]), torch.Size([27]), torch.Size([25]), torch.Size([20]), torch.Size([33])]\n",
      "shape of generated: torch.Size([8, 804])\n",
      "prompt_token_count: 36\n",
      "prompt_lengths: [27, 36, 17, 33, 27, 25, 20, 33]\n",
      "shape of responses: [torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768])]\n",
      "response_texts: ['\\n\\n\\nSellout: 1% of 1% of the 1% by 1.\\n\\n\\nSaleoff: 1% of 1% by 1.\\n\\n\\nSale: 1 of 1% by 1.\\n\\n\\nSale: 10% 10%\\n\\n\\nSale: 1% of 1% of 1% by 10%\\n\\n\\nSale: 1% of 1% by 10%\\n\\n\\nSale: 1% by 1% by 10%\\n\\n\\nPayscale, by market share of the\\n\\nSale as used by the seller, by the selling buyer,\\n\\nSale, buyer of\\n\\n\\nSale, seller of\\n\\n\\nA sales, a,\\n\\n\\nSale: 1% of 1% of 1% by 1.\\n\\n\\nSale: 1% of 1% by 1.\\n\\n\\nSale: 1% of 1% by 1.\\n\\n\\nSale:1% of 1% by 1.\\n\\n\\nSale:1 per cent by 1.\\n\\n\\nSale:1% by 1% by 1.\\n\\n\\nSale: 1% by 0.0% by 0.0%,1.\\n\\n\\nSale1,1,1,1,1,1,1,1:1.\\n\\n1,1,1,1,1,1,1,1,1,1:1!\\n\\n1 and 1,1,1,1,1,1,1.\\n\\n1,1,1,1,1,1,1,1:1.\\n\\n1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,Payscale, by market share of the\\n\\n\\n2% of 1%\\n\\nSale: 1% of 1% by 1%\\n\\n\\nSale: 1% of 1% by 1% by 1. 1.\\n\\nSales:1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1 to 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.\\n1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.\\n1,1,1,1,1,1,1,1,1,1,1,,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1', '\\n\\nIn this study, we found that an increase in the number of healthcare healthcare workers increased the total amount of pay for employees who provide healthcare services, and that the number of people who work as employees rose by an average of $8.2 million annually.\\n\\nOne of the main reasons for the increase was the cost of healthcare service to services for more employees to receive a higher increase in health care to workers. For the cost of health care, the average employees and the total amount of health care workers pay each member is $4.\\n\\nOur research found that the number of healthcare workers to pay for pay increases of the total amount of pay to the employees per employees which increased the total amount of the total amount pay of the employees.\\n\\n\\nThe number of the healthcare workers and the number of total amount of employees to pay each of the employees is $4.\\n\\nThe number of employees to pay each of the total amount of pay of the employees is $4.\\n\\nThe company that was the number of the employees to pay each of the total amount of the employees to pay each of them the amount amount of pay of each employee.\\n\\nThe number of employees to pay each of the total amount of both employees in the total amount of each employees to pay to each of them.\\n\\nThe number of employees to pay each of the employees to pay each of the total amount of each employees to pay each of each one-person employees employees total amount of total amount pay the total amount to the employees.\\n\\nThe total amount amount pay to employees total amount of all employees to pay each employees total amount pay of total amount to each employees to pay each of total amount pay of total amount amount.\\n\\nTotal amount number of people to pay employees employees total amount amount employees total amount amount of paid to employees, total amount amount total amount pay amount to total amount pay them out to pay the amount. total amount amount amount amount amount the amount pay of employees salaries amount amount amount total amount. total amount amount amount amount amount number of employees to pay, total amount pay the number of employees, total amount amount to total amount pay of employees to pay each of employees pay total amount amount of each and amount amount paid to total amount in total amount amount the amount amount amount amount amount pay the amount amount amount amount amount to the total amount amount amount amount amount amount. total amount amount amount total amount pay amount amount amount amount amount amount amount to total amount amount of employees amounts amount total amount amount amount amount total amount amount amount amount amount amount amount amount the amount amount amount amount the amount amount amount amount amount amount amount amount amount amount total amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount to total amount amount amount amount total amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount the amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount total amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount number amount amount amount amount amount amount amount amount amount amounts amount amount amount amount amount amount amount number to the amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount for amount amount amount amount amount amount', '\\n\\nTo enable the application to be enabled, the application must be the same as the client\\'s data. (e.g. customer\\'s data).\\n\\nIf the database is a large database, if the app is \"full\" or even \"full,\" the application has an issue.\\n\\nIf the application contains a few requests, the app is only the requests of the first two requests would be fulfilled.\\nThis is the case of the application being turned into a test case. The number of requests you can be allowed in the app is in the app in test is the app.\\n\\nThere is a second option.\\nThe server can test with the first and second arguments which are the \"client\\'s data\". The first is the new request with the first argument and second argument being the first and second arguments.\\n\\nThis is the case of the client app. the first the second arguments which is the first and second the second arguments and second second the second arguments.\\n\\nThe second argument is the second of the the first, second and the second.\\n\\nThe second, the second the second and second.\\nA second argument is the first argument of the second argument of the first argument first the second.\\n\\nThis is not only for the case of the client. the second argument is the first, second, second, the second second.\\n\\nthe second second the second second second second.\\n\\nThis is the case of the server which is the first the second the second one, the second.\\n\\nThe second argument is the second argument a the second one the second.\\n\\n\\na second the second second first the second two the second.\\n\\nThis is the case of the second argument of the first, thesecond the second.\\n\\nor the second in the second (or of the second arguments).\\n\\n\\nThe test of theclient is the same as the case of the server.\\n. It is for the that the first first argument.\\n\\n\\nNow the second second argument is the second second for the first, second second of the second.\\n\\n\\nthe first and the second and the second are the first and second two and the second one.\\n\\nThe first argument is the second argument for the first, the second second argument the one second it the second.\\n\\nThe second argument is the first the second argument, thesecond the the second (also the second argument.\\n\\nthe second argument the first and second are the first and second arguments).\\n\\nThe third argument are is the first of the second arguments. The second argument is the third argument of the second arguments.\\n\\nthe second one.\\n\\nThe third argument is the second argument of the first first second it the second and second arguments.\\n\\nThe third argument is the second argument.\\n\\nThe first argument is the first argument, that the first the second, the second and second argument.\\n\\nThis is where the second argument is the second argument.\\n\\nis the second argument, that the second, the second it the three, the the fourth argument.\\n\\nThe second the second argument of this, to the second first the second argument. the second arguments as the one the one.\\n\\nThe first argument is the second argument. the second argument. in the first argument is the.\\n\\nis the second argument; for the first in the first, the second, the third and second the first and second are the second and third.\\n\\nThe second two argument is the the second argument. the first argument and the second, the other.\\n\\nThe second argument is the second kind the second and the first\\'s is the second or the second argument.\\n\\nThe first argument is an argument. The second argument', '\\nA few questions\\n\\n\\n#1. What are the average prices on a new book on a new product?\\n\\n\\n#1.0: The average price for new book is a higher value for the product at a market. It\\'s a simple to check that it\\'s the value when you compare it to the average price of a new book.\\n\\n#1.1: new book a new book of new book 1: new book number,\\n\\n2.\\n\\n1.\\n# 2. 2. 2. 2. 2. 1. 2. 3: new book of new book new bookbookbookbookbook\\n\\n #2: most common way for getting the value of the new book is to send a new book.\\n\\n#1. The number of new book book #2: a new book of new book which is an average price of new book.\\n\\n#2: the average price of new book #2: the prices of new book prices of book book that is to new for the book (NewBook:book,seller,sell)\\n\\n#1.\\n\\n#1 :\\n\\n#1: the prices of new book,seller,seller,sell,buy,\\n\\n[1:] The average price of each new book in a book #2: the price of the new book.\\n\\n#2:the average price of new book :\\n\\n: the average price for new bookseller,seller,sell\\n\\n#2 :\\n\\n#1:\\n\\n#2 :\\n\\n#1: the average price of new book,\\n\\n#2: the average price of new book,\\n\\n#1: the average price,\\n\\n\"The average price of new book is the lowest number of new booksellers.\\n\\nthe average price of new books like new book,\\n\\n1.\\n\\n(booksell)\\n\\nThe average price of new book,\\n\\n: the price of new book book(Book)\\n\\nor number of new booksellers,\\n\\nnew bookbook.\\n\\nthe average price for new book,\\n\\nnumber of booksellers,\\n\\nprice,\\n\\n\\nnumber of new book booksellers,\\n\\nbookbook.\\n\\nin the number of books of new book,\\n\\none, books,\\n\\n\\n\"The average price of new book is the lowest number ofbooksellers.\\n\\n\\n\"The average price of new book is the low price of new bookseller,\\n\\nbookbook.\\n\\n\" the price of new book 1, books that are not cheap and more than 4 years.\\n\\n\" (booksell_book,seller.\\n\\n,\\n\\n[1 and #1: the average price of new book from the newbook.\\n\\n#1: the average price of new book.\\n\\n,#2: the average price of new book,\\n\\n#1: the average price of new book\\n\\n\\n the average price with the average,\\n\\n.\\n\\n\\n$\\n\\n#1: the average price for the prices of different products.\\n\\nThe price,\\n\\n1.\\n\\n. The average price of new book,\\n\\n2.\\n\\n#1,1.\\n\\n1.\\n\\nthe average price of new book.\\n\\nthe average price,\\n\\n(bookbook. number\\n\\n\\nbookbook,\\nbookbook book,\\n\\nbookbookbookbook,\\n\\n,\\nbookbook,\\n\\nbookbookbookbook,\\n\\nbookbookbookbook,\\n\\n\\nand\\nbookbookbook,\\n\\nbookbookbookbookbook,bookbookbook,\\n\\nbookbook', ' at the lowest-cost. You may have to add \"Dell Sales\" to this list.\\n\\n\"Dell Sales\" is not just another generic words \"DellSales\", an word: \"DellSales\". You may have to add \"Dell Sales\" of this list. \"DellSales\" is one of the most used examples of \"DellSales\", one of the most used word: \"dull Sales (magnicated)\" : \"du-l-jn-v-q-h\", \"du-l-v-n-q-h\" \"du-vn-m-n-n-q-h\" \"du-v-q-m-q-n\" \"du-v-v-q-m-n-n-n-n\" \"du-q-mu-q-n-n-n-n-n-n-n\" \"du-l-jn-q-qn-r-n-n\" \"du-l-v-q-q-r-n\" \"du-l-q-q-r-n\" \"du-n-q-q-r-q-r-n\" \"du-l-v-q-q-r-n-n\" \"du-r-q-r-r\" \"du-s-r-q-r\" \"du-s-v-q-r-r\" \"du-v-s-r-q-r-r\" \"du-s-v-q-r-r\" \"du-v-q-r-r\" \"du-r-q-r-r\" \"du-v-v-q-s-r-r\" \"du-v-q-r-r-r\" \"du-v-s-r-q-r-r-s\" \"du-s-r-r-s-r\" \"du-v-q-r-r-r-r\" \"du-s-v-v-v-v-r\" \"du-v-v-v-v-v-r\" \"du-v-v-vr-v-v-r-vr\" \"du-v-v-v-v-r-v-r\" \"du-v-vr-q-r-r-\" \"du-v-v-q-r-r-r-\\n\\n\"du-v-v-r-r-r-s-r-r-r-r-nq \"du-v-v-s-r-p-r-r-nq-r-r\" \"du-v-v-v-n-n-n-n-q-nq-r-p-rn-r-p-r-r\"\\n\\ndu-v-v-v-q-r-r-r-n-nq-r-r-r\\n\\ndu-v-v-v-p-r-p-r-r-p\\n\\ndu-v-v-p-r-p-r-r-r\\n\\ndu-v-v-p-r-n\\n\\ndu-v-v-p-r\\n\\ndu-v-v-p-r-r-n \"du-v-r-r-r-r-r\\n\\ndu-v-v-v-r-n-r\\n\\ndu-v-v-v-r-r-r-', '\\n\\nCustomers interested in selling product to customers, e-design offers that allow customers to turn to them on an internet service or online in person with the intention to convert to another business or to the market based on market conditions or market trends.\\n\\ne-web, E-form, e-form, e-mail, e-form, e-mail, e-form, e-form-in, e-form, e-form-in, e-form, e-form, e-form, e-form, e-form,, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-in, e-form, e-form, e-form, e-form, e-form, e-form, e-in, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form,e-form, e-form, e-form, e-form,e-form,., e-form, e-form,e-form, e-form, e-form, e-form, e-form,e-form, e-form, e-form, e-form, e-form, e-form,e-form, e-form, e-form., e-form, e-form,e-form, e-form,e-form,e-form, e-form,e-form,e-form, e-form,e-form,e-form, e-form, e-form, e-form,e-form, e-form,e-form, e-form, e-form,e-form, e-form, e-form,e-form, e-form, E-form, e-forms, form,e-form, e-form,e-form,.e-form, e-form,e-form, e-form,e-form, e-form, e-form, e-e,form, e-form,e-form,e-form, e-form,e,form, e-form,e-form,e-form, e-form,e-form,e-form,i-form, e-form.eform,e-form, form-form,e-form,e-form-form,e-form,e-form-form,form,e-form,e-form,form,form,e-form, e-form,-form, e-form,form,e-form,e-form,e-form,e-form,e-form,e-form,e-form,e-form, e-form, e-form,form,e-form,e-form e-form,e-form, -form,e-form, \"e,form,e-form,form,e-form.e-form,e-form,,e-form,e-form,e-form,form,e-form,e-form.e-form,e-form-form,form', \"\\n\\nYou're a professional marketing person with a product in mind that will help you.\\n\\nYou are an experienced marketing manager with a product in your field.\\n\\nYou got an idea for a product, that you want for the idea in.\\n\\nYou would like in an a- product that are there as you will have the product in your field.\\n\\nYou are being promoted with product that you are to the from your field.\\n\\n your marketing\\n\\n\\nYour mission this mission is the to you.\\n\\nYou.\", '\\n\\nThis product is an \"unified\" model that is a test of many different different solutions and that is what you want to get with it.\\nHow to design your unique B2B marketing idea with this way, there is an application that is a good way to get the most different. in this way, you are able to optimize your marketing and other products for the most different platform, in the same way. This product, is the next. It is based on in the idea, this way, or your own design, to generate a new, the new. the product, that, the unique to create. or, or to create, or for you to create, to add to your current product. to that, there your solutions.\\n\\nHow to analyze your product from the first one, like, it, you, it, it- it. what part of product or to the first one? The way to analyze the one\\'s to the first one. you a way of dealing the one, like. the most, the most, to a of it in the place. what the the most, are the most are one this.. you to the best. for. and on. it is. the best, because it\\'s for some one, that this. it, to.\\n\\nThe best is the most. the most, it to the best. it. ( the best, what\\'s the best, that\\'s the best,, is the best, the most. the most, the thebest, to the best. the best, the best, the best, the, the most. the best, is the most, the most, themost. the best, to the best.the most. the best, the best, the most, the, or the only. the, it. the. the best, to the best. the most, the best, it, this. the, the. the, is. the. the, the, the, to. the, the. the, the, the, the,, the, the. the, it, a to the, the. to, the next, it, it. this, it, the. of, it, the. it. the, it. the, an. it, it. to. it - the, to. like, it,, the, it. the, it. the, for, to the, the, like, be. one I, it, the. to, is, the. for. it, the, to, an. for, its, the, the. it.- there, it, in and a. at. there,, the, the, to. the. in, it. as, to. to, part, it,. in part. there, it, it. of it. it, it. for. to, the, this,. to, it, the,,. it, part. the, the, to, part., you, it. for, it. to, or, to, an, to, or to, to. to, it.it. is, this, to, it.. it,. and, it, to, to, it. that, it. there, the, it., the. the. (to. to, to. to, it., it.. one, that. for, to.. the. in, the. of, it. on. to, it.. a, the. the. of. as. it. of, the to.. it. it., the, to. it, part,it. in. i, it\\'s']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool latest: 100%|██████████| 254M/254M [00:15<00:00, 16.5MB/s]\n",
      "Unzipping C:\\Users\\MOIDHA~1\\AppData\\Local\\Temp\\tmp97jof6mz.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n",
      "Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to C:\\Users\\moidhassan\\.cache\\language_tool_python.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['Compose a friendly B2B sales email based on this concept: \"Biodegradable packaging: lower footprint, same cost.\"', 'Generate a product outreach email using this information: \"Our corporate wellness programs have helped over 200 companies reduce healthcare costs by an average of $450 per employee annually while boosting morale.\"', 'Write an enterprise sales email centered on: \"Learning platform lifts test scores 18%.\"', 'Write an enterprise sales email centered on: \"Properties using our platform have seen an average 23% increase in positive reviews and a significant boost in repeat bookings.\"', 'Write an enterprise sales email centered on: \"Cut cloud spend 30–40%. 15‑min chat next week? – DataSync\"', 'Craft a formal product introduction email about: \"Our clients typically see 15-20% increase in conversion rates within 90 days.\"', 'Craft a formal product introduction email about: \"Logistics costs down 20–30%. Explore fulfillment?\"', 'Compose a friendly B2B sales email based on this concept: \"We specialize in cloud infrastructure optimization that reduces costs by 30-40% while improving performance.\"'], 'response': ['\\n\\n\\nSellout: 1% of 1% of the 1% by 1.\\n\\n\\nSaleoff: 1% of 1% by 1.\\n\\n\\nSale: 1 of 1% by 1.\\n\\n\\nSale: 10% 10%\\n\\n\\nSale: 1% of 1% of 1% by 10%\\n\\n\\nSale: 1% of 1% by 10%\\n\\n\\nSale: 1% by 1% by 10%\\n\\n\\nPayscale, by market share of the\\n\\nSale as used by the seller, by the selling buyer,\\n\\nSale, buyer of\\n\\n\\nSale, seller of\\n\\n\\nA sales, a,\\n\\n\\nSale: 1% of 1% of 1% by 1.\\n\\n\\nSale: 1% of 1% by 1.\\n\\n\\nSale: 1% of 1% by 1.\\n\\n\\nSale:1% of 1% by 1.\\n\\n\\nSale:1 per cent by 1.\\n\\n\\nSale:1% by 1% by 1.\\n\\n\\nSale: 1% by 0.0% by 0.0%,1.\\n\\n\\nSale1,1,1,1,1,1,1,1:1.\\n\\n1,1,1,1,1,1,1,1,1,1:1!\\n\\n1 and 1,1,1,1,1,1,1.\\n\\n1,1,1,1,1,1,1,1:1.\\n\\n1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,Payscale, by market share of the\\n\\n\\n2% of 1%\\n\\nSale: 1% of 1% by 1%\\n\\n\\nSale: 1% of 1% by 1% by 1. 1.\\n\\nSales:1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1 to 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.\\n1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.\\n1,1,1,1,1,1,1,1,1,1,1,,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1', '\\n\\nIn this study, we found that an increase in the number of healthcare healthcare workers increased the total amount of pay for employees who provide healthcare services, and that the number of people who work as employees rose by an average of $8.2 million annually.\\n\\nOne of the main reasons for the increase was the cost of healthcare service to services for more employees to receive a higher increase in health care to workers. For the cost of health care, the average employees and the total amount of health care workers pay each member is $4.\\n\\nOur research found that the number of healthcare workers to pay for pay increases of the total amount of pay to the employees per employees which increased the total amount of the total amount pay of the employees.\\n\\n\\nThe number of the healthcare workers and the number of total amount of employees to pay each of the employees is $4.\\n\\nThe number of employees to pay each of the total amount of pay of the employees is $4.\\n\\nThe company that was the number of the employees to pay each of the total amount of the employees to pay each of them the amount amount of pay of each employee.\\n\\nThe number of employees to pay each of the total amount of both employees in the total amount of each employees to pay to each of them.\\n\\nThe number of employees to pay each of the employees to pay each of the total amount of each employees to pay each of each one-person employees employees total amount of total amount pay the total amount to the employees.\\n\\nThe total amount amount pay to employees total amount of all employees to pay each employees total amount pay of total amount to each employees to pay each of total amount pay of total amount amount.\\n\\nTotal amount number of people to pay employees employees total amount amount employees total amount amount of paid to employees, total amount amount total amount pay amount to total amount pay them out to pay the amount. total amount amount amount amount amount the amount pay of employees salaries amount amount amount total amount. total amount amount amount amount amount number of employees to pay, total amount pay the number of employees, total amount amount to total amount pay of employees to pay each of employees pay total amount amount of each and amount amount paid to total amount in total amount amount the amount amount amount amount amount pay the amount amount amount amount amount to the total amount amount amount amount amount amount. total amount amount amount total amount pay amount amount amount amount amount amount amount to total amount amount of employees amounts amount total amount amount amount amount total amount amount amount amount amount amount amount amount the amount amount amount amount the amount amount amount amount amount amount amount amount amount amount total amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount to total amount amount amount amount total amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount the amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount total amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount number amount amount amount amount amount amount amount amount amount amounts amount amount amount amount amount amount amount number to the amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount amount for amount amount amount amount amount amount', '\\n\\nTo enable the application to be enabled, the application must be the same as the client\\'s data. (e.g. customer\\'s data).\\n\\nIf the database is a large database, if the app is \"full\" or even \"full,\" the application has an issue.\\n\\nIf the application contains a few requests, the app is only the requests of the first two requests would be fulfilled.\\nThis is the case of the application being turned into a test case. The number of requests you can be allowed in the app is in the app in test is the app.\\n\\nThere is a second option.\\nThe server can test with the first and second arguments which are the \"client\\'s data\". The first is the new request with the first argument and second argument being the first and second arguments.\\n\\nThis is the case of the client app. the first the second arguments which is the first and second the second arguments and second second the second arguments.\\n\\nThe second argument is the second of the the first, second and the second.\\n\\nThe second, the second the second and second.\\nA second argument is the first argument of the second argument of the first argument first the second.\\n\\nThis is not only for the case of the client. the second argument is the first, second, second, the second second.\\n\\nthe second second the second second second second.\\n\\nThis is the case of the server which is the first the second the second one, the second.\\n\\nThe second argument is the second argument a the second one the second.\\n\\n\\na second the second second first the second two the second.\\n\\nThis is the case of the second argument of the first, thesecond the second.\\n\\nor the second in the second (or of the second arguments).\\n\\n\\nThe test of theclient is the same as the case of the server.\\n. It is for the that the first first argument.\\n\\n\\nNow the second second argument is the second second for the first, second second of the second.\\n\\n\\nthe first and the second and the second are the first and second two and the second one.\\n\\nThe first argument is the second argument for the first, the second second argument the one second it the second.\\n\\nThe second argument is the first the second argument, thesecond the the second (also the second argument.\\n\\nthe second argument the first and second are the first and second arguments).\\n\\nThe third argument are is the first of the second arguments. The second argument is the third argument of the second arguments.\\n\\nthe second one.\\n\\nThe third argument is the second argument of the first first second it the second and second arguments.\\n\\nThe third argument is the second argument.\\n\\nThe first argument is the first argument, that the first the second, the second and second argument.\\n\\nThis is where the second argument is the second argument.\\n\\nis the second argument, that the second, the second it the three, the the fourth argument.\\n\\nThe second the second argument of this, to the second first the second argument. the second arguments as the one the one.\\n\\nThe first argument is the second argument. the second argument. in the first argument is the.\\n\\nis the second argument; for the first in the first, the second, the third and second the first and second are the second and third.\\n\\nThe second two argument is the the second argument. the first argument and the second, the other.\\n\\nThe second argument is the second kind the second and the first\\'s is the second or the second argument.\\n\\nThe first argument is an argument. The second argument', '\\nA few questions\\n\\n\\n#1. What are the average prices on a new book on a new product?\\n\\n\\n#1.0: The average price for new book is a higher value for the product at a market. It\\'s a simple to check that it\\'s the value when you compare it to the average price of a new book.\\n\\n#1.1: new book a new book of new book 1: new book number,\\n\\n2.\\n\\n1.\\n# 2. 2. 2. 2. 2. 1. 2. 3: new book of new book new bookbookbookbookbook\\n\\n #2: most common way for getting the value of the new book is to send a new book.\\n\\n#1. The number of new book book #2: a new book of new book which is an average price of new book.\\n\\n#2: the average price of new book #2: the prices of new book prices of book book that is to new for the book (NewBook:book,seller,sell)\\n\\n#1.\\n\\n#1 :\\n\\n#1: the prices of new book,seller,seller,sell,buy,\\n\\n[1:] The average price of each new book in a book #2: the price of the new book.\\n\\n#2:the average price of new book :\\n\\n: the average price for new bookseller,seller,sell\\n\\n#2 :\\n\\n#1:\\n\\n#2 :\\n\\n#1: the average price of new book,\\n\\n#2: the average price of new book,\\n\\n#1: the average price,\\n\\n\"The average price of new book is the lowest number of new booksellers.\\n\\nthe average price of new books like new book,\\n\\n1.\\n\\n(booksell)\\n\\nThe average price of new book,\\n\\n: the price of new book book(Book)\\n\\nor number of new booksellers,\\n\\nnew bookbook.\\n\\nthe average price for new book,\\n\\nnumber of booksellers,\\n\\nprice,\\n\\n\\nnumber of new book booksellers,\\n\\nbookbook.\\n\\nin the number of books of new book,\\n\\none, books,\\n\\n\\n\"The average price of new book is the lowest number ofbooksellers.\\n\\n\\n\"The average price of new book is the low price of new bookseller,\\n\\nbookbook.\\n\\n\" the price of new book 1, books that are not cheap and more than 4 years.\\n\\n\" (booksell_book,seller.\\n\\n,\\n\\n[1 and #1: the average price of new book from the newbook.\\n\\n#1: the average price of new book.\\n\\n,#2: the average price of new book,\\n\\n#1: the average price of new book\\n\\n\\n the average price with the average,\\n\\n.\\n\\n\\n$\\n\\n#1: the average price for the prices of different products.\\n\\nThe price,\\n\\n1.\\n\\n. The average price of new book,\\n\\n2.\\n\\n#1,1.\\n\\n1.\\n\\nthe average price of new book.\\n\\nthe average price,\\n\\n(bookbook. number\\n\\n\\nbookbook,\\nbookbook book,\\n\\nbookbookbookbook,\\n\\n,\\nbookbook,\\n\\nbookbookbookbook,\\n\\nbookbookbookbook,\\n\\n\\nand\\nbookbookbook,\\n\\nbookbookbookbookbook,bookbookbook,\\n\\nbookbook', ' at the lowest-cost. You may have to add \"Dell Sales\" to this list.\\n\\n\"Dell Sales\" is not just another generic words \"DellSales\", an word: \"DellSales\". You may have to add \"Dell Sales\" of this list. \"DellSales\" is one of the most used examples of \"DellSales\", one of the most used word: \"dull Sales (magnicated)\" : \"du-l-jn-v-q-h\", \"du-l-v-n-q-h\" \"du-vn-m-n-n-q-h\" \"du-v-q-m-q-n\" \"du-v-v-q-m-n-n-n-n\" \"du-q-mu-q-n-n-n-n-n-n-n\" \"du-l-jn-q-qn-r-n-n\" \"du-l-v-q-q-r-n\" \"du-l-q-q-r-n\" \"du-n-q-q-r-q-r-n\" \"du-l-v-q-q-r-n-n\" \"du-r-q-r-r\" \"du-s-r-q-r\" \"du-s-v-q-r-r\" \"du-v-s-r-q-r-r\" \"du-s-v-q-r-r\" \"du-v-q-r-r\" \"du-r-q-r-r\" \"du-v-v-q-s-r-r\" \"du-v-q-r-r-r\" \"du-v-s-r-q-r-r-s\" \"du-s-r-r-s-r\" \"du-v-q-r-r-r-r\" \"du-s-v-v-v-v-r\" \"du-v-v-v-v-v-r\" \"du-v-v-vr-v-v-r-vr\" \"du-v-v-v-v-r-v-r\" \"du-v-vr-q-r-r-\" \"du-v-v-q-r-r-r-\\n\\n\"du-v-v-r-r-r-s-r-r-r-r-nq \"du-v-v-s-r-p-r-r-nq-r-r\" \"du-v-v-v-n-n-n-n-q-nq-r-p-rn-r-p-r-r\"\\n\\ndu-v-v-v-q-r-r-r-n-nq-r-r-r\\n\\ndu-v-v-v-p-r-p-r-r-p\\n\\ndu-v-v-p-r-p-r-r-r\\n\\ndu-v-v-p-r-n\\n\\ndu-v-v-p-r\\n\\ndu-v-v-p-r-r-n \"du-v-r-r-r-r-r\\n\\ndu-v-v-v-r-n-r\\n\\ndu-v-v-v-r-r-r-', '\\n\\nCustomers interested in selling product to customers, e-design offers that allow customers to turn to them on an internet service or online in person with the intention to convert to another business or to the market based on market conditions or market trends.\\n\\ne-web, E-form, e-form, e-mail, e-form, e-mail, e-form, e-form-in, e-form, e-form-in, e-form, e-form, e-form, e-form, e-form,, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-in, e-form, e-form, e-form, e-form, e-form, e-form, e-in, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form, e-form,e-form, e-form, e-form, e-form,e-form,., e-form, e-form,e-form, e-form, e-form, e-form, e-form,e-form, e-form, e-form, e-form, e-form, e-form,e-form, e-form, e-form., e-form, e-form,e-form, e-form,e-form,e-form, e-form,e-form,e-form, e-form,e-form,e-form, e-form, e-form, e-form,e-form, e-form,e-form, e-form, e-form,e-form, e-form, e-form,e-form, e-form, E-form, e-forms, form,e-form, e-form,e-form,.e-form, e-form,e-form, e-form,e-form, e-form, e-form, e-e,form, e-form,e-form,e-form, e-form,e,form, e-form,e-form,e-form, e-form,e-form,e-form,i-form, e-form.eform,e-form, form-form,e-form,e-form-form,e-form,e-form-form,form,e-form,e-form,form,form,e-form, e-form,-form, e-form,form,e-form,e-form,e-form,e-form,e-form,e-form,e-form,e-form, e-form, e-form,form,e-form,e-form e-form,e-form, -form,e-form, \"e,form,e-form,form,e-form.e-form,e-form,,e-form,e-form,e-form,form,e-form,e-form.e-form,e-form-form,form', \"\\n\\nYou're a professional marketing person with a product in mind that will help you.\\n\\nYou are an experienced marketing manager with a product in your field.\\n\\nYou got an idea for a product, that you want for the idea in.\\n\\nYou would like in an a- product that are there as you will have the product in your field.\\n\\nYou are being promoted with product that you are to the from your field.\\n\\n your marketing\\n\\n\\nYour mission this mission is the to you.\\n\\nYou.\", '\\n\\nThis product is an \"unified\" model that is a test of many different different solutions and that is what you want to get with it.\\nHow to design your unique B2B marketing idea with this way, there is an application that is a good way to get the most different. in this way, you are able to optimize your marketing and other products for the most different platform, in the same way. This product, is the next. It is based on in the idea, this way, or your own design, to generate a new, the new. the product, that, the unique to create. or, or to create, or for you to create, to add to your current product. to that, there your solutions.\\n\\nHow to analyze your product from the first one, like, it, you, it, it- it. what part of product or to the first one? The way to analyze the one\\'s to the first one. you a way of dealing the one, like. the most, the most, to a of it in the place. what the the most, are the most are one this.. you to the best. for. and on. it is. the best, because it\\'s for some one, that this. it, to.\\n\\nThe best is the most. the most, it to the best. it. ( the best, what\\'s the best, that\\'s the best,, is the best, the most. the most, the thebest, to the best. the best, the best, the best, the, the most. the best, is the most, the most, themost. the best, to the best.the most. the best, the best, the most, the, or the only. the, it. the. the best, to the best. the most, the best, it, this. the, the. the, is. the. the, the, the, to. the, the. the, the, the, the,, the, the. the, it, a to the, the. to, the next, it, it. this, it, the. of, it, the. it. the, it. the, an. it, it. to. it - the, to. like, it,, the, it. the, it. the, for, to the, the, like, be. one I, it, the. to, is, the. for. it, the, to, an. for, its, the, the. it.- there, it, in and a. at. there,, the, the, to. the. in, it. as, to. to, part, it,. in part. there, it, it. of it. it, it. for. to, the, this,. to, it, the,,. it, part. the, the, to, part., you, it. for, it. to, or, to, an, to, or to, to. to, it.it. is, this, to, it.. it,. and, it, to, to, it. that, it. there, the, it., the. the. (to. to, to. to, it., it.. one, that. for, to.. the. in, the. of, it. on. to, it.. a, the. the. of. as. it. of, the to.. it. it., the, to. it, part,it. in. i, it\\'s']}\n",
      "queries shape: [torch.Size([27]), torch.Size([36]), torch.Size([17]), torch.Size([33]), torch.Size([27]), torch.Size([25]), torch.Size([20]), torch.Size([33])]\n",
      "responses shape: [torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768]), torch.Size([768])]\n",
      "rewards shape: [torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Batch size (2) does not match number of examples - but got 8 for: queries",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[152]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_ppo_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_reward_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentiment_analyzer\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentiment_analyzer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar_tool\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar_tool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mrun_ppo_training\u001b[39m\u001b[34m(device, top_p, temperature, max_new_tokens, num_epochs, batch_size, max_length, reward_weights, save_model, output_dir, model, tokenizer, trainer, prompts, sentiment_analyzer, grammar_tool)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresponses shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[resp.shape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mresp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mresponses]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrewards shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[reward.shape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mreward\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mrewards]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m rewards_for_log = torch.stack(rewards).to(device)\n\u001b[32m    117\u001b[39m batch_log = {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: batch_prompts, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: response_texts}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\rl_ft_gpu\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\rl_ft_gpu\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:672\u001b[39m, in \u001b[36mPPOTrainer.step\u001b[39m\u001b[34m(self, queries, responses, scores, response_masks)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[33;03mRun a PPO optimisation step given a list of queries, model responses, and rewards.\u001b[39;00m\n\u001b[32m    656\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    668\u001b[39m \u001b[33;03m    `dict[str, Any]`: A summary of the training statistics\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    670\u001b[39m bs = \u001b[38;5;28mself\u001b[39m.config.batch_size\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m queries, responses, scores, response_masks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_safety_checker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_masks\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m scores = torch.tensor(scores, device=\u001b[38;5;28mself\u001b[39m.current_device)\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_score_scaling:\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# Score scaling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\rl_ft_gpu\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:627\u001b[39m, in \u001b[36mPPOTrainer._step_safety_checker\u001b[39m\u001b[34m(self, batch_size, queries, responses, scores, masks)\u001b[39m\n\u001b[32m    625\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mElements in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be tensors - got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor_list[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor_list) != batch_size:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    628\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) does not match number of examples - but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tensor_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    629\u001b[39m         )\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# add queries, scores and responses on the correct device\u001b[39;00m\n\u001b[32m    632\u001b[39m queries = [tensor.to(\u001b[38;5;28mself\u001b[39m.current_device) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m queries]\n",
      "\u001b[31mValueError\u001b[39m: Batch size (2) does not match number of examples - but got 8 for: queries"
     ]
    }
   ],
   "source": [
    "run_ppo_training(\n",
    "    device=device,\n",
    "    top_p=top_p,\n",
    "    temperature=temperature,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    num_epochs=2,\n",
    "    batch_size=8,\n",
    "    max_length=max_length,\n",
    "    reward_weights=default_reward_weights,\n",
    "    save_model=save_model,\n",
    "    output_dir=Path(output_dir or \".\"),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trainer=trainer,\n",
    "    prompts=prompts,\n",
    "    sentiment_analyzer=sentiment_analyzer,\n",
    "    grammar_tool=grammar_tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "05bead6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_queries(input_ids: torch.Tensor, attention_mask: torch.Tensor) -> List[torch.Tensor]:\n",
    "    prompt_lengths = attention_mask.sum(dim=1)\n",
    "    print(f\"prompt_lengths inside prepare_queries: {prompt_lengths}\")\n",
    "    queries: List[torch.Tensor] = []\n",
    "    for row, length in zip(input_ids, prompt_lengths):\n",
    "        length_int = int(length.item())\n",
    "        print(f\"length_int inside prepare_queries loop: {length_int}\")\n",
    "        start_idx = row.size(0) - length_int  # account for left padding\n",
    "        print(f\"start_idx inside prepare_queries loop: {start_idx}\")\n",
    "        queries.append(row[start_idx:].clone())\n",
    "    return queries\n",
    "\n",
    "def slice_responses(\n",
    "    generated_ids: torch.Tensor,\n",
    "    prompt_token_count: int,\n",
    "    prompt_lengths: Sequence[int],\n",
    ") -> List[torch.Tensor]:\n",
    "    responses: List[torch.Tensor] = []\n",
    "    print(f\"shape of generated: {generated_ids.shape}\")\n",
    "    print(f\"prompt_token_count: {prompt_token_count}\")\n",
    "    print(f\"prompt_lengths: {prompt_lengths}\")\n",
    "    for idx in range(generated_ids.size(0)):\n",
    "        start = prompt_token_count\n",
    "        print(f\"start index for response slicing: {start}\")\n",
    "        resp_ids = generated_ids[idx, start:]\n",
    "        print(f\"shape of resp_ids: {resp_ids.shape}\")\n",
    "        if resp_ids.numel() == 0:\n",
    "            print(\"resp_ids is empty, using last token as response\")\n",
    "            resp_ids = generated_ids.new_tensor([generated_ids[idx, -1].item()])\n",
    "            print(f\"shape of resp_ids after using last token: {resp_ids.shape}\")\n",
    "        responses.append(resp_ids)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b506c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Write an enterprise sales email centered on: \"Cut cloud spend 30–40%. 15‑min chat next week? – DataSync\"', 'Craft a formal product introduction email about: \"Our clients typically see 15-20% increase in conversion rates within 90 days.\"']\n",
      "number of prompts in a batch - 2\n",
      "\n",
      "max_length: 1024\n",
      "\n",
      "input_ids: tensor([[16594,   281, 13953,  4200,  3053, 19254,   319,    25,   366, 26254,\n",
      "          6279,  4341,  1542,  1906,  1821,  7225,  1315, 20977,  1084,  8537,\n",
      "          1306,  1285,    30,   784,  6060, 28985,     1],\n",
      "        [50256, 50256, 14467,   257,  8766,  1720,  9793,  3053,   546,    25,\n",
      "           366,  5122,  7534,  6032,   766,  1315,    12,  1238,     4,  2620,\n",
      "           287, 11315,  3965,  1626,  4101,  1528,   526]], device='cuda:0')\n",
      "\n",
      "shape of input_ids - torch.Size([2, 27])\n",
      "\n",
      "shape of attention_mask - torch.Size([2, 27])\n",
      "\n",
      "prompt_lengths inside prepare_queries: tensor([27, 25], device='cuda:0')\n",
      "length_int inside prepare_queries loop: 27\n",
      "start_idx inside prepare_queries loop: 0\n",
      "length_int inside prepare_queries loop: 25\n",
      "start_idx inside prepare_queries loop: 2\n",
      "queries shape: [torch.Size([27]), torch.Size([25])]\n",
      "max_new_tokens: 768, pad_token_id: 50256, top_p: 1.0, temperature: 0.85\n",
      "\n",
      "shape of generated: torch.Size([2, 795])\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "batch_size = 2\n",
    "batch_prompts = list(prompts[idx : idx + batch_size])\n",
    "print(batch_prompts)\n",
    "print(f\"number of prompts in a batch - {len(batch_prompts)}\\n\")\n",
    "\n",
    "print(f\"max_length: {max_length}\\n\")\n",
    "enc = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=False,\n",
    "    #max_length=max_length,\n",
    ")\n",
    "input_ids = enc.input_ids.to(device)\n",
    "print(f\"input_ids: {input_ids}\\n\")\n",
    "print(f\"shape of input_ids - {input_ids.shape}\\n\")\n",
    "attention_mask = enc.attention_mask.to(device)\n",
    "print(f\"shape of attention_mask - {attention_mask.shape}\\n\")\n",
    "\n",
    "queries = prepare_queries(input_ids, attention_mask)\n",
    "print(f\"queries shape: {[query.shape for query in queries]}\")\n",
    "\n",
    "#decode queries to get actual prompt text\n",
    "decoded_queries = [tokenizer.decode(query) for query in queries]\n",
    "for i in range(len(decoded_queries)):\n",
    "    if decoded_queries[i] != batch_prompts[i]:\n",
    "        print(f\"Decoded query {i} does not match original prompt:\")\n",
    "        print(f\"Decoded: {decoded_queries[i]}\")\n",
    "        print(f\"Original: {batch_prompts[i]}\\n\")\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"max_new_tokens: {max_new_tokens}, pad_token_id: {pad_token_id}, top_p: {top_p}, temperature: {temperature}\\n\")\n",
    "gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=pad_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\n",
    "#print(**gen_kwargs)\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **gen_kwargs,\n",
    "                            )\n",
    "print(f\"shape of generated: {generated.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8d131b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of generated: torch.Size([2, 795])\n",
      "prompt_token_count: 27\n",
      "prompt_lengths: [27, 25]\n",
      "shape of generated: torch.Size([2, 795])\n",
      "prompt_token_count: 27\n",
      "prompt_lengths: [27, 25]\n",
      "start index for response slicing: 27\n",
      "shape of resp_ids: torch.Size([768])\n",
      "start index for response slicing: 27\n",
      "shape of resp_ids: torch.Size([768])\n",
      "shape of responses: [torch.Size([768]), torch.Size([768])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of generated: {generated.shape}\")\n",
    "prompt_token_count = input_ids.shape[1]\n",
    "print(f\"prompt_token_count: {prompt_token_count}\")\n",
    "prompt_lengths = attention_mask.sum(dim=1).tolist()\n",
    "print(f\"prompt_lengths: {prompt_lengths}\")\n",
    "\n",
    "responses = slice_responses(generated, prompt_token_count, prompt_lengths)\n",
    "print(f\"shape of responses: {[resp.shape for resp in responses]}\")\n",
    "\n",
    "response_texts = [tokenizer.decode(resp, skip_special_tokens=True) for resp in responses]\n",
    "#for i, text in enumerate(response_texts):\n",
    "#    print(f\"Response {i} text: {text}\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "82f08f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1x\\n\\n...\\n\\n\\n2...\\n\\n\\n.\\n\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n,\\n\\n,\\n\\n\\n,\\n,\\n\\n\\n,\\n\\n.\\n\\n\"I think many people assume that people on a website have something they would like to see from all the time in that can come some things in their free time.But many people don\\'t realise that people are most likely to see the new products in the free time.I think many people think most people will see the new products in the free time.I think many people in the industry want to see the new products in free time. They think most people of one sort will see the new products in free time,but many people don\\'t realise that many people will see the new products in free time.\\n\\n\"I think, I see the new products that, many new things. I think people who are most likely to see the new products in free time,I think that the free of, I think people have, to see the new products in free time, in a. I think most people in the industry won\\'t see the products in free time.Some, some are for not new, some one for new, some one for new,I think that, some one, some ones the new products for free time, I think that, people who are most likely to see the new products in free time.\\n\\nI think it, I think, i have it now in my free free time,and, if you do what, it\\'s for free time. I think many have a free time before those two things happened.\\n\\nI think many of new, what that is, some that are for free time, some of the new product arefor free, some of the more, some, one for new, some, some one for new,some, some, some, some, some, some, some, some, other, I can say for, to more, some, all, some, few, a, all,for, some, some, many, many, a, I can say, if you want to see, the new products,of the new products, or on, more, all the, the, all.I think, i saw, the new products in free time.\\n\\nI think some, that, most, they are for new, some, some, some, some, one for new, some, some, for new (s. they are for new, a, that, a, new, an, the product, some, some, just, in those, but some, for new andfor, people for new, and some, for new, some,, some, new, some, some, some, if you want to see, and one, some, other, all, some, or, I will, all, some, there, many, many, some, all, and some, some, some, not in, there, in, in, in, and in, my.\\n\\nA:.I think most, most, people, i the most people, and, like, all, in the, as, in all, many (s, they, I think, I have it, it, I\\'ve, that, in I am, I have, in, in, i i think. to, a, a, it are, i, in, i,I, and, most, on.\\n\\n\\n(s.,.I mean, I have it,I have, that, or there, some, for some, some, some,, some, and,I have, or,'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f0a2888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length': 1.2, 'politeness': 1.2, 'sentiment': 0.7, 'clarity': 0.6, 'cta': 1.4, 'personalization': 0.7, 'grammar': 0.8, 'value': 1.1, 'spam': 0.8, 'structure': 0.8, 'instructional_tone': 0.8, 'lexical_coherence': 1.2}\n"
     ]
    }
   ],
   "source": [
    "print(default_reward_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bbad9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -1.94\n",
      "Reward tensor: -1.940000057220459\n",
      "Total reward: -0.5400000000000003\n",
      "Reward tensor: -0.5400000214576721\n"
     ]
    }
   ],
   "source": [
    "rewards: List[torch.Tensor] = []\n",
    "for text in response_texts:\n",
    "    total_reward = compute_reward(text,\n",
    "                    sentiment_analyzer=sentiment_analyzer,\n",
    "                    tool=grammar_tool,\n",
    "                    weights=default_reward_weights\n",
    "                    )\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    reward_tensor = torch.tensor(float(total_reward),\n",
    "                    device=device,\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "    print(f\"Reward tensor: {reward_tensor}\")\n",
    "    rewards.append(reward_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6a619d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-1.9400, device='cuda:0'), tensor(-0.5400, device='cuda:0')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e7a19e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries shape: [torch.Size([27]), torch.Size([25])]\n",
      "responses shape: [torch.Size([768]), torch.Size([768])]\n",
      "rewards shape: [torch.Size([]), torch.Size([])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"queries shape: {[query.shape for query in queries]}\")\n",
    "print(f\"responses shape: {[resp.shape for resp in responses]}\")\n",
    "print(f\"rewards shape: {[reward.shape for reward in rewards]}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bfdb2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a inference function to generate email with the fine-tuned model\n",
    "def generate_email(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    device: str = \"cuda:0\",\n",
    "    max_length: int = 1024,\n",
    "    max_new_tokens: int = 768,\n",
    "    top_p: float = 1.0,\n",
    "    temperature: float = 0.85,\n",
    ") -> str:\n",
    "    model = model.to(device)\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        #max_length=max_length,\n",
    "    )\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    attention_mask = enc.attention_mask.to(device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "\n",
    "    prompt_token_count = input_ids.size(1)\n",
    "    prompt_lengths = attention_mask.sum(dim=1).tolist()\n",
    "    responses = slice_responses(generated, prompt_token_count, prompt_lengths)\n",
    "    response_text = tokenizer.decode(responses[0], skip_special_tokens=True)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47e5e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ppo_gpt2 were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of generated: torch.Size([1, 288])\n",
      "prompt_token_count: 32\n",
      "prompt_lengths: [32]\n",
      "start index for response slicing: 32\n",
      "shape of resp_ids: torch.Size([256])\n",
      "\n",
      "\n",
      "To test this idea, we created a new, fast-paced app, called \"Tune\" (tune it to your preferences). Tune is an app that lets you send your data to Google Analytics for the most accurate predictions on your data, and then use it to generate, analyze, and analyze your data to optimize your business.\n",
      "\n",
      "We analyzed the data that we wanted to analyze, and then used it to create a new app, called \"Tune\", which was optimized to generate, analyze, and analyze your data.\n",
      "\n",
      "Now, let's analyze the data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data, and analyze your data,\n",
      "shape of generated: torch.Size([1, 287])\n",
      "prompt_token_count: 31\n",
      "prompt_lengths: [31]\n",
      "start index for response slicing: 31\n",
      "shape of resp_ids: torch.Size([256])\n",
      "\n",
      "\n",
      "We will publish our first batch of products, from the top five best-selling cloud-based solutions, on the web on the 26th of August.\n",
      "\n",
      "We will publish the second batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the third batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the fourth batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the fifth batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the sixth batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the seventh batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the eighth batch of products, from the top five best-selling cloud-based solutions, on the 20th of August.\n",
      "\n",
      "We will publish the ninth batch of products, from the\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_dir = \"ppo_gpt2\"\n",
    "finetuned_model = AutoModelForCausalLMWithValueHead.from_pretrained(finetuned_model_dir)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
    "test_prompt1 = \"Write a concise, professional sales email introducing this idea: \\\"Our new AI-powered analytics platform can help businesses unlock insights from their data faster and more accurately.\\\"\"\n",
    "test_prompt2 = \"Compose a friendly B2B sales email based on this concept: \\\"We are launching a cloud-based collaboration tool that enhances team productivity and communication.\\\"\"\n",
    "test_prompts = [test_prompt1, test_prompt2]\n",
    "generated_emails = []\n",
    "for test_prompt in test_prompts:\n",
    "    generated_email = generate_email(\n",
    "        model=finetuned_model,\n",
    "        tokenizer=finetuned_tokenizer,\n",
    "        prompt=test_prompt,\n",
    "        max_length=256,\n",
    "        max_new_tokens=256,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(generated_email)\n",
    "    generated_emails.append(generated_email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c1c70c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWe\\'ll let you know if you want to send us the news and features you like in the comments below.\\n\\nIn the meantime, we\\'ll keep you posted on the latest developments in the market and the latest news from the market.\\n\\nIf you enjoy the website, we recommend you to check out our blog.\\n\\nRead also:\\n\\nHow to build a custom app with Android, iOS and Windows 8\\n\\nHow to build a custom app with iOS, Android and Windows 8\\n\\nWhat you need to know about the new API\\n\\nThe new API is called \"SockApp\", and it contains:\\n\\nA simple way to interact with the data\\n\\nA way to track the amount of data\\n\\nA way to track the number of requests\\n\\nA way to track the number of requests\\n\\nHow to implement the API\\n\\nIn the first part of the post, we will introduce the API for the API that we have set up in the past, so you can easily learn more about it.\\n\\nThe API will be written using a simple, low level API.\\n\\nThe first thing you need to do is to create a database in the database, which will contain the data.\\n\\nYou can define'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c181ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL Finetune (GPU)",
   "language": "python",
   "name": "rl_ft_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
