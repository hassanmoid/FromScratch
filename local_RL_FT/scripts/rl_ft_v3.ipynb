{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43d108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged into Hugging Face successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [HUGGING FACE LOGIN]\n",
    "# Hugging Face login cell — paste your token directly here.\n",
    "# ⚠️ Only do this on your personal machine. Do NOT share or push notebooks with your token visible.\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "hf_token = \"\" # <-- paste your actual Hugging Face token here\n",
    "login(token=hf_token, add_to_git_credential=True)\n",
    "print(\"✅ Logged into Hugging Face successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1549f3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.14 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 18:30:03) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch 2.5.1+cu121 CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# CELL 1: Install dependencies\n",
    "# Run in Anaconda or WSL2 if possible for stability.\n",
    "\n",
    "\n",
    "import sys\n",
    "print('Python', sys.version)\n",
    "\n",
    "\n",
    "# Uncomment if needed:\n",
    "# !pip install -q transformers accelerate datasets bitsandbytes trl peft auto-gptq sentence-transformers scikit-learn wandb\n",
    "\n",
    "\n",
    "# %%\n",
    "# CELL 2: Check GPU and library setup\n",
    "import torch\n",
    "print('Torch', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForCausalLM,\n",
    "AutoModelForSequenceClassification,\n",
    "BitsAndBytesConfig,\n",
    "TrainingArguments,\n",
    "Trainer,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from trl import PPOTrainer, PPOConfig\n",
    "except Exception as e:\n",
    "    print('TRL not installed or incompatible:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e73366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# CELL 3: Setup paths and directories\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path('rlft_project')\n",
    "PROJECT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR = PROJECT_DIR / 'models'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_PATH = PROJECT_DIR / 'emails.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a5e60fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 100 synthetic email examples with realistic distribution and diversity.\n",
      "outcome\n",
      "0    0.66\n",
      "1    0.21\n",
      "2    0.13\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>email_text</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Customer: marketing director at a Finance firm...</td>\n",
       "      <td>Hi there, Our payment gateway has helped sever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer: marketing director at a Retail firm ...</td>\n",
       "      <td>Good morning, We designed our POS system speci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customer: procurement head at a Manufacturing ...</td>\n",
       "      <td>Hey [Name], I noticed your company operates in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Customer: CFO at a IT Services firm looking fo...</td>\n",
       "      <td>Hey [Name], Our AI chatbot has helped several ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Customer: procurement head at a Healthcare fir...</td>\n",
       "      <td>Good morning, Our telehealth app has helped se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Customer: marketing director at a Finance firm...   \n",
       "1  Customer: marketing director at a Retail firm ...   \n",
       "2  Customer: procurement head at a Manufacturing ...   \n",
       "3  Customer: CFO at a IT Services firm looking fo...   \n",
       "4  Customer: procurement head at a Healthcare fir...   \n",
       "\n",
       "                                          email_text  outcome  \n",
       "0  Hi there, Our payment gateway has helped sever...        1  \n",
       "1  Good morning, We designed our POS system speci...        0  \n",
       "2  Hey [Name], I noticed your company operates in...        0  \n",
       "3  Hey [Name], Our AI chatbot has helped several ...        2  \n",
       "4  Good morning, Our telehealth app has helped se...        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "industries = [\n",
    "    \"Retail\", \"Finance\", \"IT Services\", \"Manufacturing\", \n",
    "    \"Travel\", \"Healthcare\", \"Education\", \"E-commerce\", \"Real Estate\"\n",
    "]\n",
    "\n",
    "products = {\n",
    "    \"Retail\": [\"POS system\", \"inventory app\", \"billing software\"],\n",
    "    \"Finance\": [\"payment gateway\", \"investment dashboard\", \"fraud detection API\"],\n",
    "    \"IT Services\": [\"AI chatbot\", \"CRM integration\", \"cloud migration service\"],\n",
    "    \"Manufacturing\": [\"supply chain tool\", \"equipment monitoring system\", \"IoT sensors\"],\n",
    "    \"Travel\": [\"booking automation platform\", \"hotel CRM\", \"analytics dashboard\"],\n",
    "    \"Healthcare\": [\"patient management system\", \"appointment scheduler\", \"telehealth app\"],\n",
    "    \"Education\": [\"learning management system\", \"student CRM\", \"assessment platform\"],\n",
    "    \"E-commerce\": [\"email automation tool\", \"conversion tracking app\", \"inventory sync system\"],\n",
    "    \"Real Estate\": [\"lead management software\", \"property analytics tool\", \"CRM suite\"]\n",
    "}\n",
    "\n",
    "def generate_context(industry, product):\n",
    "    role = random.choice([\n",
    "        \"procurement head\", \"operations manager\", \"founder\", \"marketing director\",\n",
    "        \"CTO\", \"CFO\", \"sales head\", \"admin manager\"\n",
    "    ])\n",
    "    intent = random.choice([\n",
    "        \"looking for solutions\", \"asked about pricing\", \"requested demo\", \"cold prospect\",\n",
    "        \"was referred by a partner\", \"attended a webinar\", \"existing customer\"\n",
    "    ])\n",
    "    return f\"Customer: {role} at a {industry} firm {intent} for a {product}.\"\n",
    "\n",
    "def generate_email(industry, product, outcome):\n",
    "    openings = [\n",
    "        \"Hi there,\", \"Hello [Name],\", \"Greetings,\", \"Hey [Name],\", \"Good morning,\"\n",
    "    ]\n",
    "    pitches = [\n",
    "        f\"Our {product} has helped several {industry.lower()} companies optimize their workflow.\",\n",
    "        f\"We designed our {product} specifically for {industry.lower()} businesses like yours.\",\n",
    "        f\"I noticed your company operates in {industry.lower()} — our {product} could be a great fit.\",\n",
    "        f\"With our {product}, teams in {industry.lower()} are saving hours every week.\"\n",
    "    ]\n",
    "    closings = [\n",
    "        \"Would you be open to a quick call this week?\",\n",
    "        \"Can I send over a short demo video?\",\n",
    "        \"Happy to share a case study if interested.\",\n",
    "        \"Would love to show how this could help your team.\"\n",
    "    ]\n",
    "\n",
    "    if outcome == 0:\n",
    "        # No reply: generic, slightly spammy, less personalized\n",
    "        tone = random.choice([\n",
    "            \"We offer best-in-class pricing for your needs. Let's connect soon.\",\n",
    "            \"Don’t miss this opportunity to upgrade your business solutions.\",\n",
    "            \"We are reaching out to introduce our services. Please respond for more info.\",\n",
    "            \"Exclusive offer valid till end of this week!\"\n",
    "        ])\n",
    "    elif outcome == 1:\n",
    "        # Reply but no conversion: decent personalization, informative but not compelling\n",
    "        tone = random.choice([\n",
    "            f\"Thanks for your interest earlier! Our {product} can streamline your operations easily.\",\n",
    "            f\"I’m following up regarding your query on {product}. We’d be happy to provide more details.\",\n",
    "            f\"Our {product} integrates seamlessly — perhaps a short demo could help?\",\n",
    "            f\"Following up from our last email — any feedback on the proposal?\"\n",
    "        ])\n",
    "    else:\n",
    "        # Conversion: warm, clear CTA, value-oriented\n",
    "        tone = random.choice([\n",
    "            f\"Appreciate your earlier response — many {industry.lower()} firms achieved 25% growth with our {product}.\",\n",
    "            f\"Glad you found the demo useful! Let's schedule onboarding for your team next week.\",\n",
    "            f\"Delighted to know you’re proceeding with our {product}. We'll ensure a smooth rollout.\",\n",
    "            f\"Excited to start the partnership! Our support team will reach out to get you set up.\"\n",
    "        ])\n",
    "\n",
    "    return f\"{random.choice(openings)} {random.choice(pitches)} {tone} {random.choice(closings)}\"\n",
    "\n",
    "# Create data\n",
    "data = []\n",
    "for _ in range(100):\n",
    "    industry = random.choice(industries)\n",
    "    product = random.choice(products[industry])\n",
    "    outcome = random.choices([0,1,2], weights=[60,25,15])[0]\n",
    "    context = generate_context(industry, product)\n",
    "    email = generate_email(industry, product, outcome)\n",
    "    data.append({\n",
    "        \"context\": context,\n",
    "        \"email_text\": email,\n",
    "        \"outcome\": outcome\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"rlft_project/emails.csv\", index=False)\n",
    "print(\"✅ Created 100 synthetic email examples with realistic distribution and diversity.\")\n",
    "print(df['outcome'].value_counts(normalize=True))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855b2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 100 rows\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# CELL 4: Load dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Data loaded:', len(df), 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b84687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>email_text</th>\n",
       "      <th>outcome</th>\n",
       "      <th>prompt</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Customer: marketing director at a Finance firm...</td>\n",
       "      <td>Hi there, Our payment gateway has helped sever...</td>\n",
       "      <td>1</td>\n",
       "      <td>You are a salesperson writing an outreach or r...</td>\n",
       "      <td>You are a salesperson writing an outreach or r...</td>\n",
       "      <td>Hi there, Our payment gateway has helped sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer: marketing director at a Retail firm ...</td>\n",
       "      <td>Good morning, We designed our POS system speci...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a salesperson writing an outreach or r...</td>\n",
       "      <td>You are a salesperson writing an outreach or r...</td>\n",
       "      <td>Good morning, We designed our POS system speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customer: procurement head at a Manufacturing ...</td>\n",
       "      <td>Hey [Name], I noticed your company operates in...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a salesperson writing an outreach or r...</td>\n",
       "      <td>You are a salesperson writing an outreach or r...</td>\n",
       "      <td>Hey [Name], I noticed your company operates in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Customer: marketing director at a Finance firm...   \n",
       "1  Customer: marketing director at a Retail firm ...   \n",
       "2  Customer: procurement head at a Manufacturing ...   \n",
       "\n",
       "                                          email_text  outcome  \\\n",
       "0  Hi there, Our payment gateway has helped sever...        1   \n",
       "1  Good morning, We designed our POS system speci...        0   \n",
       "2  Hey [Name], I noticed your company operates in...        0   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  You are a salesperson writing an outreach or r...   \n",
       "1  You are a salesperson writing an outreach or r...   \n",
       "2  You are a salesperson writing an outreach or r...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  You are a salesperson writing an outreach or r...   \n",
       "1  You are a salesperson writing an outreach or r...   \n",
       "2  You are a salesperson writing an outreach or r...   \n",
       "\n",
       "                                         target_text  \n",
       "0  Hi there, Our payment gateway has helped sever...  \n",
       "1  Good morning, We designed our POS system speci...  \n",
       "2  Hey [Name], I noticed your company operates in...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# CELL 5: Create prompt templates\n",
    "PROMPT_TEMPLATE = (\n",
    "\"You are a salesperson writing an outreach or reply email.\\n\"\n",
    "\"Context:\\n{context}\\n\\n\"\n",
    "\"Write a persuasive, concise email that a seller would write to a prospect or a customer.\\n\\n\"\n",
    "\"Email:\\n\"\n",
    ")\n",
    "\n",
    "df['prompt'] = df['context'].apply(lambda c: PROMPT_TEMPLATE.format(context=c))\n",
    "df['input_text'] = df['prompt']\n",
    "df['target_text'] = df['email_text']\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1f95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a salesperson writing an outreach or reply email.\n",
      "Context:\n",
      "Customer: marketing director at a Finance firm asked about pricing for a payment gateway.\n",
      "\n",
      "Write a persuasive, concise email that a seller would write to a prospect or a customer.\n",
      "\n",
      "Email:\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Hi there, Our payment gateway has helped several finance companies optimize their workflow. I’m following up regarding your query on payment gateway. We’d be happy to provide more details. Would love to show how this could help your team.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,\"input_text\"])\n",
    "print(\"\\n=====================\\n\")\n",
    "print(df.loc[0,\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a158dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\envs\\rl_ft_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86167964f994f96a1f8b68faa6576f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba671fb3fa249858144e491c935b435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: moid (moid-microsoft) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Learnings\\FromScratch\\local_RL_FT\\scripts\\wandb\\run-20251105_201737-r2hljdc0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moid-microsoft/huggingface/runs/r2hljdc0' target=\"_blank\">rlft_project\\models\\rm</a></strong> to <a href='https://wandb.ai/moid-microsoft/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moid-microsoft/huggingface' target=\"_blank\">https://wandb.ai/moid-microsoft/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moid-microsoft/huggingface/runs/r2hljdc0' target=\"_blank\">https://wandb.ai/moid-microsoft/huggingface/runs/r2hljdc0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cc1b1bc3dd43daad0e9e811a3ee7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0006, 'grad_norm': 2.3375067710876465, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n",
      "{'loss': 0.8494, 'grad_norm': 4.1476263999938965, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.7654, 'grad_norm': 4.163939952850342, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n",
      "{'loss': 0.693, 'grad_norm': 2.1181089878082275, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n",
      "{'loss': 0.6398, 'grad_norm': 2.9335880279541016, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 36.2278, 'train_samples_per_second': 11.041, 'train_steps_per_second': 1.38, 'train_loss': 0.789647216796875, 'epoch': 5.0}\n",
      "Reward Model setup complete (train step commented).\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# CELL 6: Reward Model (DistilBERT-based classifier)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['outcome'], random_state=42)\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "\n",
    "rm_model_name = 'distilbert-base-uncased'\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(rm_model_name)\n",
    "\n",
    "\n",
    "def preprocess_rm(batch):\n",
    "    texts = [c + \"\\n\" + e for c, e in zip(batch[\"context\"], batch[\"email_text\"])]\n",
    "    enc = rm_tokenizer(texts, truncation=True, padding='max_length', max_length=256)\n",
    "    enc[\"labels\"] = batch[\"outcome\"]  # <-- crucial fix\n",
    "    return enc\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(preprocess_rm, batched=True)\n",
    "val_ds = val_ds.map(preprocess_rm, batched=True)\n",
    "\n",
    "train_ds = train_ds.remove_columns(['context', 'email_text', 'outcome'])\n",
    "val_ds = val_ds.remove_columns(['context', 'email_text', 'outcome'])\n",
    "\n",
    "train_ds.set_format(type=\"torch\")\n",
    "val_ds.set_format(type=\"torch\")\n",
    "\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(rm_model_name, num_labels=3)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "output_dir=str(MODEL_DIR / 'rm'),\n",
    "num_train_epochs=5,\n",
    "per_device_train_batch_size=8,\n",
    "per_device_eval_batch_size=8,\n",
    "learning_rate=2e-5,\n",
    "fp16=torch.cuda.is_available(),\n",
    "logging_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "rm_trainer = Trainer(\n",
    "model=rm_model,\n",
    "args=training_args,\n",
    "train_dataset=train_ds,\n",
    "eval_dataset=val_ds,\n",
    ")\n",
    "\n",
    "\n",
    "rm_trainer.train() # Uncomment to train\n",
    "\n",
    "\n",
    "print('Reward Model setup complete (train step commented).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f887ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# CELL 7: Reward function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def reward_fn(prompts, responses):\n",
    "    texts = [p + '\\n' + r for p,r in zip(prompts, responses)]\n",
    "    print(f\"texts - {texts}\")\n",
    "    enc = rm_tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        enc = {k:v.to('cuda') for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = rm_model(**enc)\n",
    "        probs = F.softmax(out.logits, dim=-1)\n",
    "        score = probs[:,2]*1.0 + probs[:,1]*0.5 # map 2→1.0, 1→0.5\n",
    "    return score.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1045256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef9649a47184012817ff238ccdefaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Mistral-7B-Instruct successfully (no quantization).\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: Load Mistral-7B-Instruct without quantization (standard FP16/BF16)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Model (no bitsandbytes) ---\n",
    "# If you want faster performance with smaller VRAM footprint, use torch_dtype=torch.float16\n",
    "# or torch.bfloat16 if your GPU supports it (your RTX 4060 does)\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,   # or torch.bfloat16 for Ampere+\n",
    "    device_map=None,           # Let HF/Accelerate handle GPU/CPU split\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "policy_model.config.use_cache = True\n",
    "policy_model = policy_model.to(\"cuda\")   # ✅ manually move to GPU\n",
    "policy_model.eval()\n",
    "\n",
    "print(\"✅ Loaded Mistral-7B-Instruct successfully (no quantization).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98f36f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistralai/Mistral-7B-Instruct-v0.2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eeff43e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing PPOTrainer: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# CELL 9: PPO Config & Trainer setup\n",
    "try:\n",
    "    # Convert your policy model into a value-head model\n",
    "    model_for_ppo = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model)\n",
    "\n",
    "    ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1e-6,\n",
    "    batch_size=int(4),\n",
    "    mini_batch_size=int(1),\n",
    "    ppo_epochs=int(2),\n",
    "    gradient_accumulation_steps=int(2),\n",
    "    )\n",
    "\n",
    "\n",
    "    ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model_for_ppo,\n",
    "    tokenizer=tokenizer,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print('Error initializing PPOTrainer:', e)\n",
    "    ppo_trainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8d9d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error initializing PPOTrainer: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: PPO Config & Trainer setup\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "try:\n",
    "    # Convert your policy model into a value-head model\n",
    "    model_for_ppo = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model)\n",
    "\n",
    "    # Define PPO configuration\n",
    "    ppo_config = PPOConfig(\n",
    "        model_name=model_name,\n",
    "        learning_rate=1e-6,\n",
    "        batch_size=8,\n",
    "        mini_batch_size=4,\n",
    "        ppo_epochs=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        log_with=None,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Create PPO trainer\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=model_for_ppo,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"✅ PPOTrainer initialized successfully with value head!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error initializing PPOTrainer:\", e)\n",
    "    ppo_trainer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768503f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL Finetune (GPU)",
   "language": "python",
   "name": "rl_ft_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
