{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(\"mnist.pkl.gz\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = pickle.load(f,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.vstack((train_set[0], valid_set[0]))\n",
    "Y_train = np.hstack((train_set[1], valid_set[1]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutedMnistGenerator():\n",
    "    def __init__(self, max_iter=10):\n",
    "        f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "        train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
    "        f.close()\n",
    "\n",
    "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
    "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
    "        self.X_test = test_set[0]\n",
    "        self.Y_test = test_set[1]\n",
    "        self.max_iter = max_iter\n",
    "        self.cur_iter = 0\n",
    "\n",
    "    def get_dims(self):\n",
    "        # Get data input and output dimensions\n",
    "        return self.X_train.shape[1], 10\n",
    "\n",
    "    def next_task(self):\n",
    "        if self.cur_iter >= self.max_iter:\n",
    "            raise Exception('Number of tasks exceeded!')\n",
    "        else:\n",
    "            np.random.seed(self.cur_iter)\n",
    "            perm_inds = list(range(self.X_train.shape[1]))\n",
    "            np.random.shuffle(perm_inds)\n",
    "\n",
    "            # Retrieve train data\n",
    "            next_x_train = deepcopy(self.X_train)\n",
    "            next_x_train = next_x_train[:,perm_inds]\n",
    "            next_y_train = np.eye(10)[self.Y_train]\n",
    "\n",
    "            # Retrieve test data\n",
    "            next_x_test = deepcopy(self.X_test)\n",
    "            next_x_test = next_x_test[:,perm_inds]\n",
    "            next_y_test = np.eye(10)[self.Y_test]\n",
    "\n",
    "            self.cur_iter += 1\n",
    "\n",
    "            return next_x_train, next_y_train, next_x_test, next_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = PermutedMnistGenerator(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "in_dim, out_dim = data_gen.get_dims()\n",
    "print(in_dim)\n",
    "print(out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cla_NN(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size):\n",
    "        # input and output placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size])\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size])\n",
    "        self.task_idx = tf.placeholder(tf.int32)\n",
    "        \n",
    "    def assign_optimizer(self, learning_rate=0.001):\n",
    "        self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "\n",
    "    def assign_session(self):\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # launch a session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def train(self, x_train, y_train, task_idx, no_epochs=1000, batch_size=100, display_epoch=5):\n",
    "        N = x_train.shape[0]\n",
    "        if batch_size > N:\n",
    "            batch_size = N\n",
    "\n",
    "        sess = self.sess\n",
    "        costs = []\n",
    "        # Training cycle\n",
    "        for epoch in range(no_epochs):\n",
    "            perm_inds = list(range(x_train.shape[0]))\n",
    "            np.random.shuffle(perm_inds)\n",
    "            cur_x_train = x_train[perm_inds]\n",
    "            cur_y_train = y_train[perm_inds]\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(np.ceil(N * 1.0 / batch_size))\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                start_ind = i*batch_size\n",
    "                end_ind = np.min([(i+1)*batch_size, N])\n",
    "                batch_x = cur_x_train[start_ind:end_ind, :]\n",
    "                batch_y = cur_y_train[start_ind:end_ind, :]\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _, c = sess.run(\n",
    "                    [self.train_step, self.cost], \n",
    "                    feed_dict={self.x: batch_x, self.y: batch_y, self.task_idx: task_idx})\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_epoch == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost))\n",
    "            costs.append(avg_cost)\n",
    "        print(\"Optimization Finished!\")\n",
    "        return costs\n",
    "\n",
    "    def prediction(self, x_test, task_idx):\n",
    "        # Test model\n",
    "        prediction = self.sess.run([self.pred], feed_dict={self.x: x_test, self.task_idx: task_idx})[0]\n",
    "        return prediction\n",
    "\n",
    "    def prediction_prob(self, x_test, task_idx):\n",
    "        prob = self.sess.run([tf.nn.softmax(self.pred)], feed_dict={self.x: x_test, self.task_idx: task_idx})[0]\n",
    "        return prob\n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = self.sess.run([self.weights])[0]\n",
    "        return weights\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_NN(Cla_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size, prev_weights=None, learning_rate=0.001):\n",
    "\n",
    "        super(Vanilla_NN, self).__init__(input_size, hidden_size, output_size, training_size)\n",
    "        # init weights and biases\n",
    "        self.W, self.b, self.W_last, self.b_last, self.size = self.create_weights(\n",
    "                input_size, hidden_size, output_size, prev_weights)\n",
    "        self.no_layers = len(hidden_size) + 1\n",
    "        self.pred = self._prediction(self.x, self.task_idx)\n",
    "        self.cost = - self._logpred(self.x, self.y, self.task_idx)\n",
    "        self.weights = [self.W, self.b, self.W_last, self.b_last]\n",
    "\n",
    "        self.assign_optimizer(learning_rate)\n",
    "        self.assign_session()\n",
    "\n",
    "    def _prediction(self, inputs, task_idx):\n",
    "        act = inputs\n",
    "        for i in range(self.no_layers-1):\n",
    "            pre = tf.add(tf.matmul(act, self.W[i]), self.b[i])\n",
    "            act = tf.nn.relu(pre)\n",
    "        pre = tf.add(tf.matmul(act, tf.gather(self.W_last, task_idx)), tf.gather(self.b_last, task_idx))\n",
    "        return pre\n",
    "\n",
    "    def _logpred(self, inputs, targets, task_idx):\n",
    "        pred = self._prediction(inputs, task_idx)\n",
    "        log_lik = - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=targets))\n",
    "        return log_lik\n",
    "\n",
    "    def create_weights(self, in_dim, hidden_size, out_dim, prev_weights):\n",
    "        hidden_size = deepcopy(hidden_size)\n",
    "        hidden_size.append(out_dim)\n",
    "        hidden_size.insert(0, in_dim)\n",
    "        no_params = 0\n",
    "        no_layers = len(hidden_size) - 1\n",
    "        W = []\n",
    "        b = []\n",
    "        W_last = []\n",
    "        b_last = []\n",
    "        for i in range(no_layers-1):\n",
    "            din = hidden_size[i]\n",
    "            dout = hidden_size[i+1]\n",
    "            if prev_weights is None:\n",
    "                Wi_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
    "                bi_val = tf.truncated_normal([dout], stddev=0.1)\n",
    "            else:\n",
    "                Wi_val = tf.constant(prev_weights[0][i])\n",
    "                bi_val = tf.constant(prev_weights[1][i])\n",
    "            Wi = tf.Variable(Wi_val)\n",
    "            bi = tf.Variable(bi_val)\n",
    "            W.append(Wi)\n",
    "            b.append(bi)\n",
    "\n",
    "        if prev_weights is not None:\n",
    "            prev_Wlast = prev_weights[2]\n",
    "            prev_blast = prev_weights[3]\n",
    "            no_prev_tasks = len(prev_Wlast)\n",
    "            for j in range(no_prev_tasks):\n",
    "                W_j = prev_Wlast[j]\n",
    "                b_j = prev_blast[j]\n",
    "                Wi = tf.Variable(W_j)\n",
    "                bi = tf.Variable(b_j)\n",
    "                W_last.append(Wi)\n",
    "                b_last.append(bi)\n",
    "\n",
    "        din = hidden_size[-2]\n",
    "        dout = hidden_size[-1]\n",
    "        Wi_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
    "        bi_val = tf.truncated_normal([dout], stddev=0.1)\n",
    "        Wi = tf.Variable(Wi_val)\n",
    "        bi = tf.Variable(bi_val)\n",
    "        W_last.append(Wi)\n",
    "        b_last.append(bi)\n",
    "            \n",
    "        return W, b, W_last, b_last, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coresets, y_coresets = [], []\n",
    "x_testsets, y_testsets = [], []\n",
    "\n",
    "all_acc = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen.max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "head = 0\n",
    "batch_size = None\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "print(bsize)\n",
    "hidden_size = [100, 100]\n",
    "task_id = 0\n",
    "no_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 2.447080135\n",
      "Epoch: 0006 cost= 2.092496395\n",
      "Epoch: 0011 cost= 1.759548306\n",
      "Epoch: 0016 cost= 1.370782137\n",
      "Epoch: 0021 cost= 1.003718853\n",
      "Epoch: 0026 cost= 0.735086977\n",
      "Epoch: 0031 cost= 0.569361389\n",
      "Epoch: 0036 cost= 0.473297894\n",
      "Epoch: 0041 cost= 0.414886385\n",
      "Epoch: 0046 cost= 0.375195563\n",
      "Epoch: 0051 cost= 0.346212775\n",
      "Epoch: 0056 cost= 0.324049175\n",
      "Epoch: 0061 cost= 0.306318909\n",
      "Epoch: 0066 cost= 0.291606694\n",
      "Epoch: 0071 cost= 0.279080749\n",
      "Epoch: 0076 cost= 0.267867088\n",
      "Epoch: 0081 cost= 0.257729560\n",
      "Epoch: 0086 cost= 0.248397321\n",
      "Epoch: 0091 cost= 0.239673004\n",
      "Epoch: 0096 cost= 0.231451526\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.447080135345459,\n",
       " 2.361976146697998,\n",
       " 2.2880008220672607,\n",
       " 2.220153570175171,\n",
       " 2.1555726528167725,\n",
       " 2.092496395111084,\n",
       " 2.0295581817626953,\n",
       " 1.9655098915100098,\n",
       " 1.8994592428207397,\n",
       " 1.8308889865875244,\n",
       " 1.759548306465149,\n",
       " 1.685410976409912,\n",
       " 1.6087504625320435,\n",
       " 1.530160903930664,\n",
       " 1.4505127668380737,\n",
       " 1.3707821369171143,\n",
       " 1.2920174598693848,\n",
       " 1.215173363685608,\n",
       " 1.1411036252975464,\n",
       " 1.0704680681228638,\n",
       " 1.0037188529968262,\n",
       " 0.9411620497703552,\n",
       " 0.8829575777053833,\n",
       " 0.8292375206947327,\n",
       " 0.7799839973449707,\n",
       " 0.735086977481842,\n",
       " 0.694403886795044,\n",
       " 0.6577697992324829,\n",
       " 0.6249490976333618,\n",
       " 0.5956205725669861,\n",
       " 0.5693613886833191,\n",
       " 0.5457699298858643,\n",
       " 0.5246057510375977,\n",
       " 0.5056639909744263,\n",
       " 0.48865562677383423,\n",
       " 0.4732978940010071,\n",
       " 0.4593808650970459,\n",
       " 0.4467409551143646,\n",
       " 0.43522709608078003,\n",
       " 0.42465683817863464,\n",
       " 0.41488638520240784,\n",
       " 0.4058114290237427,\n",
       " 0.39735567569732666,\n",
       " 0.38946717977523804,\n",
       " 0.38210174441337585,\n",
       " 0.37519556283950806,\n",
       " 0.3686915338039398,\n",
       " 0.3625675141811371,\n",
       " 0.3568047881126404,\n",
       " 0.3513635993003845,\n",
       " 0.346212774515152,\n",
       " 0.3413325548171997,\n",
       " 0.33669063448905945,\n",
       " 0.3322735130786896,\n",
       " 0.3280675411224365,\n",
       " 0.324049174785614,\n",
       " 0.3202004134654999,\n",
       " 0.316516250371933,\n",
       " 0.3129870891571045,\n",
       " 0.3095932900905609,\n",
       " 0.30631890892982483,\n",
       " 0.30316007137298584,\n",
       " 0.30011114478111267,\n",
       " 0.2971753776073456,\n",
       " 0.29434412717819214,\n",
       " 0.29160669445991516,\n",
       " 0.28896552324295044,\n",
       " 0.28640133142471313,\n",
       " 0.28390347957611084,\n",
       " 0.28146499395370483,\n",
       " 0.27908074855804443,\n",
       " 0.2767447233200073,\n",
       " 0.2744560241699219,\n",
       " 0.27221426367759705,\n",
       " 0.2700176537036896,\n",
       " 0.2678670883178711,\n",
       " 0.2657621502876282,\n",
       " 0.2636989653110504,\n",
       " 0.2616735100746155,\n",
       " 0.2596854567527771,\n",
       " 0.25772956013679504,\n",
       " 0.2558045983314514,\n",
       " 0.25391149520874023,\n",
       " 0.2520468533039093,\n",
       " 0.25020986795425415,\n",
       " 0.2483973205089569,\n",
       " 0.2466086894273758,\n",
       " 0.24484293162822723,\n",
       " 0.24309931695461273,\n",
       " 0.2413766086101532,\n",
       " 0.23967300355434418,\n",
       " 0.2379886358976364,\n",
       " 0.23632480204105377,\n",
       " 0.23468121886253357,\n",
       " 0.23305711150169373,\n",
       " 0.23145152628421783,\n",
       " 0.22986513376235962,\n",
       " 0.2282969206571579,\n",
       " 0.22674459218978882,\n",
       " 0.22520700097084045]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model.train(x_train, y_train, task_id, no_epochs, bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_weights = ml_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for val in mf_weights:\n",
    "    print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mf_weights[0][0].shape)\n",
    "print(mf_weights[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(mf_weights[1][0].shape)\n",
    "print(mf_weights[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(mf_weights[2][0].shape)\n",
    "print(mf_weights[3][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14400598 -0.06807813  0.15992862  0.04808508 -0.07416071  0.03008125\n",
      "  0.14971207  0.03849918  0.137127    0.09682845 -0.1083013   0.03426158\n",
      "  0.14379996 -0.03349213  0.11760319  0.06680155  0.1118591   0.03698244\n",
      "  0.0767782   0.04276764  0.16816375 -0.02994343  0.01764071 -0.11195186\n",
      " -0.0388854  -0.1788747   0.15569273  0.07509556  0.01936444 -0.04185652\n",
      " -0.05301884  0.09146661 -0.11130648  0.08092329 -0.14070068  0.2234264\n",
      " -0.00266611 -0.10498814  0.0369416   0.03933246 -0.03040194 -0.03673282\n",
      "  0.1090413   0.09109873  0.10439321  0.11114012  0.01988952  0.09917659\n",
      "  0.02881826 -0.07612276 -0.08050232  0.17495959 -0.05906625  0.02364575\n",
      "  0.05356662 -0.05707119 -0.09027763 -0.08782687  0.07584154  0.14798243\n",
      " -0.01086231  0.10410107  0.09893222 -0.04288416  0.13437946  0.13890302\n",
      "  0.06619617 -0.06930479  0.23255861  0.03252388  0.12364687 -0.05095023\n",
      "  0.01807847  0.1402511  -0.03833523 -0.06870493 -0.13954757 -0.03303906\n",
      "  0.03370167  0.05378075  0.16298634  0.01089726 -0.02278487  0.02577476\n",
      " -0.15060128  0.01346119 -0.04770267 -0.00643235 -0.02404324  0.08985818\n",
      " -0.09970268  0.11969381  0.18718119  0.13258335 -0.02565822  0.13107832\n",
      " -0.13271412  0.10502502 -0.10382745  0.15801613]\n",
      "[ 0.08882336 -0.02526124  0.06183247 -0.00341703 -0.128181    0.06523044\n",
      " -0.06998323  0.01369701 -0.07040714  0.06463946 -0.13803118  0.15383154\n",
      " -0.08645875 -0.03683852 -0.08464809  0.15891816  0.2359937   0.08837941\n",
      " -0.01878819 -0.03849677 -0.08508109  0.06045626 -0.03423754 -0.02163226\n",
      " -0.15492605 -0.05908246  0.05622394 -0.01272678  0.11923283  0.0036254\n",
      " -0.07612979 -0.07707892 -0.15231968  0.10688879 -0.03890411  0.08582673\n",
      " -0.13772324  0.02053757 -0.02674865 -0.05351298  0.0106171  -0.04028092\n",
      " -0.02727842  0.02404615  0.05891794  0.10062439  0.04944587  0.00208073\n",
      "  0.07378246  0.11818875 -0.01742209  0.15394694  0.05925823 -0.08113575\n",
      " -0.01741269  0.0157429  -0.08655452  0.1598588  -0.00067773 -0.12457028\n",
      "  0.01675356 -0.06214754 -0.02733224  0.24383923 -0.05553389 -0.14864162\n",
      "  0.08025885 -0.01850327  0.03712289  0.20207478  0.16748413  0.1076395\n",
      " -0.10354137  0.05869836  0.00253675  0.11801584  0.03270216  0.09445861\n",
      "  0.07894152  0.01008173  0.08245368  0.00833162 -0.06404305  0.03795353\n",
      "  0.10958612 -0.00778174  0.06749447 -0.04217695 -0.01002155  0.1420477\n",
      "  0.19452351 -0.04342676  0.03215347 -0.05025041  0.09063446 -0.02740558\n",
      "  0.14810556  0.19627975 -0.20847066 -0.03088357]\n"
     ]
    }
   ],
   "source": [
    "print(mf_weights[1][0])\n",
    "print(mf_weights[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_variances = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_NN(Cla_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size, \n",
    "        no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None, learning_rate=0.001, \n",
    "        prior_mean=0, prior_var=1):\n",
    "\n",
    "        super(MFVI_NN, self).__init__(input_size, hidden_size, output_size, training_size)\n",
    "        m, v, self.size = self.create_weights(\n",
    "            input_size, hidden_size, output_size, prev_means, prev_log_variances)\n",
    "        self.W_m, self.b_m, self.W_last_m, self.b_last_m = m[0], m[1], m[2], m[3]\n",
    "        self.W_v, self.b_v, self.W_last_v, self.b_last_v = v[0], v[1], v[2], v[3]\n",
    "        self.weights = [m, v]\n",
    "\n",
    "        m, v = self.create_prior(input_size, hidden_size, output_size, prev_means, prev_log_variances, prior_mean, prior_var)\n",
    "        self.prior_W_m, self.prior_b_m, self.prior_W_last_m, self.prior_b_last_m = m[0], m[1], m[2], m[3]\n",
    "        self.prior_W_v, self.prior_b_v, self.prior_W_last_v, self.prior_b_last_v = v[0], v[1], v[2], v[3]\n",
    "\n",
    "        self.no_layers = len(self.size) - 1\n",
    "        self.no_train_samples = no_train_samples\n",
    "        self.no_pred_samples = no_pred_samples\n",
    "        self.pred = self._prediction(self.x, self.task_idx, self.no_pred_samples)\n",
    "        self.cost = tf.div(self._KL_term(), training_size) - self._logpred(self.x, self.y, self.task_idx)\n",
    "        \n",
    "        self.assign_optimizer(learning_rate)\n",
    "        self.assign_session()\n",
    "\n",
    "    def _prediction(self, inputs, task_idx, no_samples):\n",
    "        return self._prediction_layer(inputs, task_idx, no_samples)\n",
    "\n",
    "    # this samples a layer at a time\n",
    "    def _prediction_layer(self, inputs, task_idx, no_samples):\n",
    "        K = no_samples\n",
    "        act = tf.tile(tf.expand_dims(inputs, 0), [K, 1, 1])        \n",
    "        for i in range(self.no_layers-1):\n",
    "            din = self.size[i]\n",
    "            dout = self.size[i+1]\n",
    "            eps_w = tf.random_normal((K, din, dout), 0, 1, dtype=tf.float32)\n",
    "            eps_b = tf.random_normal((K, 1, dout), 0, 1, dtype=tf.float32)\n",
    "            \n",
    "            weights = tf.add(tf.multiply(eps_w, tf.exp(0.5*self.W_v[i])), self.W_m[i])\n",
    "            biases = tf.add(tf.multiply(eps_b, tf.exp(0.5*self.b_v[i])), self.b_m[i])\n",
    "            pre = tf.add(tf.einsum('mni,mio->mno', act, weights), biases)\n",
    "            act = tf.nn.relu(pre)\n",
    "        din = self.size[-2]\n",
    "        dout = self.size[-1]\n",
    "        eps_w = tf.random_normal((K, din, dout), 0, 1, dtype=tf.float32)\n",
    "        eps_b = tf.random_normal((K, 1, dout), 0, 1, dtype=tf.float32)\n",
    "\n",
    "        Wtask_m = tf.gather(self.W_last_m, task_idx)\n",
    "        Wtask_v = tf.gather(self.W_last_v, task_idx)\n",
    "        btask_m = tf.gather(self.b_last_m, task_idx)\n",
    "        btask_v = tf.gather(self.b_last_v, task_idx)\n",
    "        weights = tf.add(tf.multiply(eps_w, tf.exp(0.5*Wtask_v)), Wtask_m)\n",
    "        biases = tf.add(tf.multiply(eps_b, tf.exp(0.5*btask_v)), btask_m)\n",
    "        act = tf.expand_dims(act, 3)\n",
    "        weights = tf.expand_dims(weights, 1)\n",
    "        pre = tf.add(tf.reduce_sum(act * weights, 2), biases)\n",
    "\n",
    "        return pre\n",
    "\n",
    "    def _logpred(self, inputs, targets, task_idx):\n",
    "        pred = self._prediction(inputs, task_idx, self.no_train_samples)\n",
    "        targets = tf.tile(tf.expand_dims(targets, 0), [self.no_train_samples, 1, 1])\n",
    "        log_lik = - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=targets))\n",
    "        return log_lik\n",
    "\n",
    "    def _KL_term(self):\n",
    "        kl = 0\n",
    "        for i in range(self.no_layers-1):\n",
    "            din = self.size[i]\n",
    "            dout = self.size[i+1]\n",
    "            m, v = self.W_m[i], self.W_v[i]\n",
    "            m0, v0 = self.prior_W_m[i], self.prior_W_v[i]\n",
    "            const_term = -0.5 * dout * din\n",
    "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
    "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "\n",
    "            m, v = self.b_m[i], self.b_v[i]\n",
    "            m0, v0 = self.prior_b_m[i], self.prior_b_v[i]\n",
    "            const_term = -0.5 * dout\n",
    "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
    "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "\n",
    "        no_tasks = len(self.W_last_m)\n",
    "        din = self.size[-2]\n",
    "        dout = self.size[-1]\n",
    "        for i in range(no_tasks):\n",
    "            m, v = self.W_last_m[i], self.W_last_v[i]\n",
    "            m0, v0 = self.prior_W_last_m[i], self.prior_W_last_v[i]\n",
    "            const_term = -0.5 * dout * din\n",
    "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
    "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "\n",
    "            m, v = self.b_last_m[i], self.b_last_v[i]\n",
    "            m0, v0 = self.prior_b_last_m[i], self.prior_b_last_v[i]\n",
    "            const_term = -0.5 * dout\n",
    "            log_std_diff = 0.5 * tf.reduce_sum(np.log(v0) - v)\n",
    "            mu_diff_term = 0.5 * tf.reduce_sum((tf.exp(v) + (m0 - m)**2) / v0)\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "        return kl\n",
    "\n",
    "    def create_weights(self, in_dim, hidden_size, out_dim, prev_weights, prev_variances):\n",
    "        hidden_size = deepcopy(hidden_size)\n",
    "        hidden_size.append(out_dim)\n",
    "        hidden_size.insert(0, in_dim)\n",
    "        no_params = 0\n",
    "        no_layers = len(hidden_size) - 1\n",
    "        W_m = []\n",
    "        b_m = []\n",
    "        W_last_m = []\n",
    "        b_last_m = []\n",
    "        W_v = []\n",
    "        b_v = []\n",
    "        W_last_v = []\n",
    "        b_last_v = []\n",
    "        for i in range(no_layers-1):\n",
    "            din = hidden_size[i]\n",
    "            dout = hidden_size[i+1]\n",
    "            if prev_weights is None:\n",
    "                Wi_m_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
    "                bi_m_val = tf.truncated_normal([dout], stddev=0.1)\n",
    "                Wi_v_val = tf.constant(-6.0, shape=[din, dout])\n",
    "                bi_v_val = tf.constant(-6.0, shape=[dout])\n",
    "            else:\n",
    "                Wi_m_val = prev_weights[0][i]\n",
    "                bi_m_val = prev_weights[1][i]\n",
    "                if prev_variances is None:\n",
    "                    Wi_v_val = tf.constant(-6.0, shape=[din, dout])\n",
    "                    bi_v_val = tf.constant(-6.0, shape=[dout])\n",
    "                else:\n",
    "                    Wi_v_val = prev_variances[0][i]\n",
    "                    bi_v_val = prev_variances[1][i]\n",
    "\n",
    "            Wi_m = tf.Variable(Wi_m_val)\n",
    "            bi_m = tf.Variable(bi_m_val)\n",
    "            Wi_v = tf.Variable(Wi_v_val)\n",
    "            bi_v = tf.Variable(bi_v_val)\n",
    "            W_m.append(Wi_m)\n",
    "            b_m.append(bi_m)\n",
    "            W_v.append(Wi_v)\n",
    "            b_v.append(bi_v)\n",
    "\n",
    "        # if there are previous tasks\n",
    "        if prev_weights is not None and prev_variances is not None:\n",
    "            prev_Wlast_m = prev_weights[2]\n",
    "            prev_blast_m = prev_weights[3]\n",
    "            prev_Wlast_v = prev_variances[2]\n",
    "            prev_blast_v = prev_variances[3]\n",
    "            no_prev_tasks = len(prev_Wlast_m)\n",
    "            for i in range(no_prev_tasks):\n",
    "                W_i_m = prev_Wlast_m[i]\n",
    "                b_i_m = prev_blast_m[i]\n",
    "                Wi_m = tf.Variable(W_i_m)\n",
    "                bi_m = tf.Variable(b_i_m)\n",
    "\n",
    "                W_i_v = prev_Wlast_v[i]\n",
    "                b_i_v = prev_blast_v[i]\n",
    "                Wi_v = tf.Variable(W_i_v)\n",
    "                bi_v = tf.Variable(b_i_v)\n",
    "                \n",
    "                W_last_m.append(Wi_m)\n",
    "                b_last_m.append(bi_m)\n",
    "                W_last_v.append(Wi_v)\n",
    "                b_last_v.append(bi_v)\n",
    "\n",
    "        din = hidden_size[-2]\n",
    "        dout = hidden_size[-1]\n",
    "\n",
    "        # if point estimate is supplied\n",
    "        if prev_weights is not None and prev_variances is None:\n",
    "            Wi_m_val = prev_weights[2][0]\n",
    "            bi_m_val = prev_weights[3][0]\n",
    "        else:\n",
    "            Wi_m_val = tf.truncated_normal([din, dout], stddev=0.1)\n",
    "            bi_m_val = tf.truncated_normal([dout], stddev=0.1)\n",
    "        Wi_v_val = tf.constant(-6.0, shape=[din, dout])\n",
    "        bi_v_val = tf.constant(-6.0, shape=[dout])\n",
    "\n",
    "        Wi_m = tf.Variable(Wi_m_val)\n",
    "        bi_m = tf.Variable(bi_m_val)\n",
    "        Wi_v = tf.Variable(Wi_v_val)\n",
    "        bi_v = tf.Variable(bi_v_val)\n",
    "        W_last_m.append(Wi_m)\n",
    "        b_last_m.append(bi_m)\n",
    "        W_last_v.append(Wi_v)\n",
    "        b_last_v.append(bi_v)\n",
    "            \n",
    "        return [W_m, b_m, W_last_m, b_last_m], [W_v, b_v, W_last_v, b_last_v], hidden_size\n",
    "\n",
    "    def create_prior(self, in_dim, hidden_size, out_dim, prev_weights, prev_variances, prior_mean, prior_var):\n",
    "        hidden_size = deepcopy(hidden_size)\n",
    "        hidden_size.append(out_dim)\n",
    "        hidden_size.insert(0, in_dim)\n",
    "        no_params = 0\n",
    "        no_layers = len(hidden_size) - 1\n",
    "        W_m = []\n",
    "        b_m = []\n",
    "        W_last_m = []\n",
    "        b_last_m = []\n",
    "        W_v = []\n",
    "        b_v = []\n",
    "        W_last_v = []\n",
    "        b_last_v = []\n",
    "        for i in range(no_layers-1):\n",
    "            din = hidden_size[i]\n",
    "            dout = hidden_size[i+1]\n",
    "            if prev_weights is not None and prev_variances is not None:\n",
    "                Wi_m = prev_weights[0][i]\n",
    "                bi_m = prev_weights[1][i]\n",
    "                Wi_v = np.exp(prev_variances[0][i])\n",
    "                bi_v = np.exp(prev_variances[1][i])\n",
    "            else:\n",
    "                Wi_m = prior_mean\n",
    "                bi_m = prior_mean\n",
    "                Wi_v = prior_var\n",
    "                bi_v = prior_var\n",
    "\n",
    "            W_m.append(Wi_m)\n",
    "            b_m.append(bi_m)\n",
    "            W_v.append(Wi_v)\n",
    "            b_v.append(bi_v)\n",
    "\n",
    "        # if there are previous tasks\n",
    "        if prev_weights is not None and prev_variances is not None:\n",
    "            prev_Wlast_m = prev_weights[2]\n",
    "            prev_blast_m = prev_weights[3]\n",
    "            prev_Wlast_v = prev_variances[2]\n",
    "            prev_blast_v = prev_variances[3]\n",
    "            no_prev_tasks = len(prev_Wlast_m)\n",
    "            for i in range(no_prev_tasks):\n",
    "                Wi_m = prev_Wlast_m[i]\n",
    "                bi_m = prev_blast_m[i]\n",
    "                Wi_v = np.exp(prev_Wlast_v[i])\n",
    "                bi_v = np.exp(prev_blast_v[i])\n",
    "                \n",
    "                W_last_m.append(Wi_m)\n",
    "                b_last_m.append(bi_m)\n",
    "                W_last_v.append(Wi_v)\n",
    "                b_last_v.append(bi_v)\n",
    "\n",
    "        din = hidden_size[-2]\n",
    "        dout = hidden_size[-1]\n",
    "        Wi_m = prior_mean\n",
    "        bi_m = prior_mean\n",
    "        Wi_v = prior_var\n",
    "        bi_v = prior_var\n",
    "        W_last_m.append(Wi_m)\n",
    "        b_last_m.append(bi_m)\n",
    "        W_last_v.append(Wi_v)\n",
    "        b_last_v.append(bi_v)\n",
    "            \n",
    "        return [W_m, b_m, W_last_m, b_last_m], [W_v, b_v, W_last_v, b_last_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MOIDHA~1\\AppData\\Local\\Temp/ipykernel_14576/2654922537.py:21: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "mf_model = MFVI_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, prev_log_variances=mf_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 4.277204037\n",
      "Epoch: 0006 cost= 4.177356720\n",
      "Epoch: 0011 cost= 4.128238678\n",
      "Epoch: 0016 cost= 4.085588932\n",
      "Epoch: 0021 cost= 4.102339268\n",
      "Epoch: 0026 cost= 4.066158295\n",
      "Epoch: 0031 cost= 4.070765972\n",
      "Epoch: 0036 cost= 4.042218685\n",
      "Epoch: 0041 cost= 4.037230968\n",
      "Epoch: 0046 cost= 4.022535324\n",
      "Epoch: 0051 cost= 4.022497654\n",
      "Epoch: 0056 cost= 3.999462366\n",
      "Epoch: 0061 cost= 4.005934715\n",
      "Epoch: 0066 cost= 3.991479874\n",
      "Epoch: 0071 cost= 3.997089863\n",
      "Epoch: 0076 cost= 3.974993944\n",
      "Epoch: 0081 cost= 3.974561930\n",
      "Epoch: 0086 cost= 3.962573290\n",
      "Epoch: 0091 cost= 3.960590363\n",
      "Epoch: 0096 cost= 3.957558155\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.2772040367126465,\n",
       " 4.245637893676758,\n",
       " 4.3589372634887695,\n",
       " 4.224806308746338,\n",
       " 4.164858341217041,\n",
       " 4.177356719970703,\n",
       " 4.210019111633301,\n",
       " 4.13646125793457,\n",
       " 4.159753799438477,\n",
       " 4.2135396003723145,\n",
       " 4.128238677978516,\n",
       " 4.247989654541016,\n",
       " 4.147328853607178,\n",
       " 4.117985725402832,\n",
       " 4.148529052734375,\n",
       " 4.0855889320373535,\n",
       " 4.132907390594482,\n",
       " 4.108402252197266,\n",
       " 4.0854973793029785,\n",
       " 4.105916976928711,\n",
       " 4.102339267730713,\n",
       " 4.074819564819336,\n",
       " 4.076578617095947,\n",
       " 4.09417724609375,\n",
       " 4.069313049316406,\n",
       " 4.066158294677734,\n",
       " 4.0602312088012695,\n",
       " 4.0648112297058105,\n",
       " 4.083983898162842,\n",
       " 4.084514617919922,\n",
       " 4.070765972137451,\n",
       " 4.059077739715576,\n",
       " 4.0660719871521,\n",
       " 4.071957588195801,\n",
       " 4.046168804168701,\n",
       " 4.0422186851501465,\n",
       " 4.06585693359375,\n",
       " 4.058926105499268,\n",
       " 4.057159423828125,\n",
       " 4.046431064605713,\n",
       " 4.037230968475342,\n",
       " 4.04102897644043,\n",
       " 4.035149097442627,\n",
       " 4.037797451019287,\n",
       " 4.032679080963135,\n",
       " 4.02253532409668,\n",
       " 4.031850814819336,\n",
       " 4.027780055999756,\n",
       " 4.030267238616943,\n",
       " 4.047558784484863,\n",
       " 4.022497653961182,\n",
       " 4.020975112915039,\n",
       " 4.0261335372924805,\n",
       " 4.035214424133301,\n",
       " 4.024043083190918,\n",
       " 3.999462366104126,\n",
       " 4.006076335906982,\n",
       " 4.037302494049072,\n",
       " 4.018530368804932,\n",
       " 4.019288063049316,\n",
       " 4.005934715270996,\n",
       " 4.016989707946777,\n",
       " 4.002752304077148,\n",
       " 4.016142845153809,\n",
       " 4.026695251464844,\n",
       " 3.9914798736572266,\n",
       " 3.994791269302368,\n",
       " 3.9969277381896973,\n",
       " 4.013901710510254,\n",
       " 3.998903274536133,\n",
       " 3.9970898628234863,\n",
       " 3.994314432144165,\n",
       " 4.001731872558594,\n",
       " 3.988640785217285,\n",
       " 3.9915428161621094,\n",
       " 3.974993944168091,\n",
       " 3.9772679805755615,\n",
       " 3.9714910984039307,\n",
       " 3.972148895263672,\n",
       " 3.972748041152954,\n",
       " 3.974561929702759,\n",
       " 3.9698681831359863,\n",
       " 3.965851306915283,\n",
       " 3.9746623039245605,\n",
       " 3.969895601272583,\n",
       " 3.962573289871216,\n",
       " 3.9689276218414307,\n",
       " 3.979750156402588,\n",
       " 3.9625563621520996,\n",
       " 3.968306541442871,\n",
       " 3.960590362548828,\n",
       " 3.966703414916992,\n",
       " 3.959522247314453,\n",
       " 3.9487452507019043,\n",
       " 3.965452194213867,\n",
       " 3.9575581550598145,\n",
       " 3.9524853229522705,\n",
       " 3.9558753967285156,\n",
       " 3.9575371742248535,\n",
       " 3.945437431335449]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_model.train(x_train, y_train, head, no_epochs, bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da0a8a841d3d75c9d77813dea78da1c5555f1ef32e8e5a28b3e707c9294bd0c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
