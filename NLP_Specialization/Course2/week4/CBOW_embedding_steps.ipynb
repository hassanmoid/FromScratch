{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moidhassan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utils2 import get_dict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n"
     ]
    }
   ],
   "source": [
    "# Define a corpus\n",
    "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n",
    "\n",
    "# Print original corpus\n",
    "print(f'Corpus:  {corpus}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'tokenize' function that will include the steps previously seen\n",
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.'\n",
    "             or emoji.get_emoji_regexp().search(ch)\n",
    "           ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
      "Words (tokens):  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print new corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Save tokenized version of corpus into 'words' variable\n",
    "words = tokenize(corpus)\n",
    "\n",
    "# Print the tokenized version of the corpus\n",
    "print(f'Words (tokens):  {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this with any sentence\n",
    "tokenize(\"I am happy because I am learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'get_windows' function\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'word']\t❤️\n",
      "['❤️', 'embeddings']\tword\n",
      "['word', 'in']\tembeddings\n",
      "['embeddings', '.']\tin\n",
      "['in', 'i']\t.\n",
      "['.', 'do']\ti\n",
      "['i', '.']\tdo\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
    "for x, y in get_windows(tokenize(corpus), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming words into vectors for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I am learning\n",
      "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n",
      "Index of the word 'i':   3\n",
      "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n",
      "Word which has index 2:   happy\n",
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "\n",
    "# Define new corpus\n",
    "corpus = 'I am happy because I am learning'\n",
    "\n",
    "# Print new corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Save tokenized version of corpus into 'words' variable\n",
    "words = tokenize(corpus)\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "print(word2Ind)\n",
    "\n",
    "# Print value for the key 'i' within word2Ind dictionary\n",
    "print(\"Index of the word 'i':  \",word2Ind['i'])\n",
    "\n",
    "# Print 'Ind2word' dictionary\n",
    "print(Ind2word)\n",
    "\n",
    "# Print value for the key '2' within Ind2word dictionary\n",
    "print(\"Word which has index 2:  \",Ind2word[2] )\n",
    "\n",
    "# Save length of word2Ind dictionary into the 'V' variable\n",
    "V = len(word2Ind)\n",
    "\n",
    "# Print length of word2Ind dictionary\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Print output of 'word_to_one_hot_vector' function for word 'happy'\n",
    "print(word_to_one_hot_vector('happy', word2Ind, V))\n",
    "\n",
    "# Print output of 'word_to_one_hot_vector' function for word 'learning'\n",
    "print(word_to_one_hot_vector('learning', word2Ind, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting context word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 1., 0.]),\n",
       " array([1., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define list containing context words\n",
    "context_words = ['i', 'am', 'because', 'i']\n",
    "\n",
    "# Create one-hot vectors for each context word using list comprehension\n",
    "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "\n",
    "# Print one-hot vectors for each context word\n",
    "context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute mean of the vectors using numpy\n",
    "np.mean(context_words_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'context_words_to_vector' function that will include the steps previously seen\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\n",
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.  , 0.25, 0.25, 0.  ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\n",
    "context_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print corpus\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus\n",
    "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n",
    "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator function 'get_training_example'\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus using the generator function\n",
    "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu and Softmax activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function that will include the steps previously seen\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
    "\n",
    "# Apply ReLU to it\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'softmax' function that will include the steps previously seen\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Print softmax values for original vector\n",
    "softmax([9, 8, 11, 10, 8.5])\n",
    "\n",
    "print(np.sum(softmax([9, 8, 11, 10, 8.5])) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
    "N = 3\n",
    "\n",
    "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
    "V = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])\n",
    "\n",
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (5, 3) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "# Define the 'get_windows' function as seen in a previous notebook\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "\n",
    "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors\n",
    "\n",
    "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.   0.5  0.  ]\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Save generator object in the 'training_examples' variable with the desired arguments\n",
    "training_examples = get_training_example(words, 2, word2Ind, V)\n",
    "\n",
    "# Get first values from generator\n",
    "x_array, y_array = next(training_examples)\n",
    "\n",
    "# Print context words vector\n",
    "print(x_array)\n",
    "\n",
    "# Print one hot vector of center word\n",
    "print(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]]\n",
      "\n",
      "y:\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Copy vector\n",
    "x = x_array.copy()\n",
    "\n",
    "# Reshape it\n",
    "x.shape = (V, 1)\n",
    "\n",
    "# Print it\n",
    "print(f'x:\\n{x}\\n')\n",
    "\n",
    "# Copy vector\n",
    "y = y_array.copy()\n",
    "\n",
    "# Reshape it\n",
    "y.shape = (V, 1)\n",
    "\n",
    "# Print it\n",
    "print(f'y:\\n{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function as seen in the previous lecture notebook\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "# Define the 'softmax' function as seen in the previous lecture notebook\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w1,b1,w2,b2,x):\n",
    "    \n",
    "    z1 = np.dot(w1,x) + b1\n",
    "    h = relu(z1)\n",
    "    z2 = np.dot(w2,h) + b2\n",
    "    y_hat = softmax(z2)\n",
    "\n",
    "    return y_hat,h\n",
    "\n",
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    # Fill the loss variable with your code\n",
    "    loss = np.sum(-np.log(y_predicted)*y_actual)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4650152923611106\n"
     ]
    }
   ],
   "source": [
    "y_hat,h = forward(W1,b1,W2,b2,x)\n",
    "# Print value of cross entropy loss for prediction and target value\n",
    "print(cross_entropy_loss(y_hat, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y,y_hat,h,w1,b1,w2,b2,x,alpha = 0.03):\n",
    "\n",
    "    grad_b2 = y_hat - y\n",
    "    grad_W2 = np.dot(y_hat - y, h.T)\n",
    "    grad_b1 = relu(np.dot(w2.T, y_hat - y))\n",
    "    grad_W1 = np.dot(relu(np.dot(w2.T, y_hat - y)), x.T)\n",
    "\n",
    "    W1_new = w1 - alpha * grad_W1\n",
    "    W2_new = w2 - alpha * grad_W2\n",
    "    b1_new = b1 - alpha * grad_b1\n",
    "    b2_new = b2 - alpha * grad_b2\n",
    "\n",
    "    return W1_new,b1_new,W2_new,b2_new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_new,b1_new,W2_new,b2_new = backprop(y,y_hat,h,W1,b1,W2,b2,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4310498009586927\n"
     ]
    }
   ],
   "source": [
    "y_hat,h = forward(W1_new,b1_new,W2_new,b2_new,x)\n",
    "# Print value of cross entropy loss for prediction and target value\n",
    "print(cross_entropy_loss(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward prop with epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (5, 3) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n",
      "loss at 1st iteration 1.4650152923611106\n",
      "loss at 2 iteration 1.4310498009586927\n",
      "loss at 3 iteration 1.397790348042014\n",
      "loss at 4 iteration 1.3652380995335804\n",
      "loss at 5 iteration 1.3333932111291222\n",
      "loss at 6 iteration 1.302254837755199\n",
      "loss at 7 iteration 1.2718211490382132\n",
      "loss at 8 iteration 1.242089350518089\n",
      "loss at 9 iteration 1.213055710269879\n",
      "loss at last iteration 1.213055710269879\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "V = 5\n",
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])\n",
    "\n",
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        y_hat,h = forward(W1,b1,W2,b2,x)\n",
    "        # Print value of cross entropy loss for prediction and target value\n",
    "        print(f\"loss at 1st iteration {cross_entropy_loss(y_hat, y)}\")\n",
    "    elif i == 9:\n",
    "        y_hat,h = forward(W1,b1,W2,b2,x)\n",
    "        print(f\"loss at last iteration {cross_entropy_loss(y_hat, y)}\")\n",
    "    else:\n",
    "        W1,b1,W2,b2 = backprop(y,y_hat,h,W1,b1,W2,b2,x)\n",
    "        y_hat,h = forward(W1,b1,W2,b2,x)\n",
    "        print(f\"loss at {i+1} iteration {cross_entropy_loss(y_hat, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 1.213055710269879\n"
     ]
    }
   ],
   "source": [
    "y_hat,h = forward(W1,b1,W2,b2,x)\n",
    "print(f\"loss is {cross_entropy_loss(y_hat, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "because\n",
      "happy\n",
      "i\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "for i in range(V):\n",
    "    print(Ind2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [0.41687358 0.32735501 0.25650771]\n",
      "because: [ 0.08854191  0.22795148 -0.24833717]\n",
      "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
      "i: [ 0.28320538  0.4117634  -0.13373108]\n",
      "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W1[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [-0.23745169 -0.45738218  0.13310965]\n",
      "because: [0.06853979 0.05289671 0.1772054 ]\n",
      "happy: [ 0.25198117  0.05213053 -0.1790735 ]\n",
      "i: [ 0.05515313 -0.04704219  0.36107434]\n",
      "learning: [ 0.31723504 -0.42491515 -0.43959196]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [ 0.08971095 -0.06501358  0.19480868]\n",
      "because: [ 0.07854085  0.1404241  -0.03556588]\n",
      "happy: [ 0.00851446 -0.09369452 -0.27839106]\n",
      "i: [0.16917926 0.18236061 0.11367163]\n",
      "learning: [ 0.36761805 -0.33207929 -0.04975536]\n"
     ]
    }
   ],
   "source": [
    "W3 = (W1+W2.T)/2\n",
    "\n",
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W3[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cs224n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7ade109fcaf6ff10adabf032c9e3d03d273292ea2827ed946949d6cb5f69c2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
