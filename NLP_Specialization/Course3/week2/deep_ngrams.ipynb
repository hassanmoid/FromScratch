{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Welcome to the second assignment of course 3. In this assignment you will explore Recurrent Neural Networks `RNN`.\n",
    "- You will be using the fundamentals of google's [trax](https://github.com/google/trax) package to implement any kind of deeplearning model. \n",
    "\n",
    "By completing this assignment, you will learn how to implement models from scratch:\n",
    "- How to convert a line of text into a tensor\n",
    "- Create an iterator to feed data to the model\n",
    "- Define a GRU model using `trax`\n",
    "- Train the model using `trax`\n",
    "- Compute the accuracy of your model using the perplexity\n",
    "- Predict using your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 21:53:36.633063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "\n",
    "import w2_unittest\n",
    "\n",
    "# set random seed\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A LOVER'S COMPLAINT\", 'FROM off a hill whose concave womb reworded']\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/'\n",
    "filename = 'shakespeare_data.txt'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open(os.path.join(dirname, filename)) as files:\n",
    "    for line in files:        \n",
    "        # remove leading and trailing whitespace\n",
    "        pure_line = line.strip()\n",
    "\n",
    "        # if pure_line is not the empty string,\n",
    "        if pure_line:\n",
    "            # append it to the list\n",
    "            lines.append(pure_line)\n",
    "\n",
    "print(lines[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0: A LOVER'S COMPLAINT\n",
      "Sample line at position 999: With this night's revels and expire the term\n"
     ]
    }
   ],
   "source": [
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0: {lines[0]}\")\n",
    "print(f\"Sample line at position 999: {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0: a lover's complaint\n",
      "Sample line at position 999: with this night's revels and expire the term\n"
     ]
    }
   ],
   "source": [
    "# go through each line\n",
    "for i, line in enumerate(lines):\n",
    "    # convert to all lowercase\n",
    "    lines[i] = line.lower()\n",
    "\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0: {lines[0]}\")\n",
    "print(f\"Sample line at position 999: {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 124097\n",
      "Number of lines for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] # Create a holdout validation set\n",
    "lines = lines[:-1000] # Leave the rest for training\n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord('a'): 97\n",
      "ord('b'): 98\n",
      "ord('c'): 99\n",
      "ord(' '): 32\n",
      "ord('x'): 120\n",
      "ord('y'): 121\n",
      "ord('z'): 122\n",
      "ord('1'): 49\n",
      "ord('2'): 50\n",
      "ord('3'): 51\n"
     ]
    }
   ],
   "source": [
    "# View the unique unicode integer associated with each character\n",
    "print(f\"ord('a'): {ord('a')}\")\n",
    "print(f\"ord('b'): {ord('b')}\")\n",
    "print(f\"ord('c'): {ord('c')}\")\n",
    "print(f\"ord(' '): {ord(' ')}\")\n",
    "print(f\"ord('x'): {ord('x')}\")\n",
    "print(f\"ord('y'): {ord('y')}\")\n",
    "print(f\"ord('z'): {ord('z')}\")\n",
    "print(f\"ord('1'): {ord('1')}\")\n",
    "print(f\"ord('2'): {ord('2')}\")\n",
    "print(f\"ord('3'): {ord('3')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: line_to_tensor\n",
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \"\"\"Turns a line of text into a tensor\n",
    "\n",
    "    Args:\n",
    "        line (str): A single line of text.\n",
    "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of integers (unicode values) for the characters in the `line`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the tensor as an empty list\n",
    "    tensor = []\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # for each character:\n",
    "    for c in line:\n",
    "        \n",
    "        # convert to unicode int\n",
    "        c_int = ord(c)\n",
    "        \n",
    "        # append the unicode integer to the tensor list\n",
    "        tensor.append(c_int)\n",
    "    \n",
    "    # include the end-of-sentence integer\n",
    "    tensor.append(EOS_int)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 98, 99, 32, 120, 121, 122, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your output\n",
    "line_to_tensor('abc xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_line_to_tensor(line_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
    "        max_length (int): maximum length of the output tensor.\n",
    "        NOTE: max_length includes the end-of-sentence character that will be added\n",
    "                to the tensor.  \n",
    "                Keep in mind that the length of the tensor is always 1 + the length\n",
    "                of the original line of characters.\n",
    "        data_lines (list): list of the sentences to group into batches.\n",
    "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
    "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
    "\n",
    "    Yields:\n",
    "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
    "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
    "    \"\"\"\n",
    "    # initialize the index that points to the current position in the lines index array\n",
    "    index = 0\n",
    "    \n",
    "    # initialize the list that will contain the current batch\n",
    "    cur_batch = []\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(data_lines)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle line indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    while True:\n",
    "        \n",
    "        # if the index is greater than or equal to the number of lines in data_lines\n",
    "        if index >= num_lines:\n",
    "            # then reset the index to 0\n",
    "            index = 0\n",
    "            # shuffle line indexes if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index) \n",
    "                            \n",
    "        # get a line at the `lines_index[index]` position in data_lines\n",
    "        line = data_lines[lines_index[index]]\n",
    "        \n",
    "        # if the length of the line is less than max_length\n",
    "        if len(line) < max_length:\n",
    "            # append the line to the current batch\n",
    "            cur_batch.append(line)\n",
    "            \n",
    "        # increment the index by one\n",
    "        index += 1\n",
    "        \n",
    "        # if the current batch is now equal to the desired batch size\n",
    "        if len(cur_batch) == batch_size:\n",
    "            \n",
    "            batch = []\n",
    "            mask = []\n",
    "            \n",
    "            # go through each line (li) in cur_batch\n",
    "            for li in cur_batch:\n",
    "                # convert the line (li) to a tensor of integers\n",
    "                tensor = line_to_tensor(li)\n",
    "                \n",
    "                # Create a list of zeros to represent the padding\n",
    "                # so that the tensor plus padding will have length `max_length`\n",
    "                pad = [0] * (max_length - len(tensor))\n",
    "                \n",
    "                # combine the tensor plus pad\n",
    "                tensor_pad = tensor + pad\n",
    "                \n",
    "                # append the padded tensor to the batch\n",
    "                batch.append(tensor_pad)\n",
    "\n",
    "                # A mask for this tensor_pad is 1 whereever tensor_pad is not\n",
    "                # 0 and 0 whereever tensor_pad is 0, i.e. if tensor_pad is\n",
    "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
    "                # [1, 1, 1, 0, 0, 0]\n",
    "                example_mask = [1 if val!=0 else 0 for val in tensor_pad]\n",
    "                mask.append(example_mask) # @ KEEPTHIS\n",
    "               \n",
    "            # convert the batch (data type list) to a numpy array\n",
    "            batch_np_arr = np.array(batch)\n",
    "            mask_np_arr = np.array(mask)\n",
    "            \n",
    "            ### END CODE HERE ##\n",
    "            \n",
    "            # Yield two copies of the batch and mask.\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "            \n",
    "            # reset the current batch to an empty list\n",
    "            cur_batch = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
       "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
       " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
       "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
       " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out your data generator\n",
    "tmp_lines = ['12345678901', #length 11\n",
    "             '123456789', # length 9\n",
    "             '234567890', # length 9\n",
    "             '345678901'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = data_generator(batch_size=2, \n",
    "                              max_length=10, \n",
    "                              data_lines=tmp_lines,\n",
    "                              shuffle=False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_data_generator(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "infinite_data_generator = itertools.cycle(\n",
    "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
    "print(len(ten_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: GRULM\n",
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    \"\"\"Returns a GRU language model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
    "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    model = tl.Serial( \n",
    "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(n_units=vocab_size), # Dense layer\n",
    "      tl.LogSoftmax(), # Log Softmax\n",
    "    ) \n",
    "    ### END CODE HERE ###\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# testing your model\n",
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_GRULM(GRULM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used lines from the dataset: 25881\n",
      "Batch size (a power of 2): 32\n",
      "Number of steps to cover one epoch: 808\n"
     ]
    }
   ],
   "source": [
    "def n_used_lines(lines, max_length):\n",
    "    '''\n",
    "    Args: \n",
    "    lines: all lines of text an array of lines\n",
    "    max_length - max_length of a line in order to be considered an int\n",
    "    output_dir - folder to save your file an int\n",
    "    Return:\n",
    "    number of efective examples\n",
    "    '''\n",
    "\n",
    "    n_lines = 0\n",
    "    for l in lines:\n",
    "        if len(l) <= max_length:\n",
    "            n_lines += 1\n",
    "    return n_lines\n",
    "\n",
    "num_used_lines = n_used_lines(lines, 32)\n",
    "print('Number of used lines from the dataset:', num_used_lines)\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(num_used_lines/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(model, data_generator, lines, eval_lines, batch_size=32, max_length=64, n_steps=1, output_dir='model/'): \n",
    "    \"\"\"Function that trains the model\n",
    "\n",
    "    Args:\n",
    "        model (trax.layers.combinators.Serial): GRU model.\n",
    "        data_generator (function): Data generator function.\n",
    "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
    "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
    "        lines (list): List of lines to use for training. Defaults to lines.\n",
    "        eval_lines (list): List of lines to use for evaluation. Defaults to eval_lines.\n",
    "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
    "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    bare_train_generator = data_generator(batch_size=batch_size, max_length=max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size=batch_size, max_length=max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "    \n",
    "    train_task = training.TrainTask( \n",
    "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
    "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
    "        optimizer=trax.optimizers.Adam(0.0005)     # Don't forget to add the learning rate parameter TO 0.0005\n",
    "    ) \n",
    "    \n",
    "    eval_task = training.EvalTask( \n",
    "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # Don't forget to instantiate these objects\n",
    "        n_eval_batches=3  # For better evaluation accuracy in reasonable time \n",
    "    ) \n",
    "    \n",
    "    training_loop = training.Loop(model, \n",
    "                                  train_task, \n",
    "                                  eval_tasks=[eval_task], \n",
    "                                  output_dir=output_dir) \n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # We return this because it contains a handle to the model, which has the weights etc.\n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moidhassan/miniconda3/envs/cs224n/lib/python3.7/site-packages/jax/lib/xla_bridge.py:356: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
      "/home/moidhassan/miniconda3/envs/cs224n/lib/python3.7/site-packages/jax/lib/xla_bridge.py:369: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 3411200\n",
      "Step      1: Ran 1 train steps in 4.77 secs\n",
      "Step      1: train CrossEntropyLoss |  5.54521561\n",
      "Step      1: eval  CrossEntropyLoss |  5.54100625\n",
      "Step      1: eval          Accuracy |  0.16224351\n"
     ]
    }
   ],
   "source": [
    "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
    "import itertools\n",
    "output_dir = './model/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_dir)\n",
    "except OSError as e:\n",
    "    pass\n",
    "\n",
    "training_loop = train_model(GRULM(), data_generator, lines=lines, eval_lines=eval_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 3411200\n",
      "Step      1: Ran 1 train steps in 11.93 secs\n",
      "Step      1: train CrossEntropyLoss |  5.54509449\n",
      "Step      1: eval  CrossEntropyLoss |  5.54269346\n",
      "Step      1: eval          Accuracy |  0.09107612\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function. This cell may take some seconds to execute.\n",
    "w2_unittest.test_train_model(train_model, GRULM(), data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(preds, target):\n",
    "    \"\"\"Function to test the model.\n",
    "\n",
    "    Args:\n",
    "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
    "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
    "\n",
    "    Returns:\n",
    "        float: log_perplexity of the model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    log_p = np.sum(preds * tl.one_hot(target, preds.shape[-1]), axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)          # You should check if the target equals 0\n",
    "    log_p = log_p * non_pad                             # Get rid of the padding    \n",
    "    \n",
    "    log_ppx = np.sum(log_p, axis=1) / np.sum(non_pad, axis=1) # Remember to set the axis properly when summing up\n",
    "    log_ppx = np.mean(log_ppx) #Â Compute the mean of the previous expression\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 1.7646704 5.8396473\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Testing \n",
    "model = GRULM()\n",
    "model.init_from_file('model.pkl.gz')\n",
    "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
    "preds = model(batch[0])\n",
    "log_ppx = test_model(preds, batch[1])\n",
    "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "pretrained_model = GRULM()\n",
    "pretrained_model.init_from_file('model.pkl.gz')\n",
    "w2_unittest.unittest_test_model(test_model, pretrained_model)\n",
    "del pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i habe them frinks in that commi\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to generate some news sentence\n",
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n",
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bid not foulf to firghy seeson,\n",
      "so as he us bray us away]\n",
      "but chay't inkopine ther teep an\n"
     ]
    }
   ],
   "source": [
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
