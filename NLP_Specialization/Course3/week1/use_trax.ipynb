{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-29 16:27:34.419840: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # regular ol' numpy\n",
    "\n",
    "from trax import layers as tl  # core building block\n",
    "from trax import shapes  # data signatures: dimensionality and type\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                         1.3.9\n"
     ]
    }
   ],
   "source": [
    "# Trax version 1.3.9 or better \n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Serial\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Layers\n",
    "# Create a relu trax layer\n",
    "relu = tl.Relu()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", relu.name)\n",
    "print(\"expected inputs :\", relu.n_in)\n",
    "print(\"promised outputs :\", relu.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = relu(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10. -20. -30.   1.   2.   3.]\n"
     ]
    }
   ],
   "source": [
    "# Create a concatenate trax layer\n",
    "concat = tl.Concatenate()\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat.name)\n",
    "print(\"expected inputs :\", concat.n_in)\n",
    "print(\"promised outputs :\", concat.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 3\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.]\n",
      "x3 : [0.99 1.98 2.97] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n"
     ]
    }
   ],
   "source": [
    "# Configure a concatenate layer\n",
    "concat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat_3.name)\n",
    "print(\"expected inputs :\", concat_3.n_in)\n",
    "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "x3 = x2 * 0.99\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2)\n",
    "print(\"x3 :\", x3, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat_3([x1, x2, x3])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Concatenate in module trax.layers.combinators:\n",
      "\n",
      "class Concatenate(trax.layers.base.Layer)\n",
      " |  Concatenate(n_items=2, axis=-1)\n",
      " |  \n",
      " |  Concatenates a number of tensors into a single tensor.\n",
      " |  \n",
      " |  For example::\n",
      " |  \n",
      " |      x = np.array([1, 2])\n",
      " |      y = np.array([3, 4])\n",
      " |      z = np.array([5, 6])\n",
      " |      concat3 = tl.Concatenate(n_items=3)\n",
      " |      z = concat3((x, y, z))  # z = [1, 2, 3, 4, 5, 6]\n",
      " |  \n",
      " |  Use the `axis` argument to specify on which axis to concatenate the tensors.\n",
      " |  By default it's the last axis, `axis=-1`, and `n_items=2`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Concatenate\n",
      " |      trax.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_items=2, axis=-1)\n",
      " |      Creates a partially initialized, unconnected layer instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        n_in: Number of inputs expected by this layer.\n",
      " |        n_out: Number of outputs promised by this layer.\n",
      " |        name: Class-like name for this layer; for use when printing this layer.\n",
      " |        sublayers_to_print: Sublayers to display when printing out this layer;\n",
      " |          if None (the default), display all sublayers.\n",
      " |  \n",
      " |  forward(self, xs)\n",
      " |      Executes this layer as part of a forward pass through the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __call__(self, x, weights=None, state=None, rng=None)\n",
      " |      Makes layers callable; for use in tests or interactive settings.\n",
      " |      \n",
      " |      This convenience method helps library users play with, test, or otherwise\n",
      " |      probe the behavior of layers outside of a full training environment. It\n",
      " |      presents the layer as callable function from inputs to outputs, with the\n",
      " |      option of manually specifying weights and non-parameter state per individual\n",
      " |      call. For convenience, weights and non-parameter state are cached per layer\n",
      " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
      " |      and acquiring non-empty values either by initialization or from values\n",
      " |      explicitly provided via the weights and state keyword arguments, in which\n",
      " |      case the old weights will be preserved, and the state will be updated.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
      " |        state: State or `None`; if `None`, use self's cached state value.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Renders this layer as a medium-detailed string, to help in debugging.\n",
      " |      \n",
      " |      Subclasses should aim for high-signal/low-noise when overriding this\n",
      " |      method.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A high signal-to-noise string representing this layer.\n",
      " |  \n",
      " |  __setattr__(self, attr, value)\n",
      " |      Sets class attributes and protects from typos.\n",
      " |      \n",
      " |      In Trax layers, we only allow to set the following public attributes::\n",
      " |      \n",
      " |        - weights\n",
      " |        - state\n",
      " |        - rng\n",
      " |      \n",
      " |      This function prevents from setting other public attributes to avoid typos,\n",
      " |      for example, this is not possible and would be without this function::\n",
      " |      \n",
      " |        [typo]   layer.weighs = some_tensor\n",
      " |      \n",
      " |      If you need to set other public attributes in a derived class (which we\n",
      " |      do not recommend as in almost all cases it suffices to use a private\n",
      " |      attribute), override self._settable_attrs to include the attribute name.\n",
      " |      \n",
      " |      Args:\n",
      " |        attr: Name of the attribute to be set.\n",
      " |        value: Value to be assigned to the attribute.\n",
      " |  \n",
      " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
      " |      Custom backward pass to propagate gradients in a custom way.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
      " |        output: The result of running this layer on inputs.\n",
      " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
      " |            and shape must match output.\n",
      " |        weights: This layer's weights.\n",
      " |        state: This layer's state prior to the current forward pass.\n",
      " |        new_state: This layer's state after the current forward pass.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The custom gradient signal for the input. Note that we need to return\n",
      " |        a gradient for each argument of forward, so it will usually be a tuple\n",
      " |        of signals: the gradient for inputs and weights.\n",
      " |  \n",
      " |  init(self, input_signature, rng=None, use_cache=False)\n",
      " |      Initializes weights/state of this layer and its sublayers recursively.\n",
      " |      \n",
      " |      Initialization creates layer weights and state, for layers that use them.\n",
      " |      It derives the necessary array shapes and data types from the layer's input\n",
      " |      signature, which is itself just shape and data type information.\n",
      " |      \n",
      " |      For layers without weights or state, this method safely does nothing.\n",
      " |      \n",
      " |      This method is designed to create weights/state only once for each layer\n",
      " |      instance, even if the same layer instance occurs in multiple places in the\n",
      " |      network. This enables weight sharing to be implemented as layer sharing.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or list/tuple of `ShapeDtype` instances.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |        use_cache: If `True`, and if this layer instance has already been\n",
      " |            initialized elsewhere in the network, then return special marker\n",
      " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
      " |            Else return this layer's newly initialized weights and state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `(weights, state)` tuple.\n",
      " |  \n",
      " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
      " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
      " |      \n",
      " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
      " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
      " |      `'input_signature'`, which are used to initialize this layer.\n",
      " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
      " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
      " |      `'flat_state'` item and the state it not restored either.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_name: Name/path of the pickled weights/state file.\n",
      " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
      " |            initialize both weights and state.\n",
      " |        input_signature: Input signature to be used instead of the one from file.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `(weights, state)` tuple.\n",
      " |  \n",
      " |  init_weights_and_state(self, input_signature)\n",
      " |      Initializes weights and state, to handle input with the given signature.\n",
      " |      \n",
      " |      A layer subclass must override this method if the layer uses weights or\n",
      " |      state. To initialize weights, set `self.weights` to desired (typically\n",
      " |      random) values. To initialize state (uncommon), set `self.state` to desired\n",
      " |      starting values.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or a list/tuple of `ShapeDtype` instances.\n",
      " |  \n",
      " |  output_signature(self, input_signature)\n",
      " |      Returns output signature this layer would give for `input_signature`.\n",
      " |  \n",
      " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
      " |      Applies this layer as a pure function with no optional args.\n",
      " |      \n",
      " |      This method exposes the layer's computation as a pure function. This is\n",
      " |      especially useful for JIT compilation. Do not override, use `forward`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: A tuple or list of trainable weights, with one element for this\n",
      " |            layer if this layer has no sublayers, or one for each sublayer if\n",
      " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
      " |            weights, the corresponding weights element is an empty tuple.\n",
      " |        state: Layer-specific non-parameter state that can update between batches.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
      " |          to implement layer sharing in combinators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
      " |        promised by this layer, and are packaged as described in the `Layer`\n",
      " |        class docstring.\n",
      " |  \n",
      " |  save_to_file(self, file_name, weights_only=False, input_signature=None)\n",
      " |      Saves this layer and its sublayers to a pickled checkpoint.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_name: Name/path of the pickled weights/state file.\n",
      " |        weights_only: If `True`, save only the layer's weights. Else\n",
      " |            save both weights and state.\n",
      " |        input_signature: Input signature to be used.\n",
      " |  \n",
      " |  weights_and_state_signature(self, input_signature, unsafe=False)\n",
      " |      Return a pair containing the signatures of weights and state.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  has_backward\n",
      " |      Returns `True` if this layer provides its own custom backward pass code.\n",
      " |      \n",
      " |      A layer subclass that provides custom backward pass code (for custom\n",
      " |      gradients) must override this method to return `True`.\n",
      " |  \n",
      " |  n_in\n",
      " |      Returns how many tensors this layer expects as input.\n",
      " |  \n",
      " |  n_out\n",
      " |      Returns how many tensors this layer promises as output.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this layer.\n",
      " |  \n",
      " |  rng\n",
      " |      Returns this layer's current single-use random number generator.\n",
      " |      \n",
      " |      Code that wants to base random samples on this generator must explicitly\n",
      " |      split off new generators from it. (See, for example, the `rng` setter code\n",
      " |      below.)\n",
      " |  \n",
      " |  state\n",
      " |      Returns a tuple containing this layer's state; may be empty.\n",
      " |      \n",
      " |      If the layer has sublayers, the state by convention will be\n",
      " |      a tuple of length `len(sublayers)` containing sublayer states.\n",
      " |      Note that in this case self._state only marks which ones are shared.\n",
      " |  \n",
      " |  sublayers\n",
      " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns this layer's weights.\n",
      " |      \n",
      " |      Depending on the layer, the weights can be in the form of:\n",
      " |      \n",
      " |        - an empty tuple\n",
      " |        - a tensor (ndarray)\n",
      " |        - a nested structure of tuples and tensors\n",
      " |      \n",
      " |      If the layer has sublayers, the weights by convention will be\n",
      " |      a tuple of length `len(sublayers)` containing the weights of sublayers.\n",
      " |      Note that in this case self._weights only marks which ones are shared.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tl.Concatenate) #Uncomment this to see the function docstring with explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal shape: (4,) Data Type: <class 'tuple'>\n",
      "Shapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: <class 'trax.shapes.ShapeDtype'>\n",
      "-- Properties --\n",
      "name : LayerNorm\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights : [1. 1. 1. 1.]\n",
      "biases : [0. 0. 0. 0.] \n",
      "\n",
      "-- Inputs --\n",
      "x : [0. 1. 2. 3.]\n",
      "-- Outputs --\n",
      "y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moidhassan/miniconda3/envs/cs224n/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3094: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"ones\")\n",
      "/home/moidhassan/miniconda3/envs/cs224n/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3085: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    }
   ],
   "source": [
    "# Layer initialization\n",
    "norm = tl.LayerNorm()\n",
    "# You first must know what the input data will look like\n",
    "x = np.array([0, 1, 2, 3], dtype=\"float\")\n",
    "\n",
    "# Use the input data signature to get shape and type for initializing weights and biases\n",
    "norm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n",
    "\n",
    "print(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\n",
    "print(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", norm.name)\n",
    "print(\"expected inputs :\", norm.n_in)\n",
    "print(\"promised outputs :\", norm.n_out)\n",
    "# Weights and biases\n",
    "print(\"weights :\", norm.weights[0])\n",
    "print(\"biases :\", norm.weights[1], \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x)\n",
    "\n",
    "# Outputs\n",
    "y = norm(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : TimesTwo\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [1 2 3] \n",
      "\n",
      "-- Outputs --\n",
      "y : [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom layer\n",
    "# In this example you will create a layer to calculate the input times 2\n",
    "\n",
    "def TimesTwo():\n",
    "    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x):\n",
    "        return x * 2\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "\n",
    "# Test it\n",
    "times_two = TimesTwo()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", times_two.name)\n",
    "print(\"expected inputs :\", times_two.n_in)\n",
    "print(\"promised outputs :\", times_two.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = times_two(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moidhassan/miniconda3/envs/cs224n/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3094: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"ones\")\n",
      "/home/moidhassan/miniconda3/envs/cs224n/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3085: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial[\n",
      "  LayerNorm\n",
      "  Serial[\n",
      "    Relu\n",
      "  ]\n",
      "  TimesTwo\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [LayerNorm, Serial[\n",
      "  Relu\n",
      "], TimesTwo]\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights & biases: ((DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), ((), (), ()), ()) \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0.        0.        0.        1.4142132 2.8284264]\n"
     ]
    }
   ],
   "source": [
    "# Serial combinator\n",
    "serial = tl.Serial(\n",
    "    tl.LayerNorm(),         # normalize input\n",
    "    tl.Relu(),              # convert negative values to zero\n",
    "    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n",
    "    \n",
    "    ### START CODE HERE\n",
    "#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n",
    "#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n",
    "#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n",
    "    ### END CODE HERE\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = np.array([-2, -1, 0, 1, 2]) #input\n",
    "serial.init(shapes.signature(x)) #initialising serial instance\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial,\"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out)\n",
    "print(\"weights & biases:\", serial.weights, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good old numpy :  <class 'numpy.ndarray'> \n",
      "\n",
      "jax trax numpy :  <class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# Numpy vs fastmath.numpy have different data types\n",
    "# Regular ol' numpy\n",
    "x_numpy = np.array([1, 2, 3])\n",
    "print(\"good old numpy : \", type(x_numpy), \"\\n\")\n",
    "\n",
    "# Fastmath and jax numpy\n",
    "x_jax = fastmath.numpy.array([1, 2, 3])\n",
    "print(\"jax trax numpy : \", type(x_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_a: None\n",
      "Parameter x of instance_b: None\n"
     ]
    }
   ],
   "source": [
    "class My_Class: #Definition of My_class\n",
    "    x = None    \n",
    "\n",
    "instance_a= My_Class() #To create an instance from class \"My_Class\" you have to call \"My_Class\"\n",
    "instance_b= My_Class()\n",
    "print('Parameter x of instance_a: ' + str(instance_a.x)) #To get a parameter 'x' from an instance 'a', write 'a.x'\n",
    "print('Parameter x of instance_b: ' + str(instance_b.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_a: 5\n"
     ]
    }
   ],
   "source": [
    "instance_a.x = 5\n",
    "print('Parameter x of instance_a: ' + str(instance_a.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_c: 10\n"
     ]
    }
   ],
   "source": [
    "instance_c = My_Class(10)\n",
    "print('Parameter x of instance_c: ' + str(instance_c.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y\n",
    "    def __call__(self, z): # __call__ method with self and z as arguments\n",
    "        self.x += z        # Adds z to parameter x when called \n",
    "        print(\"Call method\")\n",
    "        print(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_d = My_Class(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call method\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "instance_d(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "        \n",
    "    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n",
    "        result = self.x_1 + self.x_2 \n",
    "        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of 10 and 15 is 25\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "instance_e = My_Class(10,15)\n",
    "def test_class_definition():\n",
    "    \n",
    "    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n",
    "    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n",
    "    assert instance_e() == 25, \"Check the __call__ method\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "test_class_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        result = self.x_1*self.x_2 + w\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of my_method: 26\n"
     ]
    }
   ],
   "source": [
    "instance_f = My_Class(1,10)\n",
    "print(\"Output of my_method:\",instance_f.my_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z):\n",
    "        ### START CODE HERE           #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):               #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2  \n",
    "        return a\n",
    "    def my_method(self, w):           #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        b = self.x_1*self.x_2 + w\n",
    "        return b\n",
    "    def new_method(self, v):          #Calls My_method with argument v\n",
    "        result = self.my_method(v)\n",
    "        ### END CODE HERE ### \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of my_method: 26\n",
      "Output of new_method: 26\n"
     ]
    }
   ],
   "source": [
    "instance_g = My_Class(1,10)\n",
    "print(\"Output of my_method:\",instance_g.my_method(16))\n",
    "\n",
    "print(\"Output of new_method:\",instance_g.new_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def additional_method(self): #Prints the value of parameter x_1\n",
    "        print(self.x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x_1 of instance_sub_a: 1\n",
      "Parameter x_2 of instance_sub_a: 10\n",
      "Output of my_method of instance_sub_a: 26\n"
     ]
    }
   ],
   "source": [
    "instance_sub_a = sub_c(1,10)\n",
    "print('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\n",
    "print('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\n",
    "print(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n",
    "        b = self.x_1*self.x_2 \n",
    "        \n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of overridden my_method of test: 30\n"
     ]
    }
   ],
   "source": [
    "test = sub_c(3,10)\n",
    "assert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n",
    "\n",
    "print(\"Output of overridden my_method of test:\",test.my_method()) #notice we didn't pass any parameter to call my_method\n",
    "#print(\"Output of overridden my_method of test:\",test.my_method(16)) #try to see what happens if you call it with 1 argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_method for an instance of sub_c returns: 10\n",
      "My_method for an instance of My_Class returns: 20\n"
     ]
    }
   ],
   "source": [
    "y,z= 1,10\n",
    "instance_sub_a = sub_c(y,z)\n",
    "instance_a = My_Class(y,z)\n",
    "print('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\n",
    "print('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "import numpy as np\n",
    "\n",
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = [0] * 10\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = len(b)\n",
    "lines_index = [*range(a_size)] # is equivalent to [i for i in range(0,a_size)], the difference being the advantage of using * to pass values of range iterator to list directly\n",
    "index = 0                      # similar to index in data_generator below\n",
    "for i in range(b_size):        # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0 so the sequences circle back at the end to point to the first index\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "    \n",
    "    b[i] = a[lines_index[index]]     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "    \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original order of index: [0, 1, 2, 3]\n",
      "Shuffled order of index: [2, 3, 0, 1]\n",
      "New value order for first batch: [3, 4, 1, 2]\n",
      "\n",
      "Shuffled Indexes for Batch No.2 :[0, 1, 2, 3]\n",
      "Values for Batch No.2 :[1, 2, 3, 4]\n",
      "\n",
      "Shuffled Indexes for Batch No.3 :[1, 2, 0, 3]\n",
      "Values for Batch No.3 :[2, 3, 1, 4]\n",
      "\n",
      "Final value of b: [3, 4, 1, 2, 1, 2, 3, 4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = []\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = 10\n",
    "lines_index = [*range(a_size)]\n",
    "print(\"Original order of index:\",lines_index)\n",
    "\n",
    "# if we shuffle the index_list we can change the order of our circular list\n",
    "# without modifying the order or our original data\n",
    "rnd.shuffle(lines_index) # Shuffle the order\n",
    "print(\"Shuffled order of index:\",lines_index)\n",
    "\n",
    "print(\"New value order for first batch:\",[a[index] for index in lines_index])\n",
    "batch_counter = 1\n",
    "index = 0                # similar to index in data_generator below\n",
    "for i in range(b_size):  # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "        batch_counter += 1\n",
    "        rnd.shuffle(lines_index) # Re-shuffle the order\n",
    "        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n",
    "        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n",
    "    \n",
    "    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "print()    \n",
    "print(\"Final value of b:\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random as rnd\n",
    "a = [1,2,3]\n",
    "np.random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, data_x, data_y, shuffle=True):\n",
    "    '''\n",
    "      Input: \n",
    "        batch_size - integer describing the batch size\n",
    "        data_x - list containing samples\n",
    "        data_y - list containing labels\n",
    "        shuffle - Shuffle the data order\n",
    "      Output:\n",
    "        a tuple containing 2 elements:\n",
    "        X - list of dim (batch_size) of samples\n",
    "        Y - list of dim (batch_size) of labels\n",
    "    '''\n",
    "    \n",
    "    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n",
    "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n",
    "    \n",
    "    \n",
    "    # If shuffle is set to true, we traverse the list in a random way\n",
    "    if shuffle:\n",
    "        rnd.shuffle(index_list) # Inplace shuffle of the list\n",
    "    #    print(index_list)\n",
    "    \n",
    "    index = 0 # Start with the first element\n",
    "    # START CODE HERE    \n",
    "    # Fill all the None values with code taking reference of what you learned so far\n",
    "    while True:\n",
    "        X = [0] * batch_size # We can create a list with batch_size elements. \n",
    "        Y = [0] * batch_size # We can create a list with batch_size elements.\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Wrap the index each time that we reach the end of the list\n",
    "            if index >= data_lng:\n",
    "                index = 0\n",
    "                # Shuffle the index_list if shuffle is true\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(index_list) # re-shuffle the order\n",
    "                    #print(\"Inner shuffle True\")\n",
    "            \n",
    "            #print(index_list[index])\n",
    "            X[i] = data_x[index_list[index]] # We set the corresponding element in x\n",
    "            Y[i] = data_y[index_list[index]] # We set the corresponding element in y\n",
    "    # END CODE HERE            \n",
    "            index += 1\n",
    "        \n",
    "        yield((X, Y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] [1, 4, 9, 16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 3, 2], [1, 9, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [xi ** 2 for xi in x]\n",
    "#for i in range(3):\n",
    "print(x,y)\n",
    "t = data_generator(3, x, y, shuffle=True)\n",
    "next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbba06fbed6ee386bde69489e98164e7190208f7eb3dc907db615d0c466303dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
