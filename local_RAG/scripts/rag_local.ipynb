{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37353048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps to install llama-cpp-python on Windows with CUDA support\n",
    "# 1. Install Visual Studio 2022 with \"Desktop development with C++\" workload\n",
    "# 2. Install CUDA Toolkit from NVIDIA's website if not already installed\n",
    "# 3. Install latest version of cmake\n",
    "# 4. Open x64 Native Tools Command Prompt for VS 2022\n",
    "# 5. Run the following command:\n",
    "    # pip install llama-index\n",
    "    # pip install llama-index-llms-llama-cpp\n",
    "    # pip install sentence-transformers\n",
    "    # pip install chromadb\n",
    "    # pip install faiss-cpu\n",
    "    # pip install pypdf\n",
    "    # pip install docx2txt\n",
    "    # pip install python-pptx\n",
    "    # set FORCE_CMAKE=1\n",
    "    # set CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89\n",
    "    # pip install llama-cpp-python --force-reinstall --no-cache-dir --verbose --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125\n",
    "# 6. Verify installation by running python shell and executing:\n",
    "    # from llama_cpp import Llama\n",
    "    # llm = Llama(model_path=r\"C:\\Users\\moidhassan\\Downloads\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", n_gpu_layers=-1)\n",
    "# 7. If you encounter issues, refer to the GitHub repository: https://github.com/abetlen/llama-cpp-python\n",
    "# Note: Adjust the model_path to point to your downloaded GGUF model file.\n",
    "# 8. For CUDA support, ensure your GPU is compatible and the correct CUDA version is installed.\n",
    "# 9. Test with a simple script to ensure everything is working fine.\n",
    "# Note: Some installations may require restarting the terminal or IDE to recognize new environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96e7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c37968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Imports & Configuration\n",
    "# All necessary libraries and configuration constants.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "from textwrap import dedent\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.llama_cpp import LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd4ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:27:54 - Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Constants ---\n",
    "\n",
    "# 1. Paths\n",
    "# Adjust these paths to match your folder structure.\n",
    "CV_PATH = \"../data/CV\"\n",
    "FINANCIAL_PATH = \"../data/financial\"\n",
    "SPECS_PATH = \"../data/specs\"\n",
    "REIMBURSEMENT_PATH = \"../data/reimbursement\"\n",
    "DATA_PATHS_MAP = {\n",
    "    \"CV\": CV_PATH,\n",
    "    \"FINANCIAL\": FINANCIAL_PATH,\n",
    "    \"SPECS\": SPECS_PATH,\n",
    "    \"REIMBURSEMENT\": REIMBURSEMENT_PATH\n",
    "}\n",
    "MODEL_PATH = \"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"local_rag_demo\"\n",
    "\n",
    "# 2. Embedding Model\n",
    "# \"all-MiniLM-L6-v2\" is small (~90MB), fast, and high-quality.\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# 3. Chunking Parameters\n",
    "# Using a semantic splitter to keep sentences intact.\n",
    "CHUNK_SIZE = 768      # Target size of each chunk (in characters)\n",
    "CHUNK_OVERLAP = 128     # Overlap between chunks to maintain context\n",
    "\n",
    "# 4. LLM Parameters\n",
    "# These are settings for the Mistral 7B model.\n",
    "LLM_N_CTX = 8192*2       # Context window size (Mistral has a large one)\n",
    "LLM_N_GPU_LAYERS = 35  # Layers to offload to GPU (adjust based on your VRAM)\n",
    "LLM_TEMPERATURE = 0.2  # Low temperature for factual, less \"creative\" answers\n",
    "LLM_MAX_NEW_TOKENS = 768 # Max tokens to generate in an answer\n",
    "\n",
    "# 5. Retrieval Parameters\n",
    "TOP_K_RESULTS = 5      # Number of context chunks to retrieve\n",
    "\n",
    "# 6. Helper Function\n",
    "def log(message):\n",
    "    \"\"\"Helper function for formatted logging.\"\"\"\n",
    "    print(f\"[INFO] {time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\")\n",
    "\n",
    "log(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ab02b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.1: Load Documents\n",
    "# **Goal:** Load documents from each directory path separately and group them. This allows us to create a dedicated index for \"CVs\", \"Financials\", etc. (Multi-Index RAG).\n",
    "\n",
    "# %%\n",
    "def load_documents(data_paths_map):\n",
    "    \"\"\"\n",
    "    Loads documents from multiple directories and groups them by source name.\n",
    "    \n",
    "    Args:\n",
    "        data_paths_map (dict): A dictionary mapping index names (e.g., \"CV\") \n",
    "                               to their directory paths.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the index names and values are \n",
    "              lists of LlamaIndex Document objects.\n",
    "    \"\"\"\n",
    "    log(\"Starting to load documents for multiple indexes...\")\n",
    "    grouped_documents = {}\n",
    "    \n",
    "    # Iterate through the map: (Index Name) -> (Directory Path)\n",
    "    for name, path in data_paths_map.items():\n",
    "        if not Path(path).exists():\n",
    "            log(f\"Warning: Path for '{name}' does not exist, skipping: {path}\")\n",
    "            continue\n",
    "        \n",
    "        # SimpleDirectoryReader reads all files in the directory\n",
    "        reader = SimpleDirectoryReader(path, recursive=True)\n",
    "        try:\n",
    "            documents = reader.load_data()\n",
    "            grouped_documents[name] = documents\n",
    "            log(f\"Successfully loaded {len(documents)} documents for index: '{name}'\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error loading data from {path} for '{name}': {e}\")\n",
    "            \n",
    "    total_docs = sum(len(docs) for docs in grouped_documents.values())\n",
    "    log(f\"✅ Total documents loaded for {len(grouped_documents)} indexes: {total_docs}\")\n",
    "    \n",
    "    if not grouped_documents:\n",
    "        log(\"Error: No documents were loaded. Please check your DATA_PATHS_MAP.\")\n",
    "        \n",
    "    return grouped_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d42e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:30:13 - Starting to load documents for multiple indexes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 02:30:19,835 - WARNING - Ignoring wrong pointing object 41 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:30:22 - Successfully loaded 51 documents for index: 'CV'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 02:30:55,926 - WARNING - invalid pdf header: b'\\xac\\xed\\x00\\x05u'\n",
      "2025-10-29 02:30:55,926 - WARNING - incorrect startxref pointer(1)\n",
      "2025-10-29 02:30:55,943 - WARNING - parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:31:04 - Successfully loaded 98 documents for index: 'FINANCIAL'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 02:31:29,103 - WARNING - Ignoring wrong pointing object 41 0 (offset 0)\n",
      "2025-10-29 02:31:55,302 - WARNING - parsing for Object Streams\n",
      "2025-10-29 02:31:55,660 - WARNING - parsing for Object Streams\n",
      "2025-10-29 02:31:59,248 - WARNING - parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:32:03 - Successfully loaded 843 documents for index: 'SPECS'\n",
      "[INFO] 2025-10-29 02:32:03 - Successfully loaded 15 documents for index: 'REIMBURSEMENT'\n",
      "[INFO] 2025-10-29 02:32:03 - ✅ Total documents loaded for 4 indexes: 1007\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.1 (Modified for Multi-Index) ---\n",
    "# This will scan your directories and populate the 'grouped_documents' dictionary.\n",
    "grouped_documents = load_documents(DATA_PATHS_MAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4de5a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CV', 'FINANCIAL', 'SPECS', 'REIMBURSEMENT'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9179b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document content snippet from index 'CV': ABHISHEK  RANJAN                  +91  9040140733   \n",
      " Data  Scientist  |  AI  Engineer                     13eee079@gmail.com  \n",
      " \n",
      "ABOUT  ME  \n",
      "Data  Sc...\n",
      "\n",
      "First document content snippet from index 'FINANCIAL': ...\n",
      "\n",
      "First document content snippet from index 'SPECS': Surface USB-C® Travel Hub\n",
      "All the connections, wherever you are\n",
      "Pitch\n",
      "Turn your laptop into an on-the-go productivity companion with \n",
      "this elegant, mu...\n",
      "\n",
      "First document content snippet from index 'REIMBURSEMENT': moid hassan\n",
      "RD17471298637220809\n",
      "Ramaswamy\n",
      "KA04AA9727\n",
      "Car\n",
      "May 13th 2025, 3:25 PM\n",
      "Booking History\n",
      "Customer Name\n",
      "Ride ID\n",
      "Driver name\n",
      "Vehicle Number\n",
      "Mode ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a snippet from one of the first documents loaded\n",
    "\n",
    "for index_name, docs in grouped_documents.items():\n",
    "    if docs:\n",
    "        print(f\"First document content snippet from index '{index_name}': {docs[0].text[:150]}...\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e2c82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.2: Load Embedding Model\n",
    "# We define the function to load the `sentence-transformer` model and then call it.\n",
    "\n",
    "# %%\n",
    "def load_embedding_model(model_name):\n",
    "    \"\"\"\n",
    "    Loads the SentenceTransformer embedding model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model from Hugging Face.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    log(f\"Loading embedding model: {model_name}...\")\n",
    "    # The first time you run this, it will download the model.\n",
    "    embed_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Example: Test the model\n",
    "    sample_vec = embed_model.encode(\"This is a test sentence.\")\n",
    "    log(f\"✅ Embedding model loaded. Vector size: {len(sample_vec)}\")\n",
    "    return embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8791f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 02:33:29,590 - INFO - Use pytorch device_name: cpu\n",
      "2025-10-29 02:33:29,590 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "[INFO] 2025-10-29 02:33:29 - Loading embedding model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:33:34 - ✅ Embedding model loaded. Vector size: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.2 ---\n",
    "# This will download the model if you don't have it cached.\n",
    "print(f\"Loading embedding model: {EMBED_MODEL_NAME}...\")\n",
    "embed_model = load_embedding_model(EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baabc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.3: Chunk and Embed Documents\n",
    "# **Goal:** Convert raw documents into semantically meaningful chunks and generate high-quality embedding vectors for each chunk using the loaded model.\n",
    "\n",
    "# %%\n",
    "def chunk_and_embed(documents, embed_model):\n",
    "    \"\"\"\n",
    "    Chunks documents and generates embeddings for each chunk.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of LlamaIndex Document objects for a single index group.\n",
    "        embed_model (SentenceTransformer): The embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated IDs, chunk text, and embeddings \n",
    "              for the input documents.\n",
    "    \"\"\"\n",
    "    log(\"Starting document chunking and embedding...\")\n",
    "    \n",
    "    if not documents:\n",
    "        log(\"No documents to chunk. Skipping...\")\n",
    "        return {'ids': [], 'chunks': [], 'embeddings': []}\n",
    "\n",
    "    # This is the correct, semantic way to chunk text.\n",
    "    # It splits on sentences and respects word boundaries.\n",
    "    text_splitter = SentenceSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    \n",
    "    # LlamaIndex's SentenceSplitter returns \"Node\" objects.\n",
    "    nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    log(f\"Split {len(documents)} source documents into {len(nodes)} chunks.\")\n",
    "    \n",
    "    log(\"Generating embeddings for all chunks...\")\n",
    "    # 1. Get all the text content first\n",
    "    all_chunk_text = [node.get_content() for node in nodes]\n",
    "    \n",
    "    # 2. Generate embeddings in a single, efficient batch\n",
    "    # This is *much* faster than embedding chunks one by one.\n",
    "    embeddings = embed_model.encode(all_chunk_text, show_progress_bar=True)\n",
    "    \n",
    "    # 3. Create the IDs list\n",
    "    ids = [f\"chunk_{i}\" for i in range(len(nodes))]\n",
    "    \n",
    "    log(f\"✅ Generated {len(embeddings)} embeddings.\")\n",
    "    return {\n",
    "        'ids': ids, \n",
    "        'chunks': all_chunk_text, \n",
    "        'embeddings': embeddings.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5dfbd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:33:54 - Processing group: CV (51 documents)\n",
      "[INFO] 2025-10-29 02:33:54 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 02:33:59 - Split 51 source documents into 80 chunks.\n",
      "[INFO] 2025-10-29 02:33:59 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:34:01 - ✅ Generated 80 embeddings.\n",
      "[INFO] 2025-10-29 02:34:01 - Processing group: FINANCIAL (98 documents)\n",
      "[INFO] 2025-10-29 02:34:01 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 02:34:01 - Split 98 source documents into 121 chunks.\n",
      "[INFO] 2025-10-29 02:34:01 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:34:03 - ✅ Generated 121 embeddings.\n",
      "[INFO] 2025-10-29 02:34:03 - Processing group: SPECS (843 documents)\n",
      "[INFO] 2025-10-29 02:34:03 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 02:34:15 - Split 843 source documents into 3283 chunks.\n",
      "[INFO] 2025-10-29 02:34:15 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 103/103 [01:06<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:35:22 - ✅ Generated 3283 embeddings.\n",
      "[INFO] 2025-10-29 02:35:22 - Processing group: REIMBURSEMENT (15 documents)\n",
      "[INFO] 2025-10-29 02:35:22 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 02:35:22 - Split 15 source documents into 15 chunks.\n",
      "[INFO] 2025-10-29 02:35:22 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:35:22 - ✅ Generated 15 embeddings.\n",
      "[INFO] 2025-10-29 02:35:22 - ✅ Completed chunking and embedding for 4 groups.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.3 (Modified for Multi-Index) ---\n",
    "# This will chunk and embed the documents for all groups.\n",
    "# The results are stored in a dictionary matching the group names.\n",
    "embedded_groups = {}\n",
    "\n",
    "for name, docs in grouped_documents.items():\n",
    "    log(f\"Processing group: {name} ({len(docs)} documents)\")\n",
    "    \n",
    "    # We call the function and get the processed data back\n",
    "    processed_data = chunk_and_embed(docs, embed_model)\n",
    "    \n",
    "    embedded_groups[name] = processed_data\n",
    "\n",
    "log(f\"✅ Completed chunking and embedding for {len(embedded_groups)} groups.\")\n",
    "# You can now inspect 'embedded_groups' to see IDs, chunks, and embeddings for each index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21aa92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.4: Initialize Vector Store (Modified for Multi-Index)\n",
    "# **Goal:** Create a **separate Chroma collection** for each document group (e.g., `rag_docs_cv`, `rag_docs_financial`). We use a persistent client so the data is saved to the disk and doesn't need to be re-embedded every time.\n",
    "\n",
    "# %%\n",
    "def initialize_vector_stores(embedded_groups):\n",
    "    \"\"\"\n",
    "    Initializes and populates a ChromaDB collection for each document group.\n",
    "    \n",
    "    Args:\n",
    "        embedded_groups (dict): Dictionary from chunking/embedding step.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping group names to their Chroma Collection objects.\n",
    "    \"\"\"\n",
    "    log(\"Initializing ChromaDB Persistent Client...\")\n",
    "    # This client saves the database files to the CHROMA_DB_PATH directory.\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "    all_collections = {}\n",
    "    \n",
    "    for name, data in embedded_groups.items():\n",
    "        # Create a unique collection name based on the index name (e.g., 'rag_docs_cv')\n",
    "        collection_name = f\"rag_docs_{name.lower()}\"\n",
    "        log(f\"Creating/getting collection: '{collection_name}'\")\n",
    "        \n",
    "        # Get or create the collection\n",
    "        collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "        \n",
    "        ids, document_chunks, embeddings = data['ids'], data['chunks'], data['embeddings']\n",
    "\n",
    "        # Check if the collection is already populated (fast startup on subsequent runs)\n",
    "        if collection.count() > 0:\n",
    "            log(f\"Collection '{collection_name}' already exists and has {collection.count()} entries. Skipping storage.\")\n",
    "        elif not ids:\n",
    "             log(f\"Collection '{collection_name}' is empty and no data to add. Skipping.\")\n",
    "        else:\n",
    "            log(f\"Collection '{collection_name}' is empty. Adding {len(ids)} embeddings...\")\n",
    "            \n",
    "            # Add to Chroma in large batches for efficiency (much faster than chunk-by-chunk)\n",
    "            batch_size = 500\n",
    "            for i in range(0, len(ids), batch_size):\n",
    "                batch_ids = ids[i:i+batch_size]\n",
    "                batch_docs = document_chunks[i:i+batch_size]\n",
    "                batch_embeds = embeddings[i:i+batch_size]\n",
    "                \n",
    "                collection.add(\n",
    "                    ids=batch_ids,\n",
    "                    documents=batch_docs,\n",
    "                    embeddings=batch_embeds\n",
    "                )\n",
    "                log(f\"Added batch {i//batch_size + 1} to {collection_name}.\")\n",
    "                \n",
    "            log(f\"✅ Stored {collection.count()} chunks in {collection_name}.\")\n",
    "\n",
    "        all_collections[name] = collection\n",
    "        \n",
    "    return all_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8fe30c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 02:35:51,238 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:35:51 - Initializing ChromaDB Persistent Client...\n",
      "[INFO] 2025-10-29 02:35:52 - Creating/getting collection: 'rag_docs_cv'\n",
      "[INFO] 2025-10-29 02:35:52 - Collection 'rag_docs_cv' is empty. Adding 80 embeddings...\n",
      "[INFO] 2025-10-29 02:35:52 - Added batch 1 to rag_docs_cv.\n",
      "[INFO] 2025-10-29 02:35:52 - ✅ Stored 80 chunks in rag_docs_cv.\n",
      "[INFO] 2025-10-29 02:35:52 - Creating/getting collection: 'rag_docs_financial'\n",
      "[INFO] 2025-10-29 02:35:52 - Collection 'rag_docs_financial' is empty. Adding 121 embeddings...\n",
      "[INFO] 2025-10-29 02:37:00 - Added batch 1 to rag_docs_financial.\n",
      "[INFO] 2025-10-29 02:37:00 - ✅ Stored 121 chunks in rag_docs_financial.\n",
      "[INFO] 2025-10-29 02:37:00 - Creating/getting collection: 'rag_docs_specs'\n",
      "[INFO] 2025-10-29 02:37:00 - Collection 'rag_docs_specs' is empty. Adding 3283 embeddings...\n",
      "[INFO] 2025-10-29 02:37:01 - Added batch 1 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:02 - Added batch 2 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:04 - Added batch 3 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:04 - Added batch 4 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:05 - Added batch 5 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:06 - Added batch 6 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:07 - Added batch 7 to rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:07 - ✅ Stored 3283 chunks in rag_docs_specs.\n",
      "[INFO] 2025-10-29 02:37:07 - Creating/getting collection: 'rag_docs_reimbursement'\n",
      "[INFO] 2025-10-29 02:37:07 - Collection 'rag_docs_reimbursement' is empty. Adding 15 embeddings...\n",
      "[INFO] 2025-10-29 02:37:07 - Added batch 1 to rag_docs_reimbursement.\n",
      "[INFO] 2025-10-29 02:37:07 - ✅ Stored 15 chunks in rag_docs_reimbursement.\n",
      "[INFO] 2025-10-29 02:37:07 - ✅ Successfully initialized 4 Chroma collections.\n",
      "Available Collections: ['CV', 'FINANCIAL', 'SPECS', 'REIMBURSEMENT']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.4 (Modified for Multi-Index) ---\n",
    "# This creates the physical ChromaDB files on your disk.\n",
    "collections_map = initialize_vector_stores(embedded_groups)\n",
    "log(f\"✅ Successfully initialized {len(collections_map)} Chroma collections.\")\n",
    "print(\"Available Collections:\", list(collections_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4c40e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.5: Load Local LLM\n",
    "# **Goal:** Load the local GGUF model (Mistral) into memory using the **`LlamaCPP`** wrapper. This makes the model available for text generation.\n",
    "\n",
    "# %%\n",
    "def load_llm():\n",
    "    \"\"\"\n",
    "    Loads the local GGUF model using the LlamaCPP class.\n",
    "    \n",
    "    Returns:\n",
    "        LlamaCPP: The loaded LLM instance.\n",
    "    \"\"\"\n",
    "    log(f\"Loading local LLM from {MODEL_PATH} (this may take 20-30s)...\")\n",
    "    if not Path(MODEL_PATH).exists():\n",
    "        log(f\"Error: Model file not found at {MODEL_PATH}\")\n",
    "        log(f\"Please run the huggingface-cli download command in the script's docstring.\")\n",
    "        return None  # Return None instead of exiting\n",
    "        \n",
    "    llm = LlamaCPP(  # MODIFIED: Correct casing: LlamaCPP\n",
    "        model_path=MODEL_PATH,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        max_new_tokens=LLM_MAX_NEW_TOKENS,\n",
    "        context_window=LLM_N_CTX,\n",
    "        model_kwargs={\n",
    "            # set to -1 to use all cores\n",
    "            \"n_threads\": -1,\n",
    "            # offload layers to GPU\n",
    "            \"n_gpu_layers\": LLM_N_GPU_LAYERS\n",
    "        },\n",
    "        # Enable verbose logging from LlamaCpp\n",
    "        verbose=True\n",
    "    )\n",
    "    log(\"✅ Local LLM loaded.\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e7ee784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hf download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir ./models'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the model using hf download command\n",
    "\"\"\"hf download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir ./models\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "238b48ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:38:00 - Loading local LLM from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (this may take 20-30s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 7100 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  4095.05 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 16384\n",
      "llama_context: n_ctx_per_seq = 16384\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 16384 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB ( 16384 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      CUDA0 compute buffer size =  1092.01 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    44.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:38:08 - ✅ Local LLM loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.5 ---\n",
    "# This will take 20-30 seconds (or more) as it loads the model into memory (and VRAM).\n",
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed41e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.6: Define RAG Retrieval and Generation Functions\n",
    "# **Goal:** Define the core functions for **Routing**, **Retrieval**, and **Generation**. This implements the Multi-Index strategy with the \"Fallback to All Indexes\" logic.\n",
    "\n",
    "# %%\n",
    "def route_query_to_index(query):\n",
    "    \"\"\"\n",
    "    Simple routing function based on keyword detection.\n",
    "    If no keywords match, returns 'FALLBACK_ALL' to trigger a search across all indexes.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    if \"cv\" in query_lower or \"resume\" in query_lower or \"experience\" in query_lower:\n",
    "        return \"CV\"\n",
    "    elif \"financial\" in query_lower or \"report\" in query_lower or \"balance\" in query_lower:\n",
    "        return \"FINANCIAL\"\n",
    "    elif \"spec\" in query_lower or \"architecture\" in query_lower or \"design\" in query_lower:\n",
    "        return \"SPECS\"\n",
    "    elif \"reimbursement\" in query_lower or \"expense\" in query_lower or \"travel\" in query_lower:\n",
    "        return \"REIMBURSEMENT\"\n",
    "    else:\n",
    "        # **FALLBACK LOGIC**\n",
    "        log(\"No specific keywords found. Triggering search across ALL available indexes.\")\n",
    "        return \"FALLBACK_ALL\" # Special marker for the retrieval function\n",
    "\n",
    "def retrieve_context(query, collections_map, embed_model, top_k=TOP_K_RESULTS):\n",
    "    \"\"\"\n",
    "    Performs the full RAG retrieval process: Routing -> Embedding -> Query.\n",
    "    \n",
    "    If routing returns 'FALLBACK_ALL', it searches all collections and combines results.\n",
    "        \n",
    "    Returns:\n",
    "        str: A single string containing all retrieved context chunks, separated by newlines.\n",
    "    \"\"\"\n",
    "    # 1. ROUTING: Determine which index(es) to search\n",
    "    index_name = route_query_to_index(query)\n",
    "    \n",
    "    # 2. Determine which collections to use\n",
    "    if index_name == \"FALLBACK_ALL\":\n",
    "        # Search all collections\n",
    "        collections_to_search = collections_map.values()\n",
    "        # For general search, retrieve fewer chunks per index but keep total_chunks high\n",
    "        # We will split the TOP_K_RESULTS across all collections.\n",
    "        k_per_index = max(1, TOP_K_RESULTS // len(collections_map)) \n",
    "        \n",
    "    elif index_name in collections_map:\n",
    "        # Search only the routed collection\n",
    "        collections_to_search = [collections_map[index_name]]\n",
    "        k_per_index = TOP_K_RESULTS\n",
    "        log(f\"Searching in single collection: {index_name}\")\n",
    "        \n",
    "    else:\n",
    "        return f\"Error: Index '{index_name}' could not be found. Context unavailable.\"\n",
    "\n",
    "    # 3. EMBEDDING: Vectorize the query once\n",
    "    query_emb = embed_model.encode(query)\n",
    "    combined_contexts = []\n",
    "    \n",
    "    # 4. QUERY: Loop through the chosen collections\n",
    "    for collection in collections_to_search:\n",
    "        log(f\"Querying collection: {collection.name} for {k_per_index} chunks.\")\n",
    "        \n",
    "        # Search the collection\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_emb.tolist()], \n",
    "            n_results=k_per_index\n",
    "        )\n",
    "        \n",
    "        # Extract and append the documents (text chunks)\n",
    "        contexts = results.get('documents', [[]])[0]\n",
    "        combined_contexts.extend(contexts)\n",
    "        \n",
    "    # Combine the final context chunks into a single string\n",
    "    return \"\\n\\n\".join(combined_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46c06f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def rag_answer(query, llm, collections_map, embed_model):\n",
    "    \"\"\"\n",
    "    Generates a RAG-augmented answer from the LLM.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's input question.\n",
    "        llm (LlamaCPP): The loaded LLM instance.\n",
    "        collections_map (dict): The map of index names to Chroma collection objects.\n",
    "        embed_model (SentenceTransformer): The embedding model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The final, synthesized answer from the LLM.\n",
    "    \"\"\"\n",
    "    # 1. RETRIEVAL: Get the relevant context chunks (now handles routing and fallback)\n",
    "    context = retrieve_context(query, collections_map, embed_model)\n",
    "    \n",
    "    # 2. PROMPT CONSTRUCTION (Prompt Engineering)\n",
    "    # The prompt strictly instructs the LLM to use only the provided context.\n",
    "    prompt = dedent(f\"\"\"\n",
    "        You are a helpful and professional assistant. Your task is to provide a concise and accurate answer \n",
    "        based **ONLY** on the context provided below.\n",
    "        If the answer is not present in the context, you MUST state, \"I cannot find a definitive answer in the provided documents.\"\n",
    "\n",
    "        Context:\n",
    "        ---\n",
    "        {context}\n",
    "        ---\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "    \"\"\")\n",
    "    \n",
    "    # 3. GENERATION: Pass the prompt to the local LLM\n",
    "    output = llm.complete(prompt)\n",
    "    \n",
    "    return output.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aef2e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST QUERY 1 ---\n",
      "🧑‍💻 Query: What is the experience level of the candidate \"Moid Hassan\" in Microsoft in the CV documents?\n",
      "➡️ Routing Decision: CV\n",
      "[INFO] 2025-10-29 02:38:52 - Searching in single collection: CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:38:53 - Querying collection: rag_docs_cv for 5 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_perf_context_print:        load time =   50028.53 ms\n",
      "llama_perf_context_print: prompt eval time =   50027.87 ms /  4222 tokens (   11.85 ms per token,    84.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54263.91 ms /    99 runs   (  548.12 ms per token,     1.82 tokens per second)\n",
      "llama_perf_context_print:       total time =  104388.88 ms /  4321 tokens\n",
      "llama_perf_context_print:    graphs reused =         94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Time taken: 104.53 seconds\n",
      "\n",
      "🧠 RAG Answer:\n",
      "Moid Hassan's CV documents do not provide any specific information about his experience level with Microsoft. However, he has mentioned working on Microsoft technologies such as C#, .NET, and SQL. Additionally, he has listed certifications from Microsoft, including \"Microsoft Certified: Azure Developer Associate\" and \"Microsoft Certified: Azure Solutions Architect Expert.\" These certifications suggest that Moid Hassan has gained some level of proficiency and experience in using Microsoft Azure.\n",
      "\n",
      "--- TEST QUERY 2 ---\n",
      "🧑‍💻 Query: Tell me about the documents I have, focusing on any key dates.\n",
      "[INFO] 2025-10-29 02:40:37 - No specific keywords found. Triggering search across ALL available indexes.\n",
      "➡️ Routing Decision: FALLBACK_ALL\n",
      "[INFO] 2025-10-29 02:40:37 - No specific keywords found. Triggering search across ALL available indexes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:40:37 - Querying collection: rag_docs_cv for 1 chunks.\n",
      "[INFO] 2025-10-29 02:40:37 - Querying collection: rag_docs_financial for 1 chunks.\n",
      "[INFO] 2025-10-29 02:40:37 - Querying collection: rag_docs_specs for 1 chunks.\n",
      "[INFO] 2025-10-29 02:40:37 - Querying collection: rag_docs_reimbursement for 1 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 76 prefix-match hit, remaining 1559 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   50028.53 ms\n",
      "llama_perf_context_print: prompt eval time =   14728.55 ms /  1559 tokens (    9.45 ms per token,   105.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =  198385.76 ms /   388 runs   (  511.30 ms per token,     1.96 tokens per second)\n",
      "llama_perf_context_print:       total time =  214450.67 ms /  1947 tokens\n",
      "llama_perf_context_print:    graphs reused =        375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Time taken: 214.69 seconds\n",
      "\n",
      "🧠 RAG Answer:\n",
      "The documents provided include a resume, academic transcripts, personal information, and a declaration. The resume lists the individual's core competencies, which include data analytics and visualization, data management, leadership and collaboration, and communication and problem-solving skills. The academic transcripts show the individual's grades from their Post Graduate Diploma in Statistics, Post Graduate Diploma in HRM, and Bachelor of Arts in Economics (Honours). The personal information includes the individual's date of birth, gender, nationality, marital status, and languages. The declaration is a statement that the information provided is true to the best of the individual's knowledge and belief.\n",
      "\n",
      "        In addition to these documents, there is also a Microsoft Limited Warranty and Protection Plan document. This document contains information about how Microsoft sets the start date for its limited hardware warranty and protection plan coverage. It also includes some frequently asked questions about the registration process and start date adjustments.\n",
      "\n",
      "        The Microsoft document mentions a few key dates. For the limited hardware warranty, Microsoft uses data such as the manufacturing date, ship date, and activation to calculate the start date. For the protection plan, the coverage start dates are based on the point of sale and are set during the registration process, using the customer device invoice date. The registration process is not changing, and partners are required to submit the customer device invoice date to set the protection plan start date. If an exception is being requested, the registration of the Microsoft Protection Plan must still be completed within 10 business days of the customer device invoice date. When an exception is approved, the requestor is required to provide all the data required for re-registration, including a list of all impacted serial numbers. The total charge for the protection plan is 9140.29.\n",
      "\n",
      "--- TEST QUERY 3 ---\n",
      "🧑‍💻 Query: What is the main architecture of the Eiffel Tower?\n",
      "➡️ Routing Decision: SPECS\n",
      "[INFO] 2025-10-29 02:44:12 - Searching in single collection: SPECS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.78it/s]\n",
      "Llama.generate: 75 prefix-match hit, remaining 3276 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 02:44:12 - Querying collection: rag_docs_specs for 5 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   50028.53 ms\n",
      "llama_perf_context_print: prompt eval time =   36875.21 ms /  3276 tokens (   11.26 ms per token,    88.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18172.40 ms /    34 runs   (  534.48 ms per token,     1.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   55083.71 ms /  3310 tokens\n",
      "llama_perf_context_print:    graphs reused =         32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Time taken: 55.28 seconds\n",
      "\n",
      "🧠 RAG Answer:\n",
      "I cannot find a definitive answer in the provided documents regarding the main architecture of the Eiffel Tower. The context only provides technical specifications of a device.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.7: Test the RAG Pipeline\n",
    "# **Goal:** Execute a single test query to ensure the routing, retrieval, and generation steps work together.\n",
    "\n",
    "# %%\n",
    "test_queries = [\n",
    "    # 1. ROUTED QUERY: Should hit the 'CV' index\n",
    "    \"What is the experience level of the candidate \\\"Moid Hassan\\\" in Microsoft in the CV documents?\",\n",
    "    \n",
    "    # 2. FALLBACK QUERY: Should hit ALL indexes (triggering the fallback logic)\n",
    "    \"Tell me about the documents I have, focusing on any key dates.\",\n",
    "    \n",
    "    # 3. IRRELEVANT QUERY: Should test the LLM's constraint\n",
    "    \"What is the main architecture of the Eiffel Tower?\",\n",
    "]\n",
    "\n",
    "# Run the tests\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n--- TEST QUERY {i+1} ---\")\n",
    "    print(f\"🧑‍💻 Query: {query}\")\n",
    "    \n",
    "    # 1. Determine where the query is routed\n",
    "    routed_index = route_query_to_index(query)\n",
    "    print(f\"➡️ Routing Decision: {routed_index}\")\n",
    "    \n",
    "    # 2. Get the RAG Answer\n",
    "    start_time = time.time()\n",
    "    response = rag_answer(query, llm, collections_map, embed_model)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"⏱️ Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"\\n🧠 RAG Answer:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620d82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_gpu)",
   "language": "python",
   "name": "rag_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
