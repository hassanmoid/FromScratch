{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66609e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moidhassan\\AppData\\Local\\anaconda3\\envs\\rag_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cuda\n",
      "‚úÖ Cell 1 executed successfully ‚Äî environment ready.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Imports & Configuration\n",
    "# All necessary libraries and configuration constants.\n",
    "\n",
    "# %%\n",
    "# Step 0 - Import all necessary libraries\n",
    "# ---------------------------------------\n",
    "\n",
    "# Basic utilities\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers pipeline for LLM inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# LangChain ecosystem (community, text splitters, embeddings, FAISS, and HuggingFace LLM)\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Step 1 - Check environment\n",
    "# ---------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# Step 2 - Base data path\n",
    "# -----------------------\n",
    "#DATA_DIR = \"data\"\n",
    "#os.makedirs(DATA_DIR, exist_ok=True)\n",
    "#print(f\"üìÅ Base data directory: {os.path.abspath(DATA_DIR)}\")\n",
    "\n",
    "# Expected folder structure:\n",
    "# data/\n",
    "# ‚îú‚îÄ‚îÄ CV/\n",
    "# ‚îú‚îÄ‚îÄ financial/\n",
    "# ‚îú‚îÄ‚îÄ reimbursement/\n",
    "# ‚îî‚îÄ‚îÄ specs/\n",
    "# \n",
    "# Each folder should contain PDFs, DOCX, or TXT files for that category.\n",
    "\n",
    "print(\"‚úÖ Cell 1 executed successfully ‚Äî environment ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15be7b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cell 2A executed successfully ‚Äî configuration and helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2A. Configuration & Helper Functions\n",
    "# \n",
    "# This cell defines:\n",
    "# - Folder structure and configuration constants\n",
    "# - Supported file extensions and their respective loaders\n",
    "# - Utility functions for:\n",
    "#   - Loading documents from multiple folders\n",
    "#   - Splitting them into overlapping chunks for embedding\n",
    "\n",
    "# %%\n",
    "# Step 2A - Define configuration and helper functions\n",
    "\n",
    "# --- Global configuration ---\n",
    "DATA_DIR = \"../data\"\n",
    "CATEGORY_PATHS = {\n",
    "    \"CV\": os.path.join(DATA_DIR, \"CV\"),\n",
    "    \"FINANCIAL\": os.path.join(DATA_DIR, \"financial\"),\n",
    "    \"REIMBURSEMENT\": os.path.join(DATA_DIR, \"reimbursement\"),\n",
    "    \"SPECS\": os.path.join(DATA_DIR, \"specs\"),\n",
    "}\n",
    "\n",
    "# --- Map supported file extensions to document loaders ---\n",
    "EXT_TO_LOADER = {\n",
    "    \".pdf\": (PyPDFLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf-8\"}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "}\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all supported documents recursively from the given folder.\n",
    "    Returns a list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    folder = Path(folder_path)\n",
    "    if not folder.exists():\n",
    "        return docs\n",
    "\n",
    "    for file_path in folder.rglob(\"*\"):\n",
    "        if file_path.is_dir():\n",
    "            continue\n",
    "        ext = file_path.suffix.lower()\n",
    "        if ext not in EXT_TO_LOADER:\n",
    "            continue\n",
    "        loader_cls, loader_kwargs = EXT_TO_LOADER[ext]\n",
    "        try:\n",
    "            loader = loader_cls(str(file_path), **loader_kwargs)\n",
    "            loaded = loader.load()\n",
    "            docs.extend(loaded if isinstance(loaded, list) else [loaded])\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not load {file_path}: {e}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def process_all_documents(category_paths, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Loads and chunks all documents from all categories.\n",
    "    Returns a dictionary:\n",
    "        {\n",
    "          \"CV\": [chunk_dicts...],\n",
    "          \"FINANCIAL\": [chunk_dicts...],\n",
    "          ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    grouped_chunks = {}\n",
    "    total_docs = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    print(f\"using chunk_size-{chunk_size} and chunk_overlap-{chunk_overlap} for splitting\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    for category, folder_path in category_paths.items():\n",
    "        print(f\"\\n[INFO] Processing category: {category}\")\n",
    "        docs = load_documents_from_folder(folder_path)\n",
    "        num_docs = len(docs)\n",
    "        total_docs += num_docs\n",
    "        print(f\"   Loaded {num_docs} documents from {folder_path}\")\n",
    "\n",
    "        if not docs:\n",
    "            grouped_chunks[category] = []\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            chunks = splitter.split_documents(docs)\n",
    "            chunked_data = [\n",
    "                {\n",
    "                    \"text\": getattr(doc, \"page_content\", str(doc)),\n",
    "                    \"meta\": {\n",
    "                        \"source\": getattr(doc, \"metadata\", {}).get(\"source\", None),\n",
    "                        \"chunk_id\": f\"{category}_chunk_{i}\"\n",
    "                    },\n",
    "                }\n",
    "                for i, doc in enumerate(chunks)\n",
    "            ]\n",
    "            grouped_chunks[category] = chunked_data\n",
    "            total_chunks += len(chunked_data)\n",
    "            print(f\"   -> Created {len(chunked_data)} chunks for '{category}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to chunk '{category}': {e}\")\n",
    "            grouped_chunks[category] = []\n",
    "\n",
    "    print(f\"\\n‚úÖ Finished processing {total_docs} documents across all categories\")\n",
    "    print(f\"‚úÖ Total chunks created: {total_chunks}\")\n",
    "\n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cell 2A executed successfully ‚Äî configuration and helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "259293b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024+512+64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98033f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using chunk_size-1600 and chunk_overlap-256 for splitting\n",
      "\n",
      "[INFO] Processing category: CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 41 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 51 documents from ../data\\CV\n",
      "   -> Created 139 chunks for 'CV'\n",
      "\n",
      "[INFO] Processing category: FINANCIAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'\\xac\\xed\\x00\\x05u'\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 98 documents from ../data\\financial\n",
      "   -> Created 117 chunks for 'FINANCIAL'\n",
      "\n",
      "[INFO] Processing category: REIMBURSEMENT\n",
      "   Loaded 15 documents from ../data\\reimbursement\n",
      "   -> Created 22 chunks for 'REIMBURSEMENT'\n",
      "\n",
      "[INFO] Processing category: SPECS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "parsing for Object Streams\n",
      "parsing for Object Streams\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 403 documents from ../data\\specs\n",
      "   -> Created 585 chunks for 'SPECS'\n",
      "\n",
      "‚úÖ Finished processing 567 documents across all categories\n",
      "‚úÖ Total chunks created: 863\n",
      "\n",
      "--- Chunk Summary ---\n",
      "CV: 139 chunks\n",
      "  Example: ABHISHEK  RANJAN                  +91  9040140733     Data  Scientist  |  AI  Engineer                     13eee079@gmail.com     ABOUT  ME   Data  Scientist  with  3.9   years  of  experience,  with ...\n",
      "\n",
      "FINANCIAL: 117 chunks\n",
      "  Example: STOCK PLAN SERVICES REPORT April 1, 2024 - April 30, 2024 Envelope # BQGGQMBBBHRBR 1 of 8 MOID HASSAN FLAT NO 1002, BLOCK 1 MOHINDER APARTMENTS, SECTOR 12 DWARKA DELHI 110078 DL INDIA MR_CE _BQGGQMBBB...\n",
      "\n",
      "REIMBURSEMENT: 22 chunks\n",
      "  Example: moid hassan RD17471298637220809 Ramaswamy KA04AA9727 Car May 13th 2025, 3:25 PM Booking History Customer Name Ride ID Driver name Vehicle Number Mode of Vehicle Time of Ride Selected Price ‚Çπ  1181 Clu...\n",
      "\n",
      "SPECS: 585 chunks\n",
      "  Example: Surface USB-C¬Æ Travel Hub All the connections, wherever you are Pitch Turn your laptop into an on-the-go productivity companion with  this elegant, multi-port travel adapter. Designed for on-the-go  p...\n",
      "\n",
      "‚úÖ Cell 2B executed successfully ‚Äî documents loaded & chunked for all categories.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2B. Load & Chunk All Category Documents (Refactored)\n",
    "# \n",
    "# This cell:\n",
    "# - Uses `process_all_documents()` to load and chunk all documents.\n",
    "# - Displays a brief summary of the results and a preview of chunks.\n",
    "\n",
    "# %%\n",
    "# Step 2B - Load and chunk all category documents\n",
    "\n",
    "# --- Chunking parameters (easy to tune globally) ---\n",
    "CHUNK_SIZE = 1600\n",
    "CHUNK_OVERLAP = 256\n",
    "\n",
    "grouped_chunks = process_all_documents(CATEGORY_PATHS, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "# --- Display a quick summary ---\n",
    "print(\"\\n--- Chunk Summary ---\")\n",
    "for category, chunks in grouped_chunks.items():\n",
    "    print(f\"{category}: {len(chunks)} chunks\")\n",
    "    if chunks:\n",
    "        sample_text = chunks[0]['text'][:200].replace('\\n', ' ')\n",
    "        print(f\"  Example: {sample_text}...\\n\")\n",
    "\n",
    "print(\"‚úÖ Cell 2B executed successfully ‚Äî documents loaded & chunked for all categories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d529f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model 'sentence-transformers/all-MiniLM-L6-v2' initialized.\n",
      "‚úÖ Cell 3A executed successfully ‚Äî embedding & FAISS helper functions ready.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3A. Embeddings & FAISS Helper Functions\n",
    "# \n",
    "# This cell:\n",
    "# - Defines global embedding configuration\n",
    "# - Initializes the HuggingFace embedding model\n",
    "# - Defines functions to build and persist FAISS vector stores\n",
    "\n",
    "# --- Embedding configuration ---\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # small, fast, good quality\n",
    "EMBED_CACHE_DIR = \"embeddings\"\n",
    "INDEX_DIR = \"faiss_indexes\"\n",
    "\n",
    "os.makedirs(EMBED_CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME,\n",
    "    cache_folder=EMBED_CACHE_DIR,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Embedding model '{EMBED_MODEL_NAME}' initialized.\")\n",
    "\n",
    "\n",
    "def build_faiss_index_for_category(category, chunks):\n",
    "    \"\"\"\n",
    "    Builds a FAISS index for the given category from chunked text data.\n",
    "    Saves the index to disk in INDEX_DIR/<category>/\n",
    "    Returns the FAISS object.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        print(f\"[WARN] No chunks found for category '{category}'. Skipping index creation.\")\n",
    "        return None\n",
    "\n",
    "    texts = [item[\"text\"] for item in chunks]\n",
    "    metadatas = [item[\"meta\"] for item in chunks]\n",
    "\n",
    "    try:\n",
    "        db = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)\n",
    "        save_path = os.path.join(INDEX_DIR, category)\n",
    "        db.save_local(save_path)\n",
    "        print(f\"‚úÖ FAISS index for '{category}' saved to '{save_path}' ({len(texts)} vectors)\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to build FAISS for '{category}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_all_faiss_indexes(grouped_chunks_dict):\n",
    "    \"\"\"\n",
    "    Builds FAISS indexes for all categories and returns a dictionary of FAISS stores.\n",
    "    \"\"\"\n",
    "    faiss_indexes = {}\n",
    "    for category, chunks in grouped_chunks_dict.items():\n",
    "        print(f\"\\n[INFO] Building FAISS index for category: {category}\")\n",
    "        index = build_faiss_index_for_category(category, chunks)\n",
    "        if index:\n",
    "            faiss_indexes[category] = index\n",
    "    print(\"\\n‚úÖ All FAISS indexes built successfully.\")\n",
    "    return faiss_indexes\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cell 3A executed successfully ‚Äî embedding & FAISS helper functions ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae849e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'ABHISHEK  RANJAN                  +91  9040140733   \\n Data  Scientist  |  AI  Engineer                     13eee079@gmail.com  \\n \\nABOUT  ME  \\nData  Scientist  with  3.9   years  of  experience,  with  expertise  in  Machine  Learning,  Deep  Learning,  Generative  AI,  NLP,  \\nLLMs,\\n \\nand\\n \\nRAG.\\n \\nSkilled\\n \\nin\\n \\nfine-tuning\\n \\nHugging\\n \\nFace\\n \\nmodels,\\n \\nbuilding\\n \\nAI-driven\\n \\nsolutions,\\n \\nand\\n \\ndeploying\\n \\nscalable\\n \\narchitectures\\n \\nusing\\n \\nLangChain\\n \\nand\\n \\nOpenAI\\n \\nAPI.\\n \\nPassionate\\n \\nabout\\n \\nAI\\n \\ninnovation\\n \\nand\\n \\nsolving\\n \\ncomplex\\n \\nchallenges\\n \\nwith\\n \\ndata-driven\\n \\nsolutions.\\nSKILLS  \\nProgramming  Languages:  \\nPython,\\n \\nSQL\\n Deep  Learning  &  AI  Architectures:  \\nArtificial\\n \\nNeural\\n \\nNetworks\\n \\n(ANN),\\n \\nConvolutional\\n \\nNeural\\n \\nNetworks\\n \\n(CNN),\\n \\nRecurrent\\n \\nNeural\\n \\nNetworks\\n \\n(RNN),\\n \\nLong\\n \\nShort-Term\\n \\nMemory\\n \\n(LSTM),\\n \\nTransformers,\\n \\nLarge\\n \\nLanguage\\n \\nModels\\n \\n(LLMs)\\n Libraries  &  Frameworks:  \\nNumPy,\\n \\nPandas,\\n \\nMatplotlib,\\n \\nSeaborn,\\n \\nScikit-Learn,\\n \\nTensorFlow,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_0'}},\n",
       " {'text': 'Long\\n \\nShort-Term\\n \\nMemory\\n \\n(LSTM),\\n \\nTransformers,\\n \\nLarge\\n \\nLanguage\\n \\nModels\\n \\n(LLMs)\\n Libraries  &  Frameworks:  \\nNumPy,\\n \\nPandas,\\n \\nMatplotlib,\\n \\nSeaborn,\\n \\nScikit-Learn,\\n \\nTensorFlow,\\n \\nPyTorch,\\n \\nKeras,\\n \\nLangChain,\\n \\nNLTK,\\n \\nOpenAI\\n \\nAPIs,\\n \\nHugging\\n \\nFace,\\n \\nFastAPI\\n Machine  Learning  &  AI  Algorithms:  \\nLinear\\n \\nRegression,\\n \\nLogistic\\n \\nRegression,\\n \\nDecision\\n \\nTrees,\\n \\nRandom\\n \\nForest,\\n \\nSupport\\n \\nVector\\n \\nMachines\\n \\n(SVM),\\n \\nNa√Øve\\n \\nBayes,\\n \\nK-Nearest\\n \\nNeighbors\\n \\n(KNN),\\n \\nK-Means\\n \\nClustering,\\n \\nPrincipal\\n \\nComponent\\n \\nAnalysis\\n \\n(PCA),\\n \\nNeural\\n \\nNetworks,\\n \\nXGBoost,\\n \\nLightGBM,\\n \\nCatBoost\\n Techniques  &  Domains:  \\nSupervised\\n \\nLearning,\\n \\nUnsupervised\\n \\nLearning,\\n \\nMachine\\n \\nLearning,\\n \\nDeep\\n \\nLearning,\\n \\nGenerative\\n \\nAI,\\n \\nNatural\\n \\nLanguage\\n \\nProcessing\\n \\n(NLP),\\n \\nTransfer\\n \\nLearning,\\n \\nEnsemble\\n \\nLearning,\\n \\nTime\\n \\nSeries\\n \\nForecasting,\\n \\nStatistical\\n \\nModeling,\\n \\nPredictive\\n \\nModeling,\\n \\nClassification,\\n \\nRegression,\\n \\nClustering,\\n \\nAnomaly\\n \\nDetection,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_1'}},\n",
       " {'text': 'Transfer\\n \\nLearning,\\n \\nEnsemble\\n \\nLearning,\\n \\nTime\\n \\nSeries\\n \\nForecasting,\\n \\nStatistical\\n \\nModeling,\\n \\nPredictive\\n \\nModeling,\\n \\nClassification,\\n \\nRegression,\\n \\nClustering,\\n \\nAnomaly\\n \\nDetection,\\n \\nDimensionality\\n \\nReduction,\\n \\nFeature\\n \\nEngineering,\\n \\nFeature\\n \\nSelection,\\n \\nHyperparameter\\n \\nTuning,\\n \\nStatistical\\n \\nAnalysis,\\n \\nModel\\n \\nInterpretability,\\n \\nMLOps,\\n \\nLLMOps,\\n \\nPrompt\\n \\nEngineering\\n Tools  &  Platforms:  \\nDocker,\\n \\nKubernetes,\\n \\nAirflow,\\n \\nMLflow,\\n \\nDVC,\\n \\nDjango,\\n \\nFlask,\\n \\nBitbucket,\\n \\nGit,\\n \\nPower\\n \\nBI,\\n \\nTableau,\\n \\nExcel,\\n \\nAnaconda,\\n \\nPyCharm,\\n \\nVisual\\n \\nStudio\\n \\nCode,\\n \\nGoogle\\n \\nColab,\\n \\nStreamlit\\n Cloud  &  Infrastructure:  \\nAmazon\\n \\nWeb\\n \\nServices\\n \\n(AWS),\\n \\nGoogle\\n \\nCloud\\n \\nPlatform\\n \\n(GCP),\\n \\nMicrosoft\\n \\nAzure\\n Mathematics  &  Statistical  Foundations:  \\nStatistics,\\n \\nProbability\\n \\nTheory,\\n \\nLinear\\n \\nAlgebra,\\n \\nCalculus,\\n \\nOptimization\\n \\nTechniques',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_2'}},\n",
       " {'text': 'Google\\n \\nCloud\\n \\nPlatform\\n \\n(GCP),\\n \\nMicrosoft\\n \\nAzure\\n Mathematics  &  Statistical  Foundations:  \\nStatistics,\\n \\nProbability\\n \\nTheory,\\n \\nLinear\\n \\nAlgebra,\\n \\nCalculus,\\n \\nOptimization\\n \\nTechniques\\n \\n \\nCERTIFICATIONS   Complete  Generative  AI  Course  with  Lang  Chain  and  Hugging  Face  -  Udemy  Generative  Artificial  Intelligence  -  Databricks',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_3'}},\n",
       " {'text': 'EXPERIENCE   \\nData  Scientist  |  Fractal  Analytics  |  July  2024  ‚Äì  Present  |  Gurgaon  \\nGenAI-driven  News  Intelligence  System   \\nDesigning\\n \\nan\\n \\nautomated\\n \\nGenAI-based\\n \\nplatform\\n \\nusing\\n \\nGPT-4\\n \\nfor\\n \\nreal-time\\n \\nnews\\n \\nclassification,\\n \\nsummarization,\\n \\nand\\n \\nrisk\\n \\nalerting.\\n \\nClassified\\n \\nmedia\\n \\narticles\\n \\nby\\n \\nsentiment,\\n \\nproduct\\n \\nmentions,\\n \\nand\\n \\nregulatory\\n \\nevents;\\n \\nbuilt\\n \\ndashboards\\n \\nfor\\n \\nbrand\\n \\nvisibility\\n \\ntracking\\n \\n&\\n \\ncrisis\\n \\ndetection,\\n \\nenhancing\\n \\nbrand\\n \\nmonitoring\\n \\nefficiency\\n \\nby\\n \\n90%.\\n \\n \\nData  Scientist  |  1DigitalStack.ai  |  July  2024  ‚Äì  June  2025  |  Gurgaon  \\nDemand  Sensing  for  Sales  Forecasting  \\nBuilt\\n \\nmachine\\n \\nlearning\\n \\npipelines\\n \\n(Bayesian\\n \\nRidge,\\n \\nProphet,\\n \\nRandom\\n \\nForest)\\n \\nfor\\n \\nsecondary\\n \\nsales\\n \\nforecasting\\n \\nat\\n \\nASM\\n \\n√ó\\n \\nPSKU\\n \\n√ó\\n \\nDepot\\n \\nlevel,\\n \\nimproving\\n \\nplanning\\n \\naccuracy\\n \\nacross\\n \\nGeneral\\n \\nTrade,\\n \\nModern\\n \\nTrade,\\n \\nand\\n \\nE-commerce\\n \\nchannels.\\n \\nAutomated\\n \\nend-to-end\\n \\npipelines\\n \\nfrom',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_4'}},\n",
       " {'text': 'at\\n \\nASM\\n \\n√ó\\n \\nPSKU\\n \\n√ó\\n \\nDepot\\n \\nlevel,\\n \\nimproving\\n \\nplanning\\n \\naccuracy\\n \\nacross\\n \\nGeneral\\n \\nTrade,\\n \\nModern\\n \\nTrade,\\n \\nand\\n \\nE-commerce\\n \\nchannels.\\n \\nAutomated\\n \\nend-to-end\\n \\npipelines\\n \\nfrom\\n \\nSnowflake\\n \\ningestion\\n \\nto\\n \\nBPM\\n \\nforecast\\n \\ndeployment,\\n \\nintegrating\\n \\nbusiness\\n \\nrules\\n \\nfor\\n \\ndynamic\\n \\nplan\\n \\nadjustment.\\n \\nFine-Tuning  Text-to-Image  Generation  for  Product  Marketing  \\nExecuted\\n \\na\\n \\nproof\\n \\nof\\n \\nconcept\\n \\n(POC)\\n \\nto\\n \\nfine-tune\\n \\na\\n \\npre-trained\\n \\nStable\\n \\nDiffusion\\n \\nmodel\\n \\nfor\\n \\nMarico‚Äôs\\n \\nproduct\\n \\nmarketing.\\n \\nCreated\\n \\na\\n \\ncustom\\n \\ndataset\\n \\nof\\n \\nproduct\\n \\nimages\\n \\nand\\n \\ntext\\n \\ndescriptions,\\n \\nperformed\\n \\nmodel\\n \\nfine-tuning\\n \\nto\\n \\ngenerate\\n \\nbrand-aligned\\n \\nvisuals\\n \\nfrom\\n \\ntext\\n \\nprompts,\\n \\nand\\n \\ndeployed\\n \\na\\n \\nprototype\\n \\nAPI\\n \\nto\\n \\nautomate\\n \\nmarketing\\n \\ncontent\\n \\ncreation,\\n \\nenhancing\\n \\nscalability\\n \\nand\\n \\ncreative\\n \\nconsistency.\\n \\n \\nAssociate  Data  Scientist  |  Blackstraw.ai  |  Sept  2021  ‚Äì  June  2024  |  Chennai   \\nEmail  Extraction',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_5'}},\n",
       " {'text': 'content\\n \\ncreation,\\n \\nenhancing\\n \\nscalability\\n \\nand\\n \\ncreative\\n \\nconsistency.\\n \\n \\nAssociate  Data  Scientist  |  Blackstraw.ai  |  Sept  2021  ‚Äì  June  2024  |  Chennai   \\nEmail  Extraction   \\nDeveloped\\n \\nan\\n \\nend-to-end\\n \\nautomated\\n \\nsolution\\n \\nusing\\n \\nLayoutLMv2\\n \\nto\\n \\nextract\\n \\nstructured\\n \\npurchase\\n \\ndata\\n \\nfrom\\n \\nmulti-format\\n \\ne-commerce\\n \\nemails\\n \\n(Amazon,\\n \\nWalmart).\\n \\nBuilt\\n \\nfull\\n \\npipelines\\n \\nincluding\\n \\nEML-to-image\\n \\npreprocessing,\\n \\nmodel\\n \\ntraining,\\n \\nrule-based\\n \\npost-processing,\\n \\nand\\n \\nscalable\\n \\ndeployment\\n \\nacross\\n \\ncloud\\n \\nand\\n \\non-premise\\n \\nsetups.\\n \\nSales  Forecasting  using  Excel   Built  dynamic  sales  forecasting  models  using  moving  averages,  exponential  smoothing,  and  regression  \\ntechniques\\n \\nin\\n \\nExcel.\\n \\nAutomated\\n \\nforecast\\n \\nreporting\\n \\nwith\\n \\npivot\\n \\ntables,\\n \\nscenario\\n \\nanalysis,\\n \\nand\\n \\nadvanced\\n \\nformulas\\n \\nenabling\\n \\nfaster\\n \\nbusiness\\n \\ndecision\\n \\nsupport.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_6'}},\n",
       " {'text': 'techniques\\n \\nin\\n \\nExcel.\\n \\nAutomated\\n \\nforecast\\n \\nreporting\\n \\nwith\\n \\npivot\\n \\ntables,\\n \\nscenario\\n \\nanalysis,\\n \\nand\\n \\nadvanced\\n \\nformulas\\n \\nenabling\\n \\nfaster\\n \\nbusiness\\n \\ndecision\\n \\nsupport.\\n \\n  \\n \\nEDUCATION  Post  Graduate  Program  in  Data  Analytics  |  Imarticus  Learning,  New  Delhi  |  2020  ‚Äì  2021  \\nBachelor\\n \\nof\\n \\nTechnology\\n \\n(B.Tech)\\n \\n|\\n \\nElectrical\\n \\n&\\n \\nElectronics\\n \\nEngineering\\n \\n|\\n \\nGIET,\\n \\nGunupur\\n \\n|\\n \\n2013\\n \\n-\\n \\n2017',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Abhishek_Ranjan_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_7'}},\n",
       " {'text': 'Akash Mondal Email : akashmondal1810@gmail.com\\nhttp://akashmondal1810.github.io/ Mobile : +91-7478060888\\nEducation\\n‚Ä¢ Indian Institute of Technology, Kharagpur Kharagpur, WB\\nB.Tech + M.Tech Integrated Dual Degree 5Y (Civil Engineering) July 2016 ‚Äì June 2021\\nExperience\\n‚Ä¢ Assistant Manager - Data Science | American Express | Bengaluru, IN Aug‚Äô21 - Present\\nGen7 Credit-Risk Prediction Models\\n‚ó¶ Developing the Enhanced version of machine learning models for credit risk estimation in the US Consumer portfolio,\\nenabling crucial decisions like POS, New Account authorization (transaction volume $1B/Month), and negative actionings.\\n‚ó¶ Designed the novel approach of survival prediction, to achieve enhanced responsiveness to delinquency rise to mitigate\\nunderprediction risk. Leveraging AWS EMR to efficiently train the model and PySpark for seamless data preparation.\\nGen6 Credit-Risk Prediction Models | Awarded Analyst of Quarter for excellent project performance',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_8'}},\n",
       " {'text': 'Gen6 Credit-Risk Prediction Models | Awarded Analyst of Quarter for excellent project performance\\n‚ó¶ Developed the enhanced machine learning models to predict customer‚Äôs credit risk, achieving a notable increase of PTI\\n$8.6M/yr compared to the Gen5 model with Gini coefficient exceeding 92% while prioritizing model interpretability\\n‚ó¶ Enhanced the model framework to predict both short and long-term risks, incorporating macroeconomic shifts. Streamlined\\nmodel configuration through Bayesian HPT and implemented a data quality checking tool, ensuring accurate prediction.\\nAutomated Model Health Tracker | Awarded SVP Award for excellent project performance\\n‚ó¶ Implemented an Automated tracking tool to check our risk model‚Äôs health in various risk segments. Engineered the whole\\npipeline using PySpark and SQL, processing terabytes of Data, and preventing credit losses of more than $0.8M/month.\\nRLA Model for Negative Actioning',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_9'}},\n",
       " {'text': 'pipeline using PySpark and SQL, processing terabytes of Data, and preventing credit losses of more than $0.8M/month.\\nRLA Model for Negative Actioning\\n‚ó¶ Uplifted the RLA model for proactive credit line decrease to generate simplified MRC to improve communication verbiage,\\nresulting in enhanced customer experience and improved default capture rate with a lift of 7% in the high-risk CM segments.\\n‚Ä¢ Research Engineer Intern | American Express ‚Äî Paper May‚Äô20 - Jul‚Äô20\\n‚ó¶ Worked with the Decision Science Team to create probabilistic models for predictive uncertainty estimation\\n‚ó¶ Used MC Dropout, Deep Ensemble and introduced two novel gradient boosting-based models to quantify uncertainty\\n‚ó¶ Received a full-time return offer for showcasing excellence in performance and project results\\n‚Ä¢ Student Developer | American Express ‚Äî Paper Sep‚Äô19 - Nov‚Äô19\\n‚ó¶ Created a domain-specific factoid question answering system, trainable in multiple domains with real-time answering,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_10'}},\n",
       " {'text': '‚Ä¢ Student Developer | American Express ‚Äî Paper Sep‚Äô19 - Nov‚Äô19\\n‚ó¶ Created a domain-specific factoid question answering system, trainable in multiple domains with real-time answering,\\nconsisting of Question Processing, Document Ranking, and BERT based Answer Extraction process.\\n‚ó¶ Received Internship offer for showcasing excellence in performance and results. Advisor: Saradindu Kar\\n‚Ä¢ Intern | National AI Resource Portal | CAI IIT Kharagpur May‚Äô19 - Jul‚Äô19\\n‚ó¶ Developed a conversational chatbot using the Rasa framework for Search and Browse of the AI Resources\\n‚ó¶ Implemented a screen scraper, to extract and organize data from various publicly available machine learning repositories\\n‚Ä¢ Machine Learning Engineer Intern | CodeFire Technologies May‚Äô18 - Jul‚Äô18\\n‚ó¶ Created an information retrieval based question answering system for answering queries related to school admission\\n‚ó¶ Developed an automated tool for Extraction, Transformation, and Loading(ETL) less structured web content data efficiently',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_11'}},\n",
       " {'text': '‚ó¶ Developed an automated tool for Extraction, Transformation, and Loading(ETL) less structured web content data efficiently\\nProjects & Competition\\n‚Ä¢ Localization of the Predominant Object in an Image Apr‚Äô19\\n‚ó¶ Ranked among the top 5% in Flipkart GRID machine learning challenge-2019 with more than 7000 registered individuals with\\na mean IOU of 89% in bounding boxes prediction to Locate the Predominant Object in the given Image\\n‚Ä¢ Classification of Argumentative Elements in Student Writing Jun‚Äô22\\n‚ó¶ Developed a predictive model to predict the effectiveness of discourse elements in the student-written argumentative essays\\nusing the RoBerta-base Model. LL Score:0.649(top 30% in the leaderboard), Link: Akash2\\n‚Ä¢ Pavement Performance Prediction Oct‚Äô20\\n‚ó¶ Used machine learning approaches to develop pavement performance prediction(PPP) models in pavement management\\nsystems to predict the functional and structural conditions of the pavement, guided by Prof. K. S. Reddy, IIT Kharagpur.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_12'}},\n",
       " {'text': 'systems to predict the functional and structural conditions of the pavement, guided by Prof. K. S. Reddy, IIT Kharagpur.\\n‚Ä¢ Expanse & Income Tracker ‚Äî Link Apr‚Äô18\\n‚ó¶ Developed an expanse and income tracker web app with user-authentication features to track personal expenses and income\\nas a part of the Microsoft CodeFunDo event. Implemented the backend using PHP and the database schema using MySQL.\\nSkills\\n‚Ä¢ General: Machine Learning, Deep Learning, Natural Language Processing, Big Data Analysis, EDA\\n‚Ä¢ Modelling: XGBoost/Gradient Boosting, Linear Models, Regression, CNN, RNN, LSTM, Transformer/BERT, Bayesian NN\\n‚Ä¢ Languages: Python, C++, C, Apache Spark/PySpark, SQL, SAS, shell scripting, HTML, CSS, MATLAB\\n‚Ä¢ Tools/Others: Linux, Windows, LATEX, Git, TensorFlow, Keras, scikit-learn, Apache Hive, Hadoop, AWS, S3, Pandas, Numpy',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_13'}},\n",
       " {'text': 'Relevant Courses & Certification\\nProgramming & Data Structures, Algorithm-I, Machine Learning, Probability Statistics, Databases, Deep Learning\\nFoundation, Natural Language Processing, Partial Differential Equations, Amazon Web Services ML Essential\\nAchievement/Competition/Por\\n‚Ä¢ Publicis Sapient Jumpstart 2019: One of the top scorers and was invited for the offline event with the core team\\n‚Ä¢ Microsoft code.fun.do Campus Hackathon 2018 : Among top 5 teams out of above 30 teams in the event\\n‚Ä¢ ACM-ICPC Asia Kolkata-Kanpur Onsite Replay Contest 2017 : Global Rank-12 and 9th in India in the Programming\\nCompetition at www.codechef.com. Team name: kgp16ce. Competition/Result Link: rank link',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Akash_Mondal_DS_2yExp.pdf',\n",
       "   'chunk_id': 'CV_chunk_14'}},\n",
       " {'text': 'Amlan Bose \\nApplication Developer \\nDynamic and creative software developer with 2.3+ years of\\nexperience in producing robust code for Multinational Insurance\\nCompany and Restaurant Chains. Motivated and Eager to support\\nthe team with my skills. \\nmeamlan24@outlook.com \\n8585857983 \\nIndia \\nin.linkedin.com/in/amlan-bose-70a221191 \\ngithub.com/Amlan24 \\nWORK EXPERIENCE \\nAssociate \\nCognizant \\n11/2019 - 02/2022\\n, \\n \\nKolkata, India \\nDesigned, built, and maintained core backend software\\ncomponents following design patterns and using Java. \\nImplemented \\nexception handling to improve application\\nfunctionality. Used node.js as a proxy to interact with\\nRESTful services and interacting with PostgreSQL\\nDatabase. \\nResponsible for API design and development of RESTful\\nServices for the enterprise Product in the business. \\nWorked on Jenkins to implement CI/CD. Created Docker\\nimages using a Docker Ô¨Åle, worked on Docker container\\nsnapshots, removing images and managing Docker',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Amlan's Resume_SWE_2YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_15'}},\n",
       " {'text': 'Worked on Jenkins to implement CI/CD. Created Docker\\nimages using a Docker Ô¨Åle, worked on Docker container\\nsnapshots, removing images and managing Docker\\nvolumes. Used Bitbucket for VCS. \\nKnowledge of \\nAWS \\ncloud service like \\nCompute, Network,\\nStorage \\nand \\nIdentity & access management. \\nExperience in building producer applications for KAFKA. \\nJava Developer \\nTCS \\n03/2022 - Present\\n, \\n \\nRevamped Applications using Spring, JAVA , JSP/Servlets,\\nMVC architecture, JDBC, oracle, SQL, XML. \\nBuilt web services with Java that were used by the\\ncustomer facing app to improve the time on page. \\nPERSONAL PROJECTS \\nSort Visualizer\\n (04/2021 - 05/2021)\\n \\nDesigned a Sort Visualizer using Python and Tkinter for GUI that\\nhelps user to understands how sorting works \\nTEACHER STUDENT CONNECT\\n (01/2020 - 02/2020)\\n \\nDesigned a Web Application where teacher and students could\\nconnect to share Ô¨Åles and look for homework and class\\nschedule. Used JSP,JS for frontend, Servlet , JAVA for backend',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Amlan's Resume_SWE_2YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_16'}},\n",
       " {'text': '(01/2020 - 02/2020)\\n \\nDesigned a Web Application where teacher and students could\\nconnect to share Ô¨Åles and look for homework and class\\nschedule. Used JSP,JS for frontend, Servlet , JAVA for backend\\nand MYSql database. Hosted in Tomcat server. \\nREAL TIME OBJECT DETECTION\\n (02/2019 - 05/2019)\\n \\nDesigned a real time objects detection using OpenCv and also\\nused YOLO Framework. Were able to detect 80 diÔ¨Äerent type\\nof objects. \\nEDUCATION \\nBachelor of Technology in Computer\\nScience and Engineering \\nFuture Institute Of Technology \\n07/2015 - 06/2019\\n, \\n \\nKolkata,India \\nSKILLS \\nPython \\nNode \\nJDBC \\nJSP \\nServlet \\nSQL \\nSpring Boot \\nReact \\nGit \\nBitbucket \\nJenkins \\nDocker \\nRestApi \\nJira \\nJ2EE \\nPostman \\nHTML \\nCSS \\nJava Script \\nJava 8 \\nUnit Testing \\nPostgreSQL \\nAgile \\nMaven \\nAWS \\nAzure \\nACHIEVEMENTS \\nAwarded Employee Of the Month in April,2020 \\n5 Star problem Solver in Hacker rank \\nScored 75% in DBMS NPTEL \\nCERTIFICATES \\nAndroid Development\\n (07/2020 - 08/2020)',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Amlan's Resume_SWE_2YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_17'}},\n",
       " {'text': 'Maven \\nAWS \\nAzure \\nACHIEVEMENTS \\nAwarded Employee Of the Month in April,2020 \\n5 Star problem Solver in Hacker rank \\nScored 75% in DBMS NPTEL \\nCERTIFICATES \\nAndroid Development\\n (07/2020 - 08/2020)\\n \\nProject-Centered Course authorized by Centrale Sup√©lec and oÔ¨Äered\\nthrough Coursera \\nData Analysis in Python\\n (02/2021 - 04/2021)\\n \\nOÔ¨Äered by University of Helsinki \\nData Science Math Skills\\n (06/2020 - 07/2020)\\n \\nOÔ¨Äered by Duke University through Coursera \\nGetting Started with AWS Machine Learning\\n (10/2020 - 11/2020)\\n \\nOÔ¨Äered by Amazon through Coursera \\nUsing Python to Access Web Data\\n (05/2021 - 06/2021)\\n \\nOÔ¨Äered by University Of Michigan \\nLANGUAGES \\nEnglish , Hindi , Bengali \\nNative or Bilingual ProÔ¨Åciency \\nINTERESTS \\nAlgorithms \\nCoding \\nNetworking \\nAI \\nAchievements/Tasks \\nAchievements/Tasks',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Amlan's Resume_SWE_2YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_18'}},\n",
       " {'text': 'Ammar  Hussain  +91-9162328642  |  ammarue188014it@gmail.com |  Linkedin  \\nAndroid  Developer  with  over  2.9+  years  of  experience  in  building  and  optimizing  user-centric  applications.  Strong  skills  in  Kotlin,  Android  SDK,  and  Jetpack  \\ncomponents\\n \\nwith\\n \\na\\n \\nfocus\\n \\non\\n \\ndata\\n \\nsynchronization\\n \\nand\\n \\napplication\\n \\nperformance.\\n \\nProven\\n \\nsuccess\\n \\nin\\n \\nhandling\\n \\ncritical\\n \\nissues\\n \\nand\\n \\ndelivering\\n \\nseamless\\n \\nmobile\\n \\nand\\n \\nwearable\\n \\nexperiences.\\n \\nPassionate\\n \\nabout\\n \\nsolving\\n \\nchallenging\\n \\nproblems\\n \\nand\\n \\ncontinuously\\n \\nimproving\\n \\napp\\n \\nperformance.\\n \\nSKILLS  \\n‚óè  \\nKotlin\\n \\n,\\n \\nJava\\n \\n,\\n \\nC/C++\\n \\n,\\n \\nXML,\\n \\nJunit\\n \\nTesting\\n \\n,\\n \\nJetpack\\n \\nComponents\\n \\n,\\n \\nAndroid\\n \\nUI/UX\\n \\n,\\n \\nBackend\\n \\nAPI\\n \\n‚óè  \\nGit\\n \\n,\\n \\nAndroid\\n \\nDevelopment\\n \\n,\\n \\nData\\n \\nStructure\\n \\n,\\n \\nAlgorithms\\n \\n,\\n \\nProblem\\n \\nsolving\\n \\n,\\n \\nAndroid\\n \\nSDK\\n \\n,\\n \\nMVVM\\n EDUCATION',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\AmmarResume_2025-2.pdf',\n",
       "   'chunk_id': 'CV_chunk_19'}},\n",
       " {'text': \",\\n \\nAndroid\\n \\nUI/UX\\n \\n,\\n \\nBackend\\n \\nAPI\\n \\n‚óè  \\nGit\\n \\n,\\n \\nAndroid\\n \\nDevelopment\\n \\n,\\n \\nData\\n \\nStructure\\n \\n,\\n \\nAlgorithms\\n \\n,\\n \\nProblem\\n \\nsolving\\n \\n,\\n \\nAndroid\\n \\nSDK\\n \\n,\\n \\nMVVM\\n EDUCATION  \\n \\nBE  -  UIET,  Panjab  University,  Chandigarh  Bachelor  of  Engineering  in  Computer  Science                                                                                                    August  2018  -  22  GPA :  8.40  \\nEXPERIENCE\\n \\n \\nSoftware  Engineer  |  Samsung  R&D,  Noida:                                                     June  2022  -  Present  ‚óè  Developed  a  Device  Interaction  module  for  real-time  transmission  of  watch  sensor  data  to  mobile  devices  via  GMS,  \\nindependently\\n \\ndesigning\\n \\nthe\\n \\narchitecture\\n \\nand\\n \\nimplementing\\n \\nGMS\\n \\ncommunication.\\n ‚óè  Engineered  backend  data  sync  between  watch  and  mobile  in  Samsung  Health's  Wearable  Framework,  ensuring  \\nseamless\\n \\ncross-platform\\n \\ndata\\n \\nflow.\\n \\nIntegrated\\n \\nadvanced\\n \\nfeatures\\n \\nand\\n \\nresolved\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\AmmarResume_2025-2.pdf',\n",
       "   'chunk_id': 'CV_chunk_20'}},\n",
       " {'text': \"seamless\\n \\ncross-platform\\n \\ndata\\n \\nflow.\\n \\nIntegrated\\n \\nadvanced\\n \\nfeatures\\n \\nand\\n \\nresolved\\n \\nGMS-related\\n \\nissues.\\n ‚óè  Implemented  file-type  data  synchronization  between  Samsung  Health  mobile  and  watch,  optimizing  large  file  transfers  \\nand\\n \\nreducing\\n \\nsync\\n \\ntime\\n \\nby\\n \\n10%\\n \\nthrough\\n \\nparallel\\n \\ncoroutine\\n \\nexecution. ‚óè  Implemented  the  Sync  Layer  in  Android  for  Samsung  Health,  enabling  large  data  transfer  in  chunks  to  prevent  OOM  \\n(Out\\n \\nof\\n \\nMemory)\\n \\nerrors.\\n ‚óè  Refactored  job  scheduling  system  from  legacy  APIs  (e.g.,  JobScheduler ,  AlarmManager )  to  WorkManager ,  \\nimproving\\n \\ntask\\n \\nreliability\\n \\nand\\n \\nefficiency\\n \\nin\\n \\nhandling\\n \\nbackground\\n \\noperations\\n \\nwhile\\n \\nadhering\\n \\nto\\n \\nAndroid's\\n \\nbest\\n \\npractices\\n \\nfor\\n \\nbattery\\n \\nand\\n \\nnetwork\\n \\nusage.\\n ‚óè  Upgraded  in  Kotlin  Jetpack  Compose  like  Data  Binding,  Room,  ViewModel,MVVM  ,Developed  and  optimized  RunTime  \\nResource\\n \\nOverlay\\n \\n(RRO),\\n \\nAIDL\\n \\netc\\n.\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\AmmarResume_2025-2.pdf',\n",
       "   'chunk_id': 'CV_chunk_21'}},\n",
       " {'text': \"and\\n \\nnetwork\\n \\nusage.\\n ‚óè  Upgraded  in  Kotlin  Jetpack  Compose  like  Data  Binding,  Room,  ViewModel,MVVM  ,Developed  and  optimized  RunTime  \\nResource\\n \\nOverlay\\n \\n(RRO),\\n \\nAIDL\\n \\netc\\n.\\n ‚óè  Wrote  Junit  TCs  for  the  project,  used  Mockito,  Mockk  etc.  \\nSoftware  Intern  |  Samsung  R&D,  Noida:                                                                                        \\nJan'\\n \\n22-June\\n \\n22\\n ‚óè  Gained  proficiency  in  various  tools  and  technologies  during  my  internship.  ‚óè  Learned  and  applied  machine  learning  algorithms  to  innovative  projects.  Converted  black  and  white  images  to  colored  images  \\nusing\\n \\nadvanced\\n \\nmachine\\n \\nlearning\\n \\ntechniques.\\n PROJECTS\\n           \\nProjemang(Project\\n \\nManagement\\n \\nAndroid\\n \\nApp)\\n \\n ‚óè  Technologies  Used:  Kotlin,  XML,  Firebase,  Android  Studio   ‚óè  Developed  an  Android  application  for  project  management,  enabling  scrum  team  members  to  assign  tasks  with  due  dates,  \\nimproving\\n \\nteam\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\AmmarResume_2025-2.pdf',\n",
       "   'chunk_id': 'CV_chunk_22'}},\n",
       " {'text': 'improving\\n \\nteam\\n \\ncoordination\\n \\nand\\n \\nproductivity.\\n \\n ‚óè  Implemented  secure  sign-in  and  sign-up  features  via  Gmail,  enhancing  user  convenience  and  security.   \\n            Gym  Management  Android  App  ‚óè  Technologies  Used:  Kotlin,  XML,  Firebase,  Android  Studio   ‚óè  Developed  functionalities  to  maintain  comprehensive  records  of  gym  members,  including  membership  details,  attendance,  \\nand\\n \\npayment\\n \\nhistory.\\n  \\nIntegrated\\n \\nFirebase\\n \\nfor\\n \\nreal-time\\n \\ndata\\n \\nstorage\\n \\nand\\n \\nsynchronization,\\n \\nproviding\\n \\ngym\\n \\nowners\\n \\nwith\\n \\nup-to-date\\n \\ninformation\\n \\nand\\n \\nseamless\\n \\nuser\\n \\nexperience.\\n \\n ACHIEVEMENT  ‚óè  Achieved   Professional  Level  Grade  in  the  SW  Certificate  Test  at  Samsung  ,  demonstrating  advanced  Coding   skills  is  Data  \\nStructures\\n \\nand\\n \\nAlgorithms.\\n ‚óè  Cleared  Infosys  Hackwithinfy ,  securing  an  offer  for  the  prestigious  Power  Programmer  role  out  of  approximately  200,000  \\nparticipants,\\n \\nwith\\n \\nonly\\n \\n600',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\AmmarResume_2025-2.pdf',\n",
       "   'chunk_id': 'CV_chunk_23'}},\n",
       " {'text': 'and\\n \\nAlgorithms.\\n ‚óè  Cleared  Infosys  Hackwithinfy ,  securing  an  offer  for  the  prestigious  Power  Programmer  role  out  of  approximately  200,000  \\nparticipants,\\n \\nwith\\n \\nonly\\n \\n600\\n \\nselected,\\n \\nshowcasing\\n \\nexceptional\\n \\nproblem-solving\\n \\nabilities\\n \\nand\\n \\ncoding\\n \\nproficiency.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\AmmarResume_2025-2.pdf',\n",
       "   'chunk_id': 'CV_chunk_24'}},\n",
       " {'text': 'Anubhav Gahlot \\nLinkedin|LeetCode|GitHub|anubhavgahlot28@gmail.com |+919871631764  \\nEDUCATION AND SCHOLASTIC ACHIEVEMENTS \\nProgram Institute Year \\nJt. MSc in Data Science and AI (IITM + UoB) Indian Institute of Technology, Madras Jan 15, 2025 \\nB.Tech in CSE with specialization in AI&ML UPES, Dehradun 2023 \\nClass XII (CBSE) Vivekanand School, New Delhi 2019 \\nClass X (CBSE) Vivekanand School, New Delhi 2017 \\n \\nACHIEVEMENTS \\nLeetCode Won 300 Days Badge 2023 on Leetcode(Solved 430+ Problems), Profile: Anubhav_Gahlot \\nIBM- \\nInnovation \\nCenter for \\nEducation \\n(IBM-ICE) \\n‚Ä¢ Project - (Deep Neural Networks Based Plant Disease Detection by Leaf Image Classification)  \\n‚Ä¢ 2nd Runner-up in Project Presentation at IBM ICE Event. \\n‚Ä¢ Competed with 100 teams for Project Presentation. \\n‚Ä¢ Collaborated with a remarkable team to develop and present an end-to-end solution for plant disease detection,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_25'}},\n",
       " {'text': '‚Ä¢ Collaborated with a remarkable team to develop and present an end-to-end solution for plant disease detection,                                          \\nsuccessfully demonstrating its application in a real-world scenario. \\n \\nRESEARCH PROJECTS \\nResearch Project \\nCeRAI \\n(Jun ‚Äì Dec‚Äô24) \\nEnhancing Fairness in Financial Data Models \\nGuide: Prof Balaraman Ravindran(IIT Madras) & Prof Samuel M Hernandez (University of Birmingham, UK) \\n‚Ä¢ Developed a bias detection and mitigation framework for mortgage underwriting decisions using Large Language \\nModels (LLMs) by implementing prompt engineering and counterfactual analysis on a high -dimensional loan \\ndataset. \\n‚Ä¢ Preprocessed and filtered large-scale loan data by selecting relevant attributes, handling racial demographics, and \\ngenerating counterfactual scenarios to measure racial disparities. \\n‚Ä¢ Applied fairness metrics and interpretability techniques to evaluate model outputs, achieving a reduction in racial',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_26'}},\n",
       " {'text': \"generating counterfactual scenarios to measure racial disparities. \\n‚Ä¢ Applied fairness metrics and interpretability techniques to evaluate model outputs, achieving a reduction in racial \\nbias for fairer, data-driven decision-making in financial applications. \\nResearch Project \\nCeRAI \\n(Dec‚Äô23 ‚Äì Jan‚Äô24) \\nSurvey of Open Source Explainability Toolkits for Fraud Detection in the Finance Sector  \\nGuide: Prof Balaraman Ravindran(IIT Madras) \\n‚Ä¢ Evaluated toolkits like Microsoft Fairlearn, Google's TensorFlow Model Moderation, TensorFlow RAI Toolkit, \\nand Fiddler and Assessed their features, applications, and use cases in Fraud Detection. \\n‚Ä¢ Examined the integration of Responsible AI principles within these toolkits. \\n‚Ä¢ Conducted a comparative analysis and provided recommendations for toolkit adoption based on specific \\nrequirements and organizational goals in Finance. \\nPERSONAL PROJECTS \\nLangChain RAG-\\nBased \\nConversational \\nSystem\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_27'}},\n",
       " {'text': 'requirements and organizational goals in Finance. \\nPERSONAL PROJECTS \\nLangChain RAG-\\nBased \\nConversational \\nSystem \\n‚Ä¢ Developed a Retrieval-Augmented Generation (RAG) chatbot using LangChain, integrating LLMs, structured output, \\nprompt engineering, and similarity search. \\n‚Ä¢ Implemented document processing pipelines for loading, splitting, and embedding data using OpenAI and \\nSentenceTransformer models. \\n‚Ä¢ Created a history-aware RAG chain and SQLite-based logging system for multi-user session management, ensuring \\nefficient and context-aware responses. \\nDNNs Based \\nRecognition of \\nPlant Disease by \\nLeaf Image \\nClassification \\n‚Ä¢ Dataset consisted of four classes: Healthy Leaf, Scab Disease, Rust, and Multiple Diseases. \\n‚Ä¢ Conducted data augmentation to enhance model generalization, and making it more robust in nature and   \\n   preprocessing techniques like Canny Edge Detection to make bounding boxes.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_28'}},\n",
       " {'text': '‚Ä¢ Conducted data augmentation to enhance model generalization, and making it more robust in nature and   \\n   preprocessing techniques like Canny Edge Detection to make bounding boxes. \\n‚Ä¢ Used the Inception v4 model for image classification of plant diseases, classifying them in 4 distinct classes. \\n‚Ä¢ Evaluated model performance using key metrics like accuracy, precision, and recall. \\nNetflix Movie \\nRecommendation \\nSystem \\n‚Ä¢ Integrated data from multiple CSV files containing 10crore+ user data and 17K+ movies into a single Data Frame. \\n‚Ä¢ Applied EDA on Time-series, imputed missing values, and preprocessed data to improve the model‚Äôs accuracy . \\n‚Ä¢ Evaluated models for recommendation using Item-Item similarity, Matrix-Factorization & Collaborative Filtering. \\n‚Ä¢ Implemented RMSE for model evaluation and improved the base score after post-processing and model stacking. \\n‚Ä¢ Achieved RMSE score 1.06 and MAPE score of 33.37 for prediction of the user rating on test data.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_29'}},\n",
       " {'text': '‚Ä¢ Achieved RMSE score 1.06 and MAPE score of 33.37 for prediction of the user rating on test data. \\nCERTIFICATIONS/SKILLS \\nIntroduction to  \\nTrading, Machine  \\nLearning & GCP  \\n(Coursera) \\n‚Ä¢  Learned the fundamentals of trading, including trends, returns, stop-loss, and volatility.  \\n‚Ä¢ Defined quantitative trading and its main strategies.  \\n‚Ä¢ Understood the basic steps in exchange, statistical, and index arbitrage.  \\n‚Ä¢ Explored the application of machine learning to financial use cases. \\nData Modeling in \\nPower BI (By \\nMicrosoft, \\nCoursera) \\n‚Ä¢ Learned to use Power BI to connect to data sources and transform them into insights. \\n‚Ä¢ Prepared Excel data for analysis in Power BI using common formulas and functions. \\n‚Ä¢ Gained skills in Power BI for creating compelling reports and dashboards. \\n‚Ä¢ Demonstrated skills with a capstone project and prepared for the Microsoft PL-300 Certification exam.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_30'}},\n",
       " {'text': '‚Ä¢ Gained skills in Power BI for creating compelling reports and dashboards. \\n‚Ä¢ Demonstrated skills with a capstone project and prepared for the Microsoft PL-300 Certification exam. \\nSoftware Skills Python(Numpy, Pandas, SkLearn), XGBoost, PyTorch, Hugging Face, MLFlow, Power BI',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anubhav_Gahlot.pdf',\n",
       "   'chunk_id': 'CV_chunk_31'}},\n",
       " {'text': 'ANURAG PANDEY \\n\\uf02d Delhi, India \\uf028+91 9953934711  \\uf02a anuragpandey2489@gmail.com \\n \\nSummary \\n \\nVice President equipped with extensive experience in Machine Learning and Big Data Engineering.  \\nEmploys excellent leadership skills and multi-tasking strengths.  \\n11 years of Total Industry Experience \\n \\nEducation \\n \\nCourse Period Institution/University Percentage \\nMTech in Data Science 2019-2022 IIT Hyderabad Pursuing \\nAdvanced Programme in Data \\nSciences 2017-18 IIM Calcutta Grade \\nExcellent \\nBachelor of Engineering: \\nElectronics & Communication 2006- 2010 Panjab University, Chandigarh 73.1 \\n \\nTechnical Skills \\n \\nMachine \\nLearning and \\nFrameworks \\nMachine Learning Foundations, XGBoost, \\nPython ‚Äì scikit-learn, Spark ML lib, Deep \\nLearning for Computer Vision/NLP, \\nPyTorch \\nProgramming Java, Python, Scala \\n \\nBig Data \\nSpark, Hive, MapReduce, NoSQL, Kafka Cloud \\nTechnologies \\nAWS Sagemaker ‚Äì Processing, Training, \\nPipeline, AWS Glue, Snowflake  \\n \\nCurrently Working As',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_32'}},\n",
       " {'text': 'Programming Java, Python, Scala \\n \\nBig Data \\nSpark, Hive, MapReduce, NoSQL, Kafka Cloud \\nTechnologies \\nAWS Sagemaker ‚Äì Processing, Training, \\nPipeline, AWS Glue, Snowflake  \\n \\nCurrently Working As \\n \\nVice President at Morgan Stanley       May 2021 ‚Äì Present \\n \\nProject Genome \\n‚ñ™ Working with E-Trade team to create and onboard an Idea/Offer level Recommendation System using AWS as the underlying \\nplatform  \\no The use case generates campaign offers/ideas recommendations for ~8M customers based on 156+ Targeting and \\nSuppression Rules for various channels like Email, Mobile and Web \\no It used the SOTA Two Tower Deep Learning Model to generate recommendations \\no It also employs effective A/B testing strategy to measure the lift these recommendations would bring into the system \\n‚ñ™ Writing the Training, Inference and Explainability pipelines in AWS Sagemaker \\n‚ñ™ Writing Eligibility Calculation logic based on the defined Targeting and Suppression Rules using AWS Glue and Snowflake SQL',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_33'}},\n",
       " {'text': '‚ñ™ Writing Eligibility Calculation logic based on the defined Targeting and Suppression Rules using AWS Glue and Snowflake SQL \\n‚ñ™ Optimizing data processing using Snowflake, AWS Glue and Sagemaker Processing \\n‚ñ™ Helping team in standardizing the AI/ML best practices across the entire Model Development LifeCycle \\n \\nProfessional Achievements \\n \\n‚ñ™ President‚Äôs Award winner at American Express ‚Äì for conceptualizing various scalable solutions to everyday problems of Data \\nHandling and Model Scoring within Amex \\n‚ñ™ Consistently shown great results in Colleague Experience Survey ‚Äì a 360\\uf0b0 feedback on Employee Satisfaction and Experience \\nwithin the team and organization \\n \\nKey Academic Projects \\n \\n‚ñ™ Key Courses in MTech : Linear Algebra, Probability, Machine Learning Fundamentals ‚Äì Empirical Risk Minimization, \\nGenerative, Discriminative & Deterministic Learning, Parametric/Non-Parametric Learning, Kernel Methods, Mixture Models,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_34'}},\n",
       " {'text': 'Generative, Discriminative & Deterministic Learning, Parametric/Non-Parametric Learning, Kernel Methods, Mixture Models, \\nRecommender Systems, Scalable Algorithms like Bloom Filters, LSH(Locality Sensitive Hashing), Count Min Sketches and many \\nmore exciting courses like Deep Learning, NLP etc. \\n‚ñ™ Have started my Research project under Prof Vineeth Balasubramanian on studying and implementing Zero Shot Learning \\nusing Different types of GANs and Diffusion techniques  \\n‚ñ™ Implement Decision Trees and Random Forests from scratch to understand the algorithm and pruning techniques in a detailed \\nmanner \\n‚ñ™ Implement Kernelized K-Means using RBF Kernel to cluster news articles.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_35'}},\n",
       " {'text': '[Type the company name] | Page \\nPrevious Experience (Recent 5 years) \\n \\nSenior Manager at American Express       Sep 2018 ‚Äì Apr 2021 \\n \\nEnterprise Machine Learning Platform \\n‚ñ™ Built a self-serve/assisted/custom Machine Learning model platform which can utilize Batch feeds and score/auto-retrain ML \\nModels using state of the art algorithms like XGBoost, LightGBM, Deep Neural Nets etc \\n‚ñ™ Architected the solutions to Key Decisioning Models like ‚Äì Consumer Response Models, Merchant Recommendation Models \\netc \\n‚ñ™ Owner of the product roadmap - defining long term and short term goals; keeping the product vision alive \\n‚ñ™ Lead the optimization workstream - needed to scale up the execution of different algorithms on big data. \\n‚ñ™ People Leader: Lead a team of 11 amazing people \\n \\n \\n \\nSenior Technical Lead at Absolutdata Analytics      Jun 2017 ‚Äì Sep 2018 \\n \\n \\nProject Chatbot Hubot ‚Ä¢ Developed a question answer based chatbot to cater redundant queries related to \\nADT Policies and regulations',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_36'}},\n",
       " {'text': 'Project Chatbot Hubot ‚Ä¢ Developed a question answer based chatbot to cater redundant queries related to \\nADT Policies and regulations \\n‚Ä¢ Built a generic framework using Python, MySQL and Google Dialogflow API and  \\nintegrated it with Tableau which could answer queries related to the dashboard. \\nRole Solution Architect \\n \\nProject MTN ‚Ä¢ Lead the onshore team to build the complete ETL framework using Shell Scripting, \\nSpark and Hive \\n‚Ä¢ Developed Customer Segmentation, Churn Model and Deduplication use cases in \\nPySpark and Knime \\n‚Ä¢ Did Performance Tuning of Hive & Spark jobs and created an optimal Queue \\nallocation strategy within a group of users \\nRole Big Data and ML \\nEngineer \\n \\n \\nSenior Associate at Sapient Global Markets      Dec 2016 ‚Äì May 2017 \\n \\nProject HSBC ‚Ä¢ Analyzed different trade risk data from multiple systems and buil t a uniform Risk \\nRepository on which further analysis processes could be run.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_37'}},\n",
       " {'text': 'Project HSBC ‚Ä¢ Analyzed different trade risk data from multiple systems and buil t a uniform Risk \\nRepository on which further analysis processes could be run. \\n‚Ä¢ Built an Activity and Exception Management framework using Kafka Elasticsearch \\nand Kibana to monitor various activities in the end to end process \\n \\nRole Big Data Developer \\n \\n \\n \\n \\nPersonal Information \\n \\nName  :  Anurag Pandey \\nGender  :  Male \\nDOB  :  24 March 1989 \\nLingual Skills :  English, Hindi \\nHobbies  :  Cooking, Travelling, Watching Movies \\nMarital Status :  Married \\nLinkedIn  :  https://www.linkedin.com/in/anurag-pandey-7b009b52',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Anurag_Pandey_Resume2022 (1).pdf',\n",
       "   'chunk_id': 'CV_chunk_38'}},\n",
       " {'text': 'Ashutosh Nayak\\nDavis, CA, 95616\\n\\uf10b (+1) 765 409 6516 | \\uf0e0 ashnayak@ucdavis.edu | \\uf092 ashutoshnayakIE | \\uf08c ashutoshnayakkgp\\nEducation\\nPurdueUniversity WestLafayette,Indiana\\nPHD,INDUSTRiALENGiNEERiNG(OPTiMiZATiONANDSiMULATiON) August2014‚ÄëDec2018\\n‚Ä¢ PhDThesis: DynamicLoadSchedulinginMicrogridswithManufacturingFacilityforEnergyEfficiency\\nIndianInstituteofTechnologyKharagpur India\\nB.TECH+M.TECH,INDUSTRiALENGiNEERiNGANDMANAGEMENT August2009‚ÄëJune2014\\n‚Ä¢ BachelorsThesis: ImprovingAutomotiveserviceforFieldFailureUsingRootCauseAnalysis\\n‚Ä¢ MastersThesis: OptimalOilVesselSchedulingalongIndianCoastlineforMinimizingTransportationCost\\nWorkExperience\\nUniversityofCalifornia,Davis Davis,CA\\nPOSTDOC,GRADUATESCHOOLOFMANAGEMENT(OR+ML) Jan2019‚Äëcurrent\\n‚Ä¢ CausalAnalysisontheeffectoflocalization(newlanguage)inamobilee‚ÄëcommerceappinIndiausingarandomizedfield\\nexperiment‚Äëwhycustomersbuyanduninstallmorewithnewlanguagefeature? UnderReview: IJRM',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_39'}},\n",
       " {'text': '‚Ä¢ CausalAnalysisontheeffectoflocalization(newlanguage)inamobilee‚ÄëcommerceappinIndiausingarandomizedfield\\nexperiment‚Äëwhycustomersbuyanduninstallmorewithnewlanguagefeature? UnderReview: IJRM\\n‚Ä¢ OngoingAnalysingthechangesinUserBehavior(purchaseandretention)inusersaftertheintroductionofVoicefeaturein\\nmobilee‚Äëcommerceapps. InpreparationtosubmittoJournalofMarketing\\n‚Ä¢ OngoingUsing sequential models (HMM and RNN Autoencoders) to understand user behavior in mobile apps to improve\\ncustomertargetingforincreasingcustomerretentionandengagement.\\n‚Ä¢ DatadrivenapproachtomodelthespreadofCOVID‚Äë19inGermanytounderstandtheimpactofNon‚ÄëpharmaceuticalInter‚Äë\\nventions: InsightsandLimitationsofNPIpolicies. Published: NatureScientificReports.\\n‚Ä¢ Isolating the effect of Mask‚Äëwearing during COVID‚Äë19 using reduced form econometric model derived from Susceptible‚Äë\\nInfectious‚ÄëRecoveredepidemiologymodel. UnderReview: PLoSGlobalHealth .\\nIndianaAdvancedManufacturingCenter WestLafayette,IN',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_40'}},\n",
       " {'text': 'Infectious‚ÄëRecoveredepidemiologymodel. UnderReview: PLoSGlobalHealth .\\nIndianaAdvancedManufacturingCenter WestLafayette,IN\\nRESEARCHASSiSTANTANDCONSULTANT Apr. 2017‚ÄëDec2018\\n‚Ä¢ Batesville Products:Developed a multi‚Äëobjective flexible machine shop module in JAVA for scheduling orders. It is cur‚Äë\\nrently deployed and used by the company for automation of scheduling orders in the shop floor based on objective func‚Äë\\ntions\\n‚Ä¢ PulmodyneDeveloped a JAVA module for Forecasting future demand and suggest inventory policy based on production\\nandpurchasecapacity. (JointworkwithZekunLiu)\\n‚Ä¢ EvonicsDeveloped a genetic algorithm based fermentation scheduling module in MATLAB aimed at minimizing the total\\nelectricitycost. Theprojectcouldsave $385,000annuallyandiscurrentlydeployedandalsousedforcapacityplanning\\nGeneralElectric WestLafayette,IN\\nRESEARCHASSiSTANT May2015‚ÄëMay2017\\n‚Ä¢ Developedaninner‚Äëouterloop discreteeventsimulationframeworkinAnyLogic foridentifyingthebottlenecksinman‚Äë',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_41'}},\n",
       " {'text': 'GeneralElectric WestLafayette,IN\\nRESEARCHASSiSTANT May2015‚ÄëMay2017\\n‚Ä¢ Developedaninner‚Äëouterloop discreteeventsimulationframeworkinAnyLogic foridentifyingthebottlenecksinman‚Äë\\nufacturinganddynamicallyreschedulingtheresourcestoavoidshiftingbottlenecks.\\n‚Ä¢ Developed optimization for capacity planningin terms of number of resources (workforce and machines) required to\\nachievethroughputbasedoninputfromtheinnerloopofscheduling.\\n‚Ä¢ Dataanalyticstoidentifybottlenecks indifferentmanufacturingfacilitiesofGE.Bottleneckswereidentifiedbasedonthe\\noutputmanipulationsfromdifferentsimulation.\\nGeneralElectric,GlobalResearchCenter Niskayuna,NY\\nOPTiMiZATiONSUMMERINTERN May2016‚ÄëAugust2016\\n‚Ä¢ Developeddiscreteeventsimulationmodelforanaircraftmanufacturingfacilityto detectthebottlenecks\\n‚Ä¢ Themodelshowedthatthereal‚Äëtimejobreschedulingandresourceallocationcould reduceoperationcostby17%\\n‚Ä¢ Themodelcouldachievethetargetthroughputof2000parts/dayevenby reducingworkcentersby40%\\nAPRiL12,2022 ASHUTOSHNAYAK ¬∑ R√âSUM√â 1',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_42'}},\n",
       " {'text': 'Interest\\nRESEARCHINTEREST\\n‚Ä¢ Dynamicdecisionsinintelligentsystemsusinghistoricalinformation,currentstateandfutureexpectations\\n‚Ä¢ IntegratingOptimizationwithReinforcementLearning: usingoptimizationtogetoptimalvaluesafter t,anduseReinforce‚Äë\\nmentLearningtolearnthebestpolicy\\n‚Ä¢ Modelinguserbehaviorforimprovingcustomerexperiencethroughtargetingoptimization\\nPEDAGOGY\\n‚Ä¢ InstructorforAnalyticalDecisionMaking(BAX443)forMastersofScienceinBusinessAnalyticsstudents(96students). The\\ncoursemainlycoversoptimization‚ÄëLinearProgramming,IntegerProgrammingandConvexoptimization.\\n‚Ä¢ Jupyternotebooksontopicsaseasyexamplesforunderstandaconcept: github.com/ashutoshnayakIE/tutorials‚Äëon‚Äëjupyter\\n‚Ä¢ Writemediumondifferenttopicsthatarenotcoveredinclasses: github.com/ashutoshnayakIE/Data‚ÄëScience‚ÄëTutorials\\nRelevantCourses\\nINDUSTRiALENGiNEERiNG\\n‚Ä¢ OperationsResearch,DynamicProgramming,Simulation,MonteCarloMethods,StochasticNetworksOptimization,Evolu‚Äë',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_43'}},\n",
       " {'text': 'RelevantCourses\\nINDUSTRiALENGiNEERiNG\\n‚Ä¢ OperationsResearch,DynamicProgramming,Simulation,MonteCarloMethods,StochasticNetworksOptimization,Evolu‚Äë\\ntionaryComputing,SupplychainOptimization,ProductionPlanningandControl,GameTheory,ReinforcementLearning\\nSTATiSTiCS\\n‚Ä¢ Linear Regression, Time Series Modeling, Stochastic Optimization, Design of Experiments, Bayesian Statistics, Stochastic\\nMethods,Multi‚ÄëvariateAnalysis,DesignofExperiments,ProbabilityandStatistics.\\nTechnicalSkills\\nSOFTWARES\\n‚Ä¢ (Optimization)GUROBI,CPLEX| (Simulation)AnyLogic,ARENA| (DataVisualization) Tableau,D3JSforWebApps\\nCOMPUTERSKiLLS\\n‚Ä¢ (Languages)Python,MATLAB,JAVA,R,Julia| (Database)ApacheSpark,SQL| (MachineLearning) TensorFlow,PyTorch\\nExtra‚Äëcurricular\\nMentorshipExperience Davis,CA\\nMSBAFACULTYMENTOR Jan2020‚Äëpresent\\n‚Ä¢ Working as a faculty mentor for MSBA students for their practicum projects with different companies. Practicum project\\nservesaspracticalexperienceofworkingwithrealcompanyprojectsforthestudents.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_44'}},\n",
       " {'text': '‚Ä¢ Working as a faculty mentor for MSBA students for their practicum projects with different companies. Practicum project\\nservesaspracticalexperienceofworkingwithrealcompanyprojectsforthestudents.\\n‚Ä¢ MentorMSBAstudentswithdifferentcasestudycompetitions,speciallyforprojectsinvolvingoptimizationandsimulation.\\nLeadershipExperience WestLafayette,IN\\nPRESiDENT,INFORMSCHAPTERPURDUE August2016‚ÄëApril2017\\n‚Ä¢ Leadateamof8memberstoorganizedifferenteventsincludingresearchtalksandsocialevents\\n‚Ä¢ SuccessfullyinvitedspeakersfromIndustriesandfirstpresidenttosecurefundingfromGraduateStudentAssociation\\n‚Ä¢ ThechaptergotaSumma‚Äëcum‚ÄëlaudeawardinINFORMSAnnualmeetingatHouston2017.\\nAwards\\n2017 BlosserEnvironmentTravelGrant ,GraduateSchool,Purdue WestLafayette,IN\\n2017 INFORMSTravelGrant ,IndustrialProfessionalColloquium LasVegas,U.S.A\\n2014 SilverMedal,forAcademicExcellenceandDepartmentRank01/48 IITKharagpur\\n2013 ProficiencyAward,forbestBachelor‚ÄôsProject IITKharagpur',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_45'}},\n",
       " {'text': '2014 SilverMedal,forAcademicExcellenceandDepartmentRank01/48 IITKharagpur\\n2013 ProficiencyAward,forbestBachelor‚ÄôsProject IITKharagpur\\n2013 J.C.GhoshMemorialAward ,forAcademicExcellence IITKharagpur\\nAPRiL12,2022 ASHUTOSHNAYAK ¬∑ R√âSUM√â 2',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ashutosh_Nayak_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_46'}},\n",
       " {'text': 'ASTHA KINRA\\nkinra@usc.edu |Los Angeles, California |+1-2135744086 |linkedin.com/in/astha-kinra |github.com/astha200\\nEDUCATION\\nUniversity of Southern California Los Angeles, California\\nM.S Computer Science; CGPA: 3.67/4.0 Aug 2023 - May 2025\\nCoursework: Analysis of Algorithms, Database Systems, Deep Learning and its Applications, Machine Learning for Data Science, Applied\\nNatural Language Processing, Information Retrieval and Web Search Engines, Affective Computing\\nThapar Institute of Engineering and Technology (T .I.E.T) Patiala, Punjab\\nB.E Electronics and Computer Engineering; CGPA: 9.54/10.0 Aug 2017 - Jun 2021\\nTECHNICAL SKILLS\\n‚Ä¢ Languages: Python, C, C++, SQL, Kotlin, Java, HTML, CSS, JavaScript, TypeScript\\n‚Ä¢ Tools/ Technologies: Git, GraphQL, REST APIs, GenAI, AWS, Docker, Jenkins, Postman, JIRA, Google Analytics, Tableau\\n‚Ä¢ Libraries/ Frameworks: PySpark, Hadoop, React, SpringBoot, Numpy , Pandas, Keras, OpenCV , PyTorch, Sklearn, NLTK, JUnit\\nWORK EXPERIENCE',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS.pdf',\n",
       "   'chunk_id': 'CV_chunk_47'}},\n",
       " {'text': '‚Ä¢ Libraries/ Frameworks: PySpark, Hadoop, React, SpringBoot, Numpy , Pandas, Keras, OpenCV , PyTorch, Sklearn, NLTK, JUnit\\nWORK EXPERIENCE\\nAmazon, Bellevue, WA | Software Development Engineer Intern May 2024 - Aug 2024\\n‚ó¶ Designed and developed a responsive analytics dashboard using ReactJS, AWS, and Amazon Meridian, enabling\\noperations managers to optimize workflows and monitor facility performance.\\n‚ó¶ Collaborated with stakeholders to define key performance indicators (KPIs), contributed to data transformation and\\npreprocessing, and delivered actionable insights through visualizations and alerts.\\n‚ó¶ Deployed frontend and backend services on AWS using CDK, CI/CD pipelines, and Route 53 via SuperNova.\\nExpedia Group, Haryana | Software Development Engineer Oct 2021 - Mar 2023\\n‚ó¶ Developed web features like ‚ÄôAir Member Only Deal‚Äô (discounted flights for Expedia members) and ‚ÄôPaid bags for',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS.pdf',\n",
       "   'chunk_id': 'CV_chunk_48'}},\n",
       " {'text': 'Expedia Group, Haryana | Software Development Engineer Oct 2021 - Mar 2023\\n‚ó¶ Developed web features like ‚ÄôAir Member Only Deal‚Äô (discounted flights for Expedia members) and ‚ÄôPaid bags for\\nmultiple passengers‚Äô using Kotlin, React.js, GraphQL, Cassandra, Elasticsearch, MongoDB, Redis cache and AWS.\\n‚ó¶ Conducted A/B tests and analyzed results to drive personalized fare recommendations and improve customer engagement\\n‚ó¶ Developed real-time analytics dashboard, delivering insights into customer behavior and feature adoption trends.\\nAmerican Express, Haryana | Analyst - Product Management, Intern Jan 2021 - Oct 2021\\n‚ó¶ Developed enterprise-wide data quality solutions, including an AI/ML-powered Anomaly Detection Framework,\\nenabling business users to validate data and generate alerts.\\n‚ó¶ Partnered with stakeholders to define product specifications and integrate feedback, ensuring alignment on business and\\ntechnical requirements.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS.pdf',\n",
       "   'chunk_id': 'CV_chunk_49'}},\n",
       " {'text': '‚ó¶ Partnered with stakeholders to define product specifications and integrate feedback, ensuring alignment on business and\\ntechnical requirements.\\n‚ó¶ Automated dataset creation for time series modeling (EWMA) and visualized key data trends using Python, improving\\ndata processing efficiency by 50% and enabling proactive decision-making.\\n‚ó¶ Built a Python + PySpark SQL application with a Tkinter interface that compares data elements providing stats (match,\\nmismatch, nulls, swap-in/out), reducing query execution time by 50% and improving usability for non-SQL users.\\nTheSmartBridge, Telangana | Machine Learning Remote Summer Intern Jun 2020 - Jul 2020\\n‚ó¶ Developed a real-time WebApp integrating a Random Forest regression model to predict country life expectancy with\\n96% accuracy, based on factors like BMI, GDP , alcohol intake, and HIV/AIDS rates.\\n‚ó¶ Deployed the model using IBM Watson Machine Learning IBM Watson Studio, enabling scalable real-time predictions.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS.pdf',\n",
       "   'chunk_id': 'CV_chunk_50'}},\n",
       " {'text': '96% accuracy, based on factors like BMI, GDP , alcohol intake, and HIV/AIDS rates.\\n‚ó¶ Deployed the model using IBM Watson Machine Learning IBM Watson Studio, enabling scalable real-time predictions.\\n‚ó¶ Designed an interactive web interface using IBM Cloud Node-Red for real-time predictions.\\nACADEMIC AND RELEVANT PROJECTS\\nBreast Cancer Detection | Python, Keras, TensorFlow, Numpy, Pandas, OpenCV , sklearn, matplotlib\\n‚ó¶ Designed a Convolutional Neural Network (CNN) model to detect tumors in mammographically captured images with\\n97% classification accuracy, employing data augmentation and image processing techniques to enhance performance.\\nCode Narrate: Matching comments to Python code using LLMs | Python, Pytorch, Sklearn, NLTK, LLMs\\n‚ó¶ Utilized transformer-based models like CodeBERT to match comments with Python code snippets, achieving the highest\\nKendall Tau score of 0.84, enhancing code readability and maintainability .',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS.pdf',\n",
       "   'chunk_id': 'CV_chunk_51'}},\n",
       " {'text': '‚ó¶ Utilized transformer-based models like CodeBERT to match comments with Python code snippets, achieving the highest\\nKendall Tau score of 0.84, enhancing code readability and maintainability .\\nInsightAgent: Autonomous Insight Generator | Python, Pandas, Matplotlib, Seaborn, OpenAI API, FAISS, LangChain, FPDF\\n‚ó¶ Developed an agentic AI system that autonomously analyzes structured datasets, detects trends and anomalies, and\\ngenerates natural language summaries with visual charts and PDF reports using RAG for context-enhanced insights.\\nACHIEVEMENTS AND AWARDS\\n‚Ä¢ Received Merit scholarship for being among top 10% students of ENC branch in T .I.E.T .\\n‚Ä¢ Awarded Best Women‚Äôs Teamat Makeathon‚Äô19- Inter College Hackathon organised in T .I.E.T by Microsoft Student Chapter.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS.pdf',\n",
       "   'chunk_id': 'CV_chunk_52'}},\n",
       " {'text': 'ASTHA KINRA\\nkinra@usc.edu |Southfield, Michigan |+1-2135744086 |linkedin.com/in/astha-kinra |github.com/astha200\\nEDUCATION\\nUniversity of Southern California Los Angeles, California\\nM.S Computer Science; CGPA: 3.67/4.0 Aug 2023 - May 2025\\nCoursework: Analysis of Algorithms, Database Systems, Deep Learning and its Applications, Machine Learning for Data Science, Applied\\nNatural Language Processing, Information Retrieval and Web Search Engines, Affective Computing\\nThapar Institute of Engineering and Technology (T .I.E.T) Patiala, Punjab\\nB.E Electronics and Computer Engineering; CGPA: 9.54/10.0 (Merit scholar) Aug 2017 - Jun 2021\\nTECHNICAL SKILLS\\n‚Ä¢ Languages: Python, C, C++, SQL, Kotlin, Java, HTML, CSS, JavaScript, TypeScript\\n‚Ä¢ Tools/ Technologies: Git, GraphQL, REST APIs, GenAI, AWS, Docker, Jenkins, JIRA, ETL, Hive, Big data, Tableau, Ollama\\n‚Ä¢ Libraries/ Frameworks: PySpark, Hadoop, React, SpringBoot, Hugging Face, Keras, OpenCV , PyTorch, Sklearn, NLTK, JUnit\\nWORK EXPERIENCE',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS_2.pdf',\n",
       "   'chunk_id': 'CV_chunk_53'}},\n",
       " {'text': '‚Ä¢ Libraries/ Frameworks: PySpark, Hadoop, React, SpringBoot, Hugging Face, Keras, OpenCV , PyTorch, Sklearn, NLTK, JUnit\\nWORK EXPERIENCE\\nLucid Motors, Southfield, MI | Software Engineer , Automation Sept 2025 - Present\\n‚ó¶ Developed Python-based automation scripts for CI/CD pipelines that collect, analyze, and visualize build and test metrics,\\nleveraging data analysis to identify bottlenecks and optimize deployment workflows\\nAmazon, Bellevue, WA | Software Development Engineer Intern May 2024 - Aug 2024\\n‚ó¶ Designed and developed a responsive analytics dashboard using ReactJS, AWS, and Amazon Meridian, enabling\\noperations managers to optimize workflows and monitor facility performance.\\n‚ó¶ Collaborated with stakeholders to define key performance indicators (KPIs), contributed to data transformation and\\npreprocessing, and delivered actionable insights through visualizations and alerts.\\n‚ó¶ Deployed frontend and backend services on AWS using CDK, CI/CD pipelines, and Route 53 via SuperNova.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS_2.pdf',\n",
       "   'chunk_id': 'CV_chunk_54'}},\n",
       " {'text': 'preprocessing, and delivered actionable insights through visualizations and alerts.\\n‚ó¶ Deployed frontend and backend services on AWS using CDK, CI/CD pipelines, and Route 53 via SuperNova.\\nExpedia Group, Haryana | Software Development Engineer Oct 2021 - Mar 2023\\n‚ó¶ Developed web features like ‚ÄôAir Member Only Deal‚Äô (discounted flights for Expedia members) and ‚ÄôPaid bags for\\nmultiple passengers‚Äô using Kotlin, React.js, GraphQL, Cassandra, Elasticsearch, MongoDB, Redis cache and AWS.\\n‚ó¶ Conducted A/B tests and analyzed results to drive personalized fare recommendations and improve customer engagement\\n‚ó¶ Developed real-time analytics dashboard, delivering insights into customer behavior and feature adoption trends.\\nAmerican Express, Haryana | Analyst - Product Management, Intern Jan 2021 - Oct 2021\\n‚ó¶ Led the development of enterprise-wide data quality solutions, including an AI/ML-powered Anomaly Detection\\nFramework, enabling business users to validate data and generate alerts.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS_2.pdf',\n",
       "   'chunk_id': 'CV_chunk_55'}},\n",
       " {'text': '‚ó¶ Led the development of enterprise-wide data quality solutions, including an AI/ML-powered Anomaly Detection\\nFramework, enabling business users to validate data and generate alerts.\\n‚ó¶ Partnered with stakeholders to define product specifications and integrate feedback, ensuring alignment on business and\\ntechnical requirements.\\n‚ó¶ Automated dataset creation for time series modeling (EWMA) and visualized key data trends using Python and Excel,\\nimproving data processing efficiency by 50% and enabling proactive decision-making.\\n‚ó¶ Built a Python + PySpark SQL application with a Tkinter interface that compares data elements providing stats (match,\\nmismatch, nulls, swap-in/out), reducing query execution time by 50% and improving usability for non-SQL users.\\nTheSmartBridge, Telangana | Machine Learning Remote Summer Intern Jun 2020 - Jul 2020\\n‚ó¶ Developed a real-time WebApp integrating a Random Forest regression model to predict country life expectancy with',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS_2.pdf',\n",
       "   'chunk_id': 'CV_chunk_56'}},\n",
       " {'text': '‚ó¶ Developed a real-time WebApp integrating a Random Forest regression model to predict country life expectancy with\\n96% accuracy, based on factors like BMI, GDP , alcohol intake, and HIV/AIDS rates.\\n‚ó¶ Deployed the model using IBM Watson Machine Learning & IBM Watson Studio, enabling scalable real-time predictions.\\n‚ó¶ Designed an interactive web interface using IBM Cloud Node-Red for real-time predictions.\\nACADEMIC AND RELEVANT PROJECTS\\nInsightAgent: Autonomous Insight Generator | Python, OpenAI API, FAISS, LangGraph, Streamlit, LangChain, Matplotlib, FPDF2\\n‚ó¶ Built a multi-agent AI system that autonomously analyzes structured datasets, detects anomalies, and generates natural\\nlanguage summaries with visual reports using RAG for context-enhanced insights and human-in-the-loop validation.\\nBreast Cancer Detection | Python, Keras, TensorFlow, Numpy, Pandas, OpenCV , sklearn, Matplotlib, Seaborn',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS_2.pdf',\n",
       "   'chunk_id': 'CV_chunk_57'}},\n",
       " {'text': 'Breast Cancer Detection | Python, Keras, TensorFlow, Numpy, Pandas, OpenCV , sklearn, Matplotlib, Seaborn\\n‚ó¶ Designed a Convolutional Neural Network (CNN) model to detect tumors in mammographically captured images with\\n97% classification accuracy, employing data augmentation and image processing techniques to enhance performance.\\nCode Narrate: Matching comments to Python code using LLMs | Python, Pytorch, Sklearn, NLTK, LLMs\\n‚ó¶ Utilized transformer-based models like CodeBERT to match comments with Python code snippets, achieving the highest\\nKendall Tau score of 0.84, enhancing code readability and maintainability .',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Astha_Kinra_Resume_DS_2.pdf',\n",
       "   'chunk_id': 'CV_chunk_58'}},\n",
       " {'text': 'Sarah  Sohrabi \\nSenior Data Scientist\\nüì≤\\n +31610051129 |  üìß\\n sarahsohrabi91@gmail.com\\nSenior Data Scientist - AI Engineer with extensive experience in Machine Learning, Statistical Modelling \\nand Generative AI. Expert in exploring deep data to find patterns and insights, engineering streamlines \\nto automate reporting and deployment. Proven communication leader to provide tangible business \\nimpact.\\nEducation\\nTop Endorsements\\nCommunication 100%\\nRelated Experience\\n Senior Data Scientist \\n Senior Data Scientist\\nCutting-Edge Tech \\nStack Agility\\nStorytelling and \\nSales Pitch\\nData \\nVisualization\\nAttention to\\nBusiness Impact\\nSkills\\nHuawei/ Jun 2016 - Jun 2018\\nTehran, Iran\\nKlarna/ May 2020 - Apr 2022\\nStockholm, Sweden\\nKPMG/ 2023 Apr - Present\\nAmsterdam, Netherlands\\nReadly/ May 2022 - Mar 2023\\nStockholm, Sweden\\n \\n Senior Data Analyst\\n Data Scientist\\nLeadership 100%\\nEmbracing Feedback 100%\\nResponsibilities\\n‚Ä¢ Developed & deployed end-to-end NLP \\nand LLM-based models (BERT, GPT) to',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\CV_SS_Microsoft.pdf',\n",
       "   'chunk_id': 'CV_chunk_59'}},\n",
       " {'text': 'Stockholm, Sweden\\n \\n Senior Data Analyst\\n Data Scientist\\nLeadership 100%\\nEmbracing Feedback 100%\\nResponsibilities\\n‚Ä¢ Developed & deployed end-to-end NLP \\nand LLM-based models (BERT, GPT) to \\npower client solution\\n‚Üí Business Impact\\n‚Ä¢ Reduced manual support hours, improved user \\nexperience, and increased operational efficiency \\n‚Ä¢ Designed & productionized RAG-based \\nassistants and content generators\\n‚Üí ‚Ä¢ Enabled customer support automation and efficient \\nknowledge retrieval across enterprise data sources\\n‚Ä¢ Led sessions with non-technical clients to \\nidentify GenAI opportunities\\n‚Üí ‚Ä¢ Designed PoCs and accelerated the adoption of AI \\ncopilots and data science applications across teams\\n‚Ä¢ Collaborated with engineers to deploy, \\nmaintain, and optimize models\\n‚Üí ‚Ä¢ Ensured reliable performance, scalability, and \\ncontinuous monitoring over time\\nüõ†\\n Tools: SQL, Python, Docker, Git, GCP, AWS, Copilot Studio, MLÔ¨Çow, genAI/LLM APIs, REST APIs\\nüéØ',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\CV_SS_Microsoft.pdf',\n",
       "   'chunk_id': 'CV_chunk_60'}},\n",
       " {'text': '‚Üí ‚Ä¢ Ensured reliable performance, scalability, and \\ncontinuous monitoring over time\\nüõ†\\n Tools: SQL, Python, Docker, Git, GCP, AWS, Copilot Studio, MLÔ¨Çow, genAI/LLM APIs, REST APIs\\nüéØ\\n Frameworks: ML, DL, NLP, BERT, RAG, LangChain, MLOps, FAISS, CI/CD, Microservices\\nüõ†\\n Tools: SQL, Python, Power BI, DBT, GitLab, AWS, MLÔ¨Çow\\nüéØ\\n KPIs: Churn Ratio, Customer Engagement (DAU/MAU), FPS Conversion, Num Active Users\\nResponsibilities\\n‚Ä¢ Applied statistical analysis on datasets \\n& visualized key insights\\n‚Üí\\nBusiness Impact\\n‚Ä¢ Cut manual work and improved customer \\nengagement and retention tracking\\n‚Ä¢ Developed and deployed CLTV and \\nChurn Models into workflows\\n‚Üí ‚Ä¢ Enabled ROI-based marketing and improved \\ncustomer retention strategies\\n‚Ä¢ Designed and analyzed A/B and causal \\nexperiments across teams\\n‚Üí ‚Ä¢ Revealed optimal UX for sales and customer \\nengagement performance\\n‚Ä¢ Mentored and upskilled junior analysts ‚Üí ‚Ä¢ Increased team productivity and built trust\\nResponsibilities',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\CV_SS_Microsoft.pdf',\n",
       "   'chunk_id': 'CV_chunk_61'}},\n",
       " {'text': '‚Üí ‚Ä¢ Revealed optimal UX for sales and customer \\nengagement performance\\n‚Ä¢ Mentored and upskilled junior analysts ‚Üí ‚Ä¢ Increased team productivity and built trust\\nResponsibilities\\n‚Ä¢ Designed & analyzed experiments \\nacross payment and checkout flows\\n‚Üí\\nBusiness Impact\\n‚Ä¢ Reduced payment defaults and enhanced overall \\ncustomer experience\\n‚Ä¢ Built ML-powered anomaly detection & \\nfraud monitoring models\\n‚Üí ‚Ä¢ Reduced false positives and mitigated compliance \\nrisks\\n‚Ä¢ Forecasted transaction volumes and \\npayment success rate\\n‚Üí ‚Ä¢ Improved prediction accuracy , enabling better \\ncapacity planning and payment reliability\\nüõ†\\n Tools: Python, SQL, Power BI, Git, Google Analytics\\nüéØ\\n KPIs: Churn Ratio, Device Usage, Customer Engagement, Sales,  ROI\\nResponsibilities\\n‚Ä¢ Delivered ad-hoc analyses, dashboards \\nand pipelines \\n‚Üí\\nBusiness Impact\\n‚Ä¢ Uncovered patterns and actionable insights\\n‚Ä¢ Led A/B testing end-to-end ‚Üí ‚Ä¢ Enabled data-driven decisions\\n‚Ä¢ Built and optimized ML models for \\ncustomer segmentation',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\CV_SS_Microsoft.pdf',\n",
       "   'chunk_id': 'CV_chunk_62'}},\n",
       " {'text': 'and pipelines \\n‚Üí\\nBusiness Impact\\n‚Ä¢ Uncovered patterns and actionable insights\\n‚Ä¢ Led A/B testing end-to-end ‚Üí ‚Ä¢ Enabled data-driven decisions\\n‚Ä¢ Built and optimized ML models for \\ncustomer segmentation\\n‚Üí ‚Ä¢ Improved targeting and sales prediction\\nSiemens/May 2013 - Sep 2013\\nTehran, Iran - Berlin, Germany\\nResponsibilities\\nSupply demand matching in the electricity market to decrease the risk of turn-off: forecasting demand \\nand planning supply\\nIntern\\nüõ†\\n Tools: SQL, Python, GCP, Git, Qlik Sense\\nüéØ\\n KPIs: Product Return Rate, Click-Through Rate, Transaction Volume, CL TV/CAC, Accuracy\\nFINANCE & E-COMMERCE\\nMEDIA PLATFORM\\nFINANCE & PRODUCT\\nTECHNOLOGY\\nTECHNOLOGY\\nMSc in Marketing, Umea University\\nBSc in Electrical Engineering & MBA, \\nSharif University of Technology\\nPhD in Business Analytics (paused), SSE',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\CV_SS_Microsoft.pdf',\n",
       "   'chunk_id': 'CV_chunk_63'}},\n",
       " {'text': 'Danish Faisal\\nSoftware Engineer\\n+91 - 9989699691\\ndanishfaisal.wwe@gmail.com\\ndanish-faisal\\ndanish-faisal\\ndanish-faisal\\nWORK EXPERIENCE\\nNCR Corporation\\nTest Engineer (August 2021 - Present)\\nWriting test cases & deriving test scenarios from BRD in early phase of development\\nEnsured that all products met functional & design speci\\x00cations\\nLead the QA group on automation responsibilities\\nDeveloped & executed - test cases & test suites\\nNCR Corporation\\nQA Intern (January 2021 - August 2021)\\nUsed Quality Center for writing test suites & for bug reporting, helped the QA team in defect tracking & bug \\x00xing\\nCompleted Become a Software Tester & Become Test Automation Engineer learning paths on LinkedIn Learning\\nPerformed Smoke, Sanity, Regression & Acceptance testing on multiple projects of Emerald product line\\nTrained on SDLC models, Software Testing & testing techniques\\nIESoftek\\nSoftware Engineer (August 2020 - January 2021)',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Danish_Faisal_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_64'}},\n",
       " {'text': 'Trained on SDLC models, Software Testing & testing techniques\\nIESoftek\\nSoftware Engineer (August 2020 - January 2021)\\nDeveloping web & hybrid mobile applications using AngularJS & Ionic Framework\\nDeveloping APIs using Java - Spring for ScrapQ apps\\nExperience gained in Agile Software delivery\\nGit-\\x00ow work\\x00ow approach utilized\\nIESoftek\\nSoftware Engineer Intern (December 2019 - July 2020)\\nExtensively worked in various phases of SDLC from design & development to testing\\nCode a web app using Vanilla technologies like HTML5, CSS3, JavaScript, Bootstrap\\nBuilt frontend of enterprise-level web applications on MEAN stack\\nWorked on maintenance and enhancements of existing applications\\nPROJECTS\\nExpense Tracker\\nhttps://github.com/danish-faisal/ExpenseTracker_MERN (February 2022 - February 2022)\\nAn app to calculate & track your income & expenses, built on MERN Stack\\nCalculator\\nhttps://github.com/danish-faisal/Simple-React-Calculator (January 2022 - January 2022)',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Danish_Faisal_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_65'}},\n",
       " {'text': 'An app to calculate & track your income & expenses, built on MERN Stack\\nCalculator\\nhttps://github.com/danish-faisal/Simple-React-Calculator (January 2022 - January 2022)\\nA simple calculator application built using React & React Hooks\\nMovieInfo\\nhttps://github.com/danish-faisal/movieinfo (November 2021 - January 2022)\\nAn application to view \\x00lms data from TMDB API with genres \\x00lter, pagination, search & view associated videos functionalities built using HTML, CSS, JavaScript\\nEDUCATION\\nMuffakham Jah College of Engineering and Technology\\nBachelor of Engineering Computer Science & Engineering\\n(August 2016 - September 2020)\\n8.2/10\\nLetter of Recommendation from Research Coordinator, R&D dept for great performance in doing research in his group\\nEnhanced problem-solving and technical skills through math, engineering & software courses\\nBest in Math for two consecutive years',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Danish_Faisal_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_66'}},\n",
       " {'text': 'Enhanced problem-solving and technical skills through math, engineering & software courses\\nBest in Math for two consecutive years\\nRelevant Coursework: C Programming & Problem Solving, Object-Oriented Programming using C++, Data Structures, Java Programming, Database Management\\nSystems, Design & Analysis of Algorithms, Operating Systems, Computer Networks, Web Programming, Computer Organization & Architecture, Software Engineering,\\nSoftware Quality & Testing\\nPROGRAMMING\\nReactJS\\nIntermediate\\nJavaScript\\nPro\\x00cient\\nHTML\\nPro\\x00cient\\nCSS\\nPro\\x00cient\\nReact Hooks\\nIntermediate\\nNodeJS\\nIntermediate\\nExpress\\nIntermediate\\nMongoDB\\nIntermediate\\nPython\\nPro\\x00cient\\nSQL\\nIntermediate\\nADDITIONAL SKILLS\\nGit\\nPro\\x00cient\\nQuality Assurance\\nPro\\x00cient\\nTFS\\nIntermediate\\nHP ALM\\nIntermediate\\nPostman\\nIntermediate\\nJira\\nBeginner\\nACHIEVEMENTS\\nAMCAT Employability Certi\\x00cate\\nAspiring Minds October 2020\\nScored 99% in Computer Science & Logical Ability and a good score in other',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Danish_Faisal_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_67'}},\n",
       " {'text': 'Intermediate\\nPostman\\nIntermediate\\nJira\\nBeginner\\nACHIEVEMENTS\\nAMCAT Employability Certi\\x00cate\\nAspiring Minds October 2020\\nScored 99% in Computer Science & Logical Ability and a good score in other\\nsections strongly recommending for SDE, SDE Trainee, Business Analyst, more\\ntech roles\\nCocubes Pre-Assess\\nCocubes April 2020\\nSecured a high overall score of 77% across Coding, Analytical, Quantitative, etc.\\nsections\\nResearch Paper Publication\\nJETIR June 2019\\nPublished research paper \"Obtaining Med-Info using Optical Character\\nrecognition\", JETIR (June-2019, Vol-6, Issue-6), (ISSN-2349-5162)\\nCERTIFICATIONS\\nBecome a Full-Stack Web Developer\\nLinkedIn January 2022\\nBecome a JavaScript Developer\\nLinkedIn October 2021\\nPostman Student Expert, APIs 101, API Tester, API Adoption & API\\nFirst badges\\nPostman\\nSeptember\\n2021\\nFront-End Web UI Frameworks and Tools: Bootstrap 4\\nCoursera January 2020\\nJavaScript Algorithms & Data Structures\\nfreeCodeCamp March 2019\\nResponsive Web Design\\nfreeCodeCamp February 2019',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Danish_Faisal_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_68'}},\n",
       " {'text': 'Dheeraj Sharma\\n/envel‚å¢pedheerajsharma4280@gmail.com ‚ôÇphone+91 6005672760 /githubGitHub URL /linkedinLinkedIn URL /codeLeetCode URL\\nEducation\\nVisvesvaraya Technological University Karnataka, India\\nPES College of Engineering| B.E. CS&E | Current CGPA: 8.89/10 2022 - 2026\\nCoursework: Operating Systems, Distributed Systems, Data Structures, Algorithms, DBMS, Computer Networks\\nWork Experience\\nCalypso (startup) Dec 2024 ‚Äì July 2025\\nResearch Intern Remote\\n‚Ä¢ Conducted empirical studies on user‚Äìinteraction friction in fragmented information channels, and prototyped\\nmodular interfaces (HTML, CSS, React) to study user cognition on live-traffic systems.\\n‚Ä¢ Performed A/B evaluations on responsive UI components, observing a 40% increase in interaction metrics.\\n‚Ä¢ Designed data-driven heuristics to optimize API request pipelines; 30% reduction in error-based queries.\\n‚Ä¢ Leveraged anonymized usage logs to model bottleneck behavior and derived actionable for iterative UX research.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Dheeraj_Sharma_Research.pdf',\n",
       "   'chunk_id': 'CV_chunk_69'}},\n",
       " {'text': '‚Ä¢ Leveraged anonymized usage logs to model bottleneck behavior and derived actionable for iterative UX research.\\nAmazon HackOn 5 May 2025 - June 2025\\nTop 150 Finalist ‚Äî National Hackathon Karnataka, India\\n‚Ä¢ Ranked among the top 150 out of 50,000+ participants through a nationwide coding and development contest.\\n‚Ä¢ Proposed a Sustainability solution tackling inefficiencies in urban waste management via smart automation.\\n‚Ä¢ Built a scalable, prototype aligned with Amazon‚Äôs innovation pillars, prioritizing feasibility and usability.\\nIndian Institute of Technology, Bhubaneswar June 2023 ‚Äì July 2023\\nAI/ML Research Intern Bhubaneswar, India\\n‚Ä¢ Faced high false-positive duplication detection in Quora datasets; designed and deployed an NLP rule-based\\nengine using NLTK, boosting accuracy by 35%.\\n‚Ä¢ Built scalable data pipelines to automate ML retraining and deployment cycles, reducing operational overhead.\\nProjects\\nTrendAI | Flask, BeautifulSoup, Pandas, NLTK, Google Trends & YouTube API GitHub',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Dheeraj_Sharma_Research.pdf',\n",
       "   'chunk_id': 'CV_chunk_70'}},\n",
       " {'text': 'Projects\\nTrendAI | Flask, BeautifulSoup, Pandas, NLTK, Google Trends & YouTube API GitHub\\n‚Ä¢ Built an end-to-end NLP-powered research prototype that mines and ranks trend signals across YouTube,\\nGoogle Trends, and Twitter. Designed novel relevance heuristics achieving 90% accuracy, demonstrated\\n40% faster detection than baseline sampling, and deployed a modular Flask‚ÄìReact pipeline for real-time\\ntrend forecasting experimentation.\\nYojanaDesk ‚Äì Collaborative Systems Prototype | React, Next.js, MongoDB, WebSockets GitHub\\n‚Ä¢ Developed a real-time coordination system to analyze latency-aware behavior in task orchestration using\\nWebSockets. Engineered role-based access and asynchronous workflows, experimentally observing 40%\\nconcurrency improvement and 25% latency reduction under synthetic loads.\\nInvoice Generator | React, Bootstrap GitHub\\n‚Ä¢ Constructed a lightweight React-based HCI tool to explore automation in structured documentation; integrated',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Dheeraj_Sharma_Research.pdf',\n",
       "   'chunk_id': 'CV_chunk_71'}},\n",
       " {'text': 'Invoice Generator | React, Bootstrap GitHub\\n‚Ä¢ Constructed a lightweight React-based HCI tool to explore automation in structured documentation; integrated\\nmulti-currency support and jsPDF-based export, reducing manual effort by 80% and improving responsiveness by\\n30% after iterative user-centric evaluation across 10+ participants.\\nSkills\\nProgramming & Scripting: Python, C/C++, Java, JavaScript, SQL, Bash\\nResearch Tooling: TensorFlow, NLTK, Statistical Modeling, Unsupervised Learning\\nExperimental Methods: Hypothesis Design, A/B Testing, Ablation Studies, Result Interpretation\\nData & System Infrastructure: Data Mining, Distributed Pipelines, AWS, Git\\nSoft Skills: Technical Writing, Documentation, Presentations, Collaboration, Public Speaking\\nResearch Interests: Applied ML, Trend Forecasting, Large-Scale Systems, Information Retrieval\\nAwards and Achievements\\n‚Ä¢ Completed certified training in feature prioritization and product‚Äìmarket experimentation via LinkedIn.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Dheeraj_Sharma_Research.pdf',\n",
       "   'chunk_id': 'CV_chunk_72'}},\n",
       " {'text': 'Awards and Achievements\\n‚Ä¢ Completed certified training in feature prioritization and product‚Äìmarket experimentation via LinkedIn.\\n‚Ä¢ Solved 410+ DSA problems on LeetCode across arrays, trees, graphs, recursion, competitive programming.\\n‚Ä¢ Commended at IIT Bhubaneswar for designing scalable ML pipelines increased system efficiency by 30%.\\n‚Ä¢ Participated for the national Walmart Sparkathon, showcasing innovation in rapid data-driven prototyping.\\n‚Ä¢ Contributed to open-source research via Google Developer Groups and led tech communities.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Dheeraj_Sharma_Research.pdf',\n",
       "   'chunk_id': 'CV_chunk_73'}},\n",
       " {'text': 'GARIMA BHATIA\\nProduct Management| Data Science | Strategy\\n@ garima.gbhatia@gmail.com /phone+91-8558881968 /map‚ôÇmar¬∂erPatiala, India /linkedinwww.linkedin.com/in/garima-bhatia-8592a2199\\nEXPERIENCE\\nSenior Analyst, AI Labs\\nAmerican Express\\n·Ωå5May 2021 ‚Äì Present /map‚ôÇmar¬∂erGurgaon, Haryana\\n‚Ä¢ Fraud Scanners Framework: Collaborating with data\\nscience teams & engineering partners to solutionize\\na framework that consists of more that 10 scanners\\nwhich identiÔ¨Åes product abusers and take appropri-\\nate actions in real time to stop the fraud which saved\\n50MM$ for the organization.\\n‚Ä¢ RedeÔ¨Åned product analytics for an AI/ML model en-\\nablement platform: DeÔ¨Åned important KPIs & created\\nMIS reports to measure platform performance & iden-\\ntify improvement areas to enhance customer experi-\\nence for both real time and batch platform.\\n‚Ä¢ Fault Tolerance Framework: Led implementation of a\\nMonitoring framework, a capability that aims to mon-\\nitor health & performance metrics of the platform and',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Garima Bhatia - American Express.pdf',\n",
       "   'chunk_id': 'CV_chunk_74'}},\n",
       " {'text': '‚Ä¢ Fault Tolerance Framework: Led implementation of a\\nMonitoring framework, a capability that aims to mon-\\nitor health & performance metrics of the platform and\\nalert in case of unexpected behavior, from conception\\nthrough launch\\nProgrammer Analyst, Enterprise Intelligence -\\nEDA\\nAmerican Express\\n·Ωå5June 2019 ‚Äì May 2021 /map‚ôÇmar¬∂erGurgaon, Haryana\\n‚Ä¢ Worked as a Product Analyst for a AI/ML model de-\\nployment platform that enables analytical users to ex-\\necute batch servicing use cases in real time\\n‚Ä¢ NOVA Chat Bot: Built a classiÔ¨Åcation model using PyS-\\npark, MLlib & logistic regression, to predict the proba-\\nbility of a customer opening the AmEx chat bot for an\\nexploratory project.\\n‚Ä¢ Meticulously performed UAT test cases for the fault\\ntolerance framework of the platform & recommended\\nÔ¨Ånely-tuned changes to the development team in or-\\nder to meet the functional requirements\\n‚Ä¢ Analyzed & IdentiÔ¨Åed top resource consuming machine\\nlearning & data models,& optimized them using PyS-',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Garima Bhatia - American Express.pdf',\n",
       "   'chunk_id': 'CV_chunk_75'}},\n",
       " {'text': 'Ô¨Ånely-tuned changes to the development team in or-\\nder to meet the functional requirements\\n‚Ä¢ Analyzed & IdentiÔ¨Åed top resource consuming machine\\nlearning & data models,& optimized them using PyS-\\npark that led to >80% savings of time and resources\\nProgrammer Analyst - Intern\\nAmerican Express\\n·Ωå5Jan 2018 ‚Äì June 2018 /map‚ôÇmar¬∂erGurgaon, Haryana\\n‚Ä¢ Campaign Migrations from Legacy Platform: Performed\\na feasibility analyses to migrate corporate campaigns\\nof JAPA markets from a legacy platform to a POA sys-\\ntem.\\n‚Ä¢ Automated Campaign execution: Automated the en-\\ntire process of executing campaigns that reduced the\\nturn around time from 48 hours to just 30 mins.\\nEDUCATION\\nB.Tech, Electronics & Communication\\nPunjab Engineering College\\n·Ωå5Aug 2015 - May 2019 ( CGPA: 8.87)\\n‚Ä¢ Co-founder and creative head of Women Empower-\\nment Cell in PEC (2016- 2019).\\n‚Ä¢ Project head for Aabha- an initiative to teach under-\\nprivileged children free of cost under NSS, PEC (2018-\\n2019).',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Garima Bhatia - American Express.pdf',\n",
       "   'chunk_id': 'CV_chunk_76'}},\n",
       " {'text': '‚Ä¢ Co-founder and creative head of Women Empower-\\nment Cell in PEC (2016- 2019).\\n‚Ä¢ Project head for Aabha- an initiative to teach under-\\nprivileged children free of cost under NSS, PEC (2018-\\n2019).\\n‚Ä¢ Joint Head of Technical Co-ordination in PEC‚Äôs Annual\\nCultural and Technical Fest - PECFEST (2017 & 2018).\\n‚Ä¢ National Scholarship: Awarded with a scholarship from\\nAICTE under Pragati Scheme for academic excellence\\nfrom 2015 to 2019\\nHigh School\\nBudha Dal Public School, Patiala\\n·Ωå5May 2013 ‚Äì May 2015 (95%)\\nACHIEVEMENTS\\n‚Ä¢ Won the SVP Star Award for leading Fraud Scanners\\nFramework, American Express(2021)\\n‚Ä¢ Secured 7th Rank out of 187 teams in a global\\nhackathon to enhance customer and colleague expe-\\nrience, American Express(2020)\\n‚Ä¢ Recognized for contribution to Colleague Hiring Pillar,\\nAmerican Express(2020)\\n‚Ä¢ Recognized for organizing 3 four days long technical\\nboot camps for new hires, American Express (2020)\\n‚Ä¢ Rewarded by leaders for taking initiative in automating',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Garima Bhatia - American Express.pdf',\n",
       "   'chunk_id': 'CV_chunk_77'}},\n",
       " {'text': 'American Express(2020)\\n‚Ä¢ Recognized for organizing 3 four days long technical\\nboot camps for new hires, American Express (2020)\\n‚Ä¢ Rewarded by leaders for taking initiative in automating\\ntools for enhanced customer experience , American\\nExpress(2019)\\nSKILLS AND TECHNOLOGIES\\nHive/Hadoop Spark Python ML (Basic) Big Data\\nMicrosoft Excel Agile Methodologies JIRA Rally\\nProduct Management\\nMY TIME\\nLearning New\\nTechnologies\\nSolving prob-\\nlems at AmEx Family &\\nFriends\\nSports\\nYoga\\nSketching',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Garima Bhatia - American Express.pdf',\n",
       "   'chunk_id': 'CV_chunk_78'}},\n",
       " {'text': 'Ginni Bajaj Corresponding Address: \\nginnibajaj11@gmail.com #274, Sector-59, Phase-4 \\nMobile: +918755966687 Mohali,160059 \\nObjective:  \\n \\nTo leverage my extensive experience and expertise in data analysis to contribute effectively to dynamic \\norganization, while continuously enhancing my skills and knowledge. \\n \\nResults-driven Data Analyst with over 6 years of experience in transforming complex data into a ctionable insights. \\nExpert in statistical analysis, mathematical modeling, and data visualization, with a proven ability to lead teams \\nand drive data-driven decision-making. Adept at leveraging advanced analytical tools and methodologies to deliver \\nimpactful results that align with organizational goals. \\n \\n    Senior Data Analyst \\n    Hashbrown Systems Pvt Ltd \\n    January 2018 - Present \\n\\uf0b7 Spearheaded data analysis initiatives that enhanced operational efficiency and strategic decision -making, \\nsignificantly contributing to the company‚Äôs growth.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\GinniBajaj_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_79'}},\n",
       " {'text': 'January 2018 - Present \\n\\uf0b7 Spearheaded data analysis initiatives that enhanced operational efficiency and strategic decision -making, \\nsignificantly contributing to the company‚Äôs growth. \\n\\uf0b7 Proficiently designed and implemented interactive dashboards, enabling real -time data visualization and \\nfacilitating stakeholder engagement. \\n\\uf0b7 Led a high -performing team of analysts, fostering a culture of excellence and  collaboration while ensuring \\nadherence to best practices in data analysis. \\n\\uf0b7 Developed robust statistical models and automated solutions, achieving a 30% reduction in processing time for \\ndata validation and reporting. \\n\\uf0b7 Collaborated with cross-functional teams to identify and implement process improvements, driving a culture of \\ncontinuous improvement across the organization. \\n \\n \\n\\uf0b7 Recognized as Employee of the Year for exceptional performance in 2019 and 2021.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\GinniBajaj_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_80'}},\n",
       " {'text': 'continuous improvement across the organization. \\n \\n \\n\\uf0b7 Recognized as Employee of the Year for exceptional performance in 2019 and 2021. \\n\\uf0b7 Enhanced the accuracy of machine learning models through meticulous verification and validation processes, \\nresulting in a 25% increase in prediction accuracy. \\n\\uf0b7 Successfully managed large datasets, utilizing advanced Excel functions and R programming to extract \\nmeaningful insights that informed strategic initiatives. \\nProfessional Summary: \\nProfessional Experience: \\nKey Achievements:',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\GinniBajaj_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_81'}},\n",
       " {'text': \"\\uf0b7 Data Analytics & Visualization: SQL, R, Advanced Excel, Power BI, Tableau \\n\\uf0b7 Data Management: Data Warehousing, Data Preparation, Data Mining \\n\\uf0b7 Leadership & Collaboration: Team Leadership, Cross-Functional Collaboration, Project Management \\n\\uf0b7 Communication & Problem Solving:  Excellent interpersonal skills, adept at translating complex data insights \\ninto actionable strategies. \\n \\n\\uf0b7 Post Graduate Diploma in Statistics \\nPanjab University, 2020 - A+ \\n\\uf0b7 Post Graduate Diploma in HRM \\nSymbiosis Centre for Distance Learning, 2018 - A+ \\n\\uf0b7 Bachelor of Arts in Economics (Honours) \\nPanjab University, 2016 - 79% \\n\\uf0b7 12th Standard \\nCBSE, 2013 - 90% \\n\\uf0b7 10th Standard \\nCBSE, 2011 - 10 CGPA \\n \\nDate of Birth: 21/06/1996 \\nGender: Female \\nFather's name: Mr. Narendra Bajaj \\nNationality: Indian \\nMarital status: Single \\nLanguages: Hindi, English, Punjabi \\n \\nI hereby declare that the above information is true to the best of my knowledge and belief. \\nGINNI BAJAJ \\nCore Competencies: \\nAcademic Profile:\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\GinniBajaj_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_82'}},\n",
       " {'text': 'Languages: Hindi, English, Punjabi \\n \\nI hereby declare that the above information is true to the best of my knowledge and belief. \\nGINNI BAJAJ \\nCore Competencies: \\nAcademic Profile: \\nPersonal Information: \\nDeclaration:',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\GinniBajaj_resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_83'}},\n",
       " {'text': 'ISHIKA DHURIA\\nPRODUCT MANAGER\\nEXECUTIVE SUMMARY\\nExperienced Product Manager with  \\nbackground spanning 7 years in\\nProduct Management & Analytics.\\nAdept at driving product strategy,\\nroadmap planning, and cross-\\nfunctional collaboration to deliver\\nexceptional user experiences.\\nDemonstrated proficiency in agile\\nmethodologies and product\\nlifecycle management. Proven\\nability to translate complex\\nrequirements into innovative\\nsolutions that align with market\\nneeds, I bring a unique blend of\\nproduct management expertise\\nand data-driven insights\\nSKILLSETS\\nCleverTap\\nAB Testing\\nFirebase & Google Analytics\\nBalsamiq & Figma\\nProduct Development\\nProduct Launch\\nProduct Management\\nCross Functional Stakeholder\\nMgt\\nPython, R,SQL\\nPowerBI, Tableau\\nBig Query\\nForecasting&Demand Planning\\nStatistical Modelling\\nSegmentation\\nOptimisation\\nSKU Rationalization\\nProduct Analytics & Mgt\\nProduct Skillset\\nTechnology\\nStatistical and Machine Learning\\nRELEVANT EXPERIENCE',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ishika Dhuria PM CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_84'}},\n",
       " {'text': \"Statistical Modelling\\nSegmentation\\nOptimisation\\nSKU Rationalization\\nProduct Analytics & Mgt\\nProduct Skillset\\nTechnology\\nStatistical and Machine Learning\\nRELEVANT EXPERIENCE\\nDecision Scientist                                          Nov'16-Mar'19\\n+91-9569905476\\ndhuriaishika@gmail.com\\nMu Sigma\\nQUALIFICATIONS\\nB.Tech(Electronics&Communication),\\nPanjab University, Chandigarh\\nCGPA-8.54\\nProduct Lifecycle ManagementRoadmappingMarket ResearchCompetitive AnalysisCustomer InsightsGo-to-Market StrategyCustomer JourneyUser Experience (UX)Product InnovationCross-functional CollaborationRequirements Gathering\\nGenerated a customisable assortment(250+ stores)for an O2O unit\\nin about 30 cities in China by choosing an optimum number of\\nSKU‚Äôs(~2500) from both online and offline pools(~35k) leading to a\\nbetter product mix\\nSaving of 1440 Man Hours per month by Time Series Forecasting\\nfor better Inventory planning\\nConsultant                                                       Apr'19-Apr'22\\nAccenture\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ishika Dhuria PM CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_85'}},\n",
       " {'text': \"better product mix\\nSaving of 1440 Man Hours per month by Time Series Forecasting\\nfor better Inventory planning\\nConsultant                                                       Apr'19-Apr'22\\nAccenture\\nCommodity Price Forecasting SaaS AI Tool- FnB, India\\nFrom Ideation to Launch, led the development of an automated\\ntool to get the commodity price forecast for the next 6 months\\nwhich led to incremental 3% net profit margin and saving of 500\\nMan Hours per month\\nProduct Manager                                          May'22-Present\\nChingari\\nChingari is a Series A funded Web 3 social media application with\\n10M+ MAU\\nSubscriptions- Launched and scaled Chingari subscriptions to $2M\\nmonthly revenue with an increase in 15% on-chain activations.\\nImproved D7 and D30 retention by 8% and 5% respectively \\nRefer&Earn- Launched contact syncing system for users to identify\\ntheir contacts not on Chingari. This resulted in 50k+new users within\\n3 months\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ishika Dhuria PM CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_86'}},\n",
       " {'text': 'Refer&Earn- Launched contact syncing system for users to identify\\ntheir contacts not on Chingari. This resulted in 50k+new users within\\n3 months\\nProfile Calls- Introduced 1-1 calls in the profile section leading to 15%\\nincremental calls and 8% revenue growth\\nCustom Gamification- Introduced quick gifting recommendations,\\nleading to a significant 66% decrease in gift-sharing time, reduced\\nfrom 15 seconds to 5 seconds\\nUI/UX changes- Improvements in Profile and Menu page resulting in\\n15% lift in user engagement\\nTableau Alternative- Created a cost effective in-house Analytical tool\\nto track product metrics saving ~100k/year on costs\\nAgency Performance Hub- Designed and launched an automated tool\\nfor real-time creator management and seamless commission payouts\\nto Paytm wallets. Onboarded ~1k agencies resulting in 1200 Man\\nHours saving per month \\nWorkforce Management SaaS Tool- Hospitality,MEA\\nSpearheaded the development of an optimisation tool to manage',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ishika Dhuria PM CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_87'}},\n",
       " {'text': 'to Paytm wallets. Onboarded ~1k agencies resulting in 1200 Man\\nHours saving per month \\nWorkforce Management SaaS Tool- Hospitality,MEA\\nSpearheaded the development of an optimisation tool to manage\\nstaff across different departments, boosting staff productivity by\\n22% and cutting labor costs by 7%\\nReduced Staff Overtime Percentage by 9%(15% to 6%)',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Ishika Dhuria PM CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_88'}},\n",
       " {'text': 'Marketing Manager | B2B E-Commerce | FMCG\\nMarketing Manager with 4 years of experience and PGDM from\\nMDI Gurgaon (Tier-1 B-School), specializing in Marketing &\\nOperations.  Effectively leveraging business operations and\\nmarketing platform skills & managing B2B Customer\\nMarketing, Brand Relations, Merchandising Revenue and FMCG\\nE-Commerce projects from ideation to implementation.\\nProfessional Summary\\nCentral Marketing Manager - FMCG\\nDec 2021 - Present\\nWork\\nExperience Developed Marketing Strategy for ~2.5 Lac FMCG Buyers\\n(Retail+WS) pan India through Customer Segmentation &\\nBuyer Behaviour Data Analytics\\nDevised In-App User Store-fronts to drive incremental\\norganic sales & maintain the right product mix on App\\nhelping Organic GMV via Stores grow by ~1.8X\\nStrategized and Rolled out of National Level Events (end\\nto end) for FMCG including conceptualization, Mechanics\\n& Gratification\\nLeveraged Tech and Buyer Behaviour Analytics to initiate\\nnew properties on Udaan App\\nUDAAN',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Kaustav Datta - FMCG Marketing Manager (Resume).pdf',\n",
       "   'chunk_id': 'CV_chunk_89'}},\n",
       " {'text': 'to end) for FMCG including conceptualization, Mechanics\\n& Gratification\\nLeveraged Tech and Buyer Behaviour Analytics to initiate\\nnew properties on Udaan App\\nUDAAN\\nRegional Trade Marketing - Food (N&E)\\nCreated the Trade and Business Development Plan (BDP)\\nfor Regions with inputs on strategic priorities and\\nalignment with Marketing Calendar to execute Regional\\nBusiness Strategy\\nStrategized with Category and Central Trade Marketing\\nteam to create business plan for category/vertical\\npenetration and buyer growth in both regions for Food\\nEstablished Merch Revenue streams in East and helped\\ndrive Organic Buyer growth through effective marketing\\ncampaigns & Mega/Periodic events (conceptualized\\nSunday Bazaar)\\nJan 2021 - Dec 2021\\nKAUSTAV DATTA\\nGurgaon, India\\nM: +91-9725677339\\nE: kaustavdatta18@gmail.com\\nL: https://www.linkedin.com/in/kaustavdatta\\nB2B Marketing & Merchandizing\\nFMCG & E-Commerce\\nDigital Marketing & Analytics\\nBizRev & PnL Management\\nProduct Marketing\\nCustomer Lifecycle Management',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Kaustav Datta - FMCG Marketing Manager (Resume).pdf',\n",
       "   'chunk_id': 'CV_chunk_90'}},\n",
       " {'text': 'L: https://www.linkedin.com/in/kaustavdatta\\nB2B Marketing & Merchandizing\\nFMCG & E-Commerce\\nDigital Marketing & Analytics\\nBizRev & PnL Management\\nProduct Marketing\\nCustomer Lifecycle Management\\nBrand Management\\nKey Skills\\nTrade Marketing & Merchandising - Food (N)\\nEstablished Merch Revenue streams in North for Food,\\nscaling monthly revenue up to ~8X by Dec 2020\\nLaunched Udaan Premier League, the first mega event\\nleading to >100% GMV Achievement and successive\\nlaunch of Udaan Mega Bharat Sale & FMCG Super Sale\\nImproved Merch Revenue generation for the app via\\nSlot Utilization and effective Merch planning\\nJul 2020 - Dec 2020\\nMerchandiser - Food (North)\\nCreated Merchandizing Revenue plans, pitched and\\nexecuted campaigns for all Food brands in North\\nMar 2020 - Jun 2020',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Kaustav Datta - FMCG Marketing Manager (Resume).pdf',\n",
       "   'chunk_id': 'CV_chunk_91'}},\n",
       " {'text': 'Lean Six Sigma Green Belt\\nKPMG Certified Operations Manager (Six Sigma principles)\\n2018Certifications\\nEducation PGDM (MBA) - MDI Gurgaon\\nMarketing & Operations\\n2016 - 2018\\nManagement Trainee, Sales & Marketing\\nB2C Marketing: Launched Hyper Local Campaign across\\n28 districts for both brands post Network Integration\\nDeveloping CXX-NPS Strategy 2019-20 UPE andNPS\\nanalysis & reporting. \\nB2B Marketing: Increased SoHo contribution in overall\\nEnterprise sales from Retail channels to 20% (33.3%\\ngrowth) in 3 months\\nLaunched Cloud Premier League in UP East leading to\\nimprovement in channel participation by 10x\\nDeveloped VIBS4RETAIL product brief & guidelines for\\nRetail and conducted training for 241 CCEs \\n5 of 6 zones achieved >100% MoM growth in VIBS\\nMobility Sales in the period & multiple Enterprise sales \\nB2C Sales: Increased Gross (111% Tgt vs Ach) and UL\\nPenetration (112% Tgt vs Ach) averaged over the 3\\nmonths stint\\nLed 8 sales executives catering to 48 Lac revenue',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Kaustav Datta - FMCG Marketing Manager (Resume).pdf',\n",
       "   'chunk_id': 'CV_chunk_92'}},\n",
       " {'text': 'B2C Sales: Increased Gross (111% Tgt vs Ach) and UL\\nPenetration (112% Tgt vs Ach) averaged over the 3\\nmonths stint\\nLed 8 sales executives catering to 48 Lac revenue\\nmarket, 3.12L population, 700+ outlets & 2800\\nsales/month, grew distributor ROI realisation by 2X.\\nJun 2018 - Jun 2019\\nB.Tech - Pandit Deendayal Petroleum University\\nMechanical Engineering\\n2012 - 2016\\nD.A.V Public School, Kota\\nClass XII - Science, CBSE Board\\n2011\\nCampus Winner - \"The Ultimate Pitch 2.0\" by Reliance Ind. Ltd.2017Academic\\nAchievements\\nAwarded funding worth 2.5 lacs from the Office of Student\\nResearch Program (ORSP), PDPU for IAF Research Project\\n2015\\nDhirubhai Ambani SSC Merit Reward by DAF for securing 4th\\nrank in State Merit List\\n2011\\n99%ile in NASSCOM Competence Assessment Technology Test2015\\n98.26%ile in CAT, 99.76%ile in NMAT & 99.64%ile in SNAP2015\\nProduct Marketing Manager\\nManaged B2C Prepaid business & Data Products (both\\nVodafone and Idea brands) in Rajasthan Circle',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Kaustav Datta - FMCG Marketing Manager (Resume).pdf',\n",
       "   'chunk_id': 'CV_chunk_93'}},\n",
       " {'text': '98.26%ile in CAT, 99.76%ile in NMAT & 99.64%ile in SNAP2015\\nProduct Marketing Manager\\nManaged B2C Prepaid business & Data Products (both\\nVodafone and Idea brands) in Rajasthan Circle\\nManaging customer base of 14 Mn through customer\\nlifecycle management, product development and GTM\\nstrategies for sales\\nDriving GSR from Retail + Digital business, Revenue\\ngrowth, Customer acquisition and renewal, Subs Quality\\nand Data product sales.\\nProduct Lead on \"Every Recharge Wins\" campaign,\\ndriving revenue growth through Retailer schemes\\nJun 2019 - Feb 2020\\nVODAFONE IDEA',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Kaustav Datta - FMCG Marketing Manager (Resume).pdf',\n",
       "   'chunk_id': 'CV_chunk_94'}},\n",
       " {'text': 'American Express     |     IIM Indore     |      SQL      |      PySpark      |     Python     |     Data Analytics     |     Decision Science \\nPROFESSIONAL EXPERIENCE \\nAmerican Express, Gurgaon Assistant Manager, Commercial Marketing Decision Science Sep‚Äô20 ‚Äì Dec‚Äô21 \\nResponsible for developing the marketing models  for US sma ll busi ness car d members and  merchants for Business F inancing and \\nSupplier Payment related pro ducts. These machine learning models are used  for new acquisitions and cross-selling and fuel profitability \\ncalculation based on N et Present Value. End-to-end responsibility of data extraction, feature engineering, modelling techniq ue, model \\ndeployment as well as model performance tracking ‚Äì all this while taking into account model compliance. \\nIn Depth understanding of various non card products such as Working Capital Terms, Business Loans, Merchant Financing, etc. W ell',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_95'}},\n",
       " {'text': 'In Depth understanding of various non card products such as Working Capital Terms, Business Loans, Merchant Financing, etc. W ell \\nversed with SQL, Hive, Spark, Decision Tree based boosting algorithms (CatBoost, XGBoost) and Python  \\nKey Projects \\nFX International Payments (FXIP) ‚Äì Propensity model: Developed the model to predict the propensity of Small \\nBusiness cardmember to enrol for FXIP. \\nEngineered features related to MYCA visits from foreign IP address, spend on foreign merchants and foreign tickets. \\nCommercial Share of Wallet: Ideated Commercial Spend and Lending share of wallet as part of Primacy Framework. \\nFormulated dependent variable using relevant line items from financial statements for size of wallet models. \\nBuilt the models to predict Lend and Spend size of wallet for Small Business and Corporates. \\nMentored a contractor to build the model predicting size of wallet for Banking pillar of Primacy framework.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_96'}},\n",
       " {'text': 'Mentored a contractor to build the model predicting size of wallet for Banking pillar of Primacy framework. \\nBusiness Loan Prepayment Prediction: Developed the model predicting interest revenue loss due to prepayment. \\nAchievements Awarded Commercial Marketing Decision Science ‚Äì Superstar of the Quarter for excellent performance in Q2‚Äô21. \\nAwarded Extra Mile award for helping the team with OCC Examination for Business Loan Email Response Model. \\nPosition of \\nResponsibilities \\nCore member of Good Citizenship Team of Enterprise Digital & Analytics since Apr‚Äô20. \\nCoordinated food grain collection drive and also volunteered for online education drive for adults by AMEX. \\nAmerican Express, Gurgaon Analyst, Global Commercial Data Science Sep‚Äô18 ‚Äì Sep‚Äô20 \\nKey Projects \\nWorking Capital Terms (WCT) - Response Prediction: Built and deployed the first version of Response models for \\nWCT for targeting across Email, MYCA and Tele channel.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_97'}},\n",
       " {'text': 'Key Projects \\nWorking Capital Terms (WCT) - Response Prediction: Built and deployed the first version of Response models for \\nWCT for targeting across Email, MYCA and Tele channel. \\nEngineered features related to time weighted solicitation history, cross product offers and AMEX website visits. \\nProposed Uplift Modelling technique to predict customer response through tele-marketing channel. \\nPartnered with marketing team to make sure the offers are smoothly delivered to customers. \\nMerchant Financing (MF) - Write off Prediction: Developed the models to predict MF write off for pure merchants as \\nwell as merchants with existing card relationship. \\nEngineered features related to disputes, frauds, bankruptcy and previous loan repayment history. \\nSmall Business Administration ‚Äì Pay check Protection Program (SBA PPP) Need Prediction: Constructed dependent \\nvariable to recognize the need of a card member or a merchant for SBA PPP loan.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_98'}},\n",
       " {'text': 'Small Business Administration ‚Äì Pay check Protection Program (SBA PPP) Need Prediction: Constructed dependent \\nvariable to recognize the need of a card member or a merchant for SBA PPP loan. \\nDeveloped and scored the models under stringent timelines as it was the special requirement for Covid program. \\nAccounts Payable Automation (APA) Payment Solution - Response Prediction: Developed and implemented the \\nunified response model predicting response across Email, MYCA and Field channels. \\nExplored features related to accounts payable, accounting services related spend, APA page visits on website. \\nSummer Intern Mentoring Program ‚Äì 2020: Mentored an intern from MDI Gurgaon on Merchant Financing \\nResponse prediction model for Tele channel. \\nAchievements Awarded Global Commercial Data Science - Analyst of the Quarter award for outstanding performance in Q3‚Äô19. \\nEXL Service, Ahmedabad Assistant Manager, Service (Analytics) Apr‚Äô18 ‚Äì Sep‚Äô18',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_99'}},\n",
       " {'text': 'EXL Service, Ahmedabad Assistant Manager, Service (Analytics) Apr‚Äô18 ‚Äì Sep‚Äô18 \\nResponsibilities Small Business Credit Card (US banking client): Analysed the credit card data to compute credit card KPIs and \\ngenerate insights deep diving those KPIs. \\nGujarat State Petroleum Corporation, Gandhinagar Graduate Engineer Trainee Jun ‚Äô15 ‚Äì Jun ‚Äô16 \\nResponsibilities Led the staff of 17 people responsible for operational and maintenance activities of oil production facilities. \\nEDUCATIONAL QUALIFICATIONS \\nMBA Indian Institute of Management (IIM), Indore 3.35/4.33 2018 \\nB.Tech, Petroleum Pandit Deendayal Petroleum University, Gandhinagar 8.41/10 2015 \\nClass XII, Gujarat Board SVP Gujarat Vidyalaya, Gandhidham 89.69% 2011 \\nClass X, Gujarat Board AUM Vidya Mandir, Gandhidham 91.08% 2009 \\nACADEMIC & CO-CURRICULAR ACHIEVEMENTS \\nCleared FRM (Financial Risk Manager) - Level 2, conducted by Global Association of Risk Professionals (GARP). 2017',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_100'}},\n",
       " {'text': 'ACADEMIC & CO-CURRICULAR ACHIEVEMENTS \\nCleared FRM (Financial Risk Manager) - Level 2, conducted by Global Association of Risk Professionals (GARP). 2017 \\nHonoured with Patidar Vidyaratna Platinum award for outstanding performance in Class XII examination. 2011 \\nAwarded scholarship for higher secondary education by IFFCO for excellent performance in Class X examination. 2009 \\nPROJECTS & RESEARCH PAPERS \\nAcademic Project Credit Card Fraud Detection Jun ‚Äô17 ‚Äì Aug ‚Äò17 \\nPredicted credit card frauds by applying various techniques such as Random Forest, Logistic Regression in R. \\nEXTRA-CURRICULAR ACTIVITIES \\nInterests Enthusiastic about playing cricket, FIFA, football and stock markets. \\n Krupal Patel  \\n  Male, 27 years \\nMobile: +91-9638706888 \\nEmail: p16krupalp@iimidr.ac.in',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\KRUPAL_PATEL_RESUME.pdf',\n",
       "   'chunk_id': 'CV_chunk_101'}},\n",
       " {'text': '-\\n2013 - 2017~ +\\n-\\n-\\n-\\n07/2020 - Ongoing~\\n-\\n-\\n-\\n10/2019 - 06/2020~\\n-\\n-\\n-\\n01/2017 - 09/2019~\\nJava Spring Spring Boot\\nSpring Data JPA Spring REST\\nHibernate GraphQL Microservices\\nXML XSD Spring Security\\nJenkins Spring Redis J2EE\\nMySQL Liquibase MongoDB\\nIntelliJ Maven GIT Scrum\\nPostman\\n\\ue02e\\nwww.enhancv.com Powered by\\n/\\nKush Patel\\nJava Developer\\n#\\x0091\\x007990752387 _kush3496\\x00gmail.com \\ue122in.linkedin.com/in/kush-patel-java\\n+Godhra, Gujarat\\nSUMMARY\\nInnovative IT professional with 4.8 years of experience. Aspiring to work for leading organization to leverage its goals and ambitions. Worked with broad range of clients including international customers. Skilled in writing clean, elegant and optimal code with keen ability to analyze and solve real world software use-cases.\\nEDUCATION\\nB.Tech Computer Engineering\\nDharmsinh Desai University \\nNadiad, Gujarat\\nRanked among top 10 academic achievers with 8.84 CGPA\\nEXPERIENCE\\nSenior Programmer Analyst\\nArgusoft India Ltd.',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Kush's Resume_SWE_5YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_102'}},\n",
       " {'text': 'EDUCATION\\nB.Tech Computer Engineering\\nDharmsinh Desai University \\nNadiad, Gujarat\\nRanked among top 10 academic achievers with 8.84 CGPA\\nEXPERIENCE\\nSenior Programmer Analyst\\nArgusoft India Ltd.\\nLocHub enables automating the entire translation roundtrip by sending content directly from your CMS system to your translation agency of choice and routing translated content back to your CMS.\\nDesigned a solution to convert WSDL file to Java sources through maven plugin and used it as dependencyImplement file upload scheduling algorithm for Rikai TMS. Divided complex requirements criteria to simple ones, designed a Predicate for each and the combined them allWorked on the code to be compatible with the SonarQube quality gate and developed test cases in JUnit\\nNorth America Learning Experience is a portal for the employees of Est√©e Lauder Companies Inc.',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Kush's Resume_SWE_5YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_103'}},\n",
       " {'text': 'North America Learning Experience is a portal for the employees of Est√©e Lauder Companies Inc.\\nAugmented API architecture to send mock data which enables parallel development of middleware and frontend microservicesExtracted common behaviour from unrelated entities into an interface and designed a service to process the common fieldsCompletely re-wrote complex legacy service with Java Streams code improving readability and maintainability\\nProgrammer Analyst\\nArgusoft India Ltd.\\nStreamlyne is a research administration software that can be used to apply for grants at educational institutions.\\nIncreased application usability significantly by integrating Orbeon FormsCreated plugin to publish the webforms on application startup, thereby reducing boot time by 50%Built the Unmarshalling API from scratch to convert plain XML data to reusable JAVA objects\\nTECH SKILLS\\nTech Stack\\nData Engineering\\nDevelopment\\nFIND ME ONLINE\\nStack Overflow\\nhttps://stackoverflow.com/users/8603014/mark42inbound\\nBLOGS',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Kush's Resume_SWE_5YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_104'}},\n",
       " {'text': 'TECH SKILLS\\nTech Stack\\nData Engineering\\nDevelopment\\nFIND ME ONLINE\\nStack Overflow\\nhttps://stackoverflow.com/users/8603014/mark42inbound\\nBLOGS\\nLambda Expressions in Java and its implementation \\nhttps://blog.argusoft.com/lambda-expressions-in-java-and-its- implementation/',\n",
       "  'meta': {'source': \"..\\\\data\\\\CV\\\\Kush's Resume_SWE_5YOE.pdf\",\n",
       "   'chunk_id': 'CV_chunk_105'}},\n",
       " {'text': 'U r m a n p r e e t  S i n g h                                             L e a d D a t a  S c i e n t i s t            \\nEmail: urmanpreetvgsom@gmail.com   Mobile: +91 8861125026                             6.25 years Experience \\nEDUCATION   \\nYear Examination Institution Board/University CGPA/Percentage \\n2017 MBA - Business Analytics IIT Kharagpur  IIT Kharagpur 7.77/10 \\n2013 B. Tech - ECE Jaypee University of IT Deemed University 77.00%  \\n \\nSKILLS        \\n\\uf0b7 Machine Learning, Deep Learning  \\uf0b7 Statistical Hypothesis Testing \\uf0b7 Sklearn, Tensorflow, Pandas, PySpark \\n\\uf0b7 Python, R, SQL, Excel \\uf0b7 Cloud, API development \\uf0b7 Clustering, Classification, Time Series \\n \\nPROFESSIONAL EXPERIENCE             TOTAL WORK EXPERIENCE: 6.25 YEARS \\n \\nEquinix, a Fortune 500 company  \\nLead Data Scientist                                                                                                                    Skoruz Tech. Payroll:  Sep 20 ‚Äì till date',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Lead Data Scientist_IIT Kharagpur_Urmanpreet Singh.pdf',\n",
       "   'chunk_id': 'CV_chunk_106'}},\n",
       " {'text': 'Lead Data Scientist                                                                                                                    Skoruz Tech. Payroll:  Sep 20 ‚Äì till date \\nMBB Labs, a R&D unit of Malaysia‚Äôs Largest Bank                           Mbb Labs Payroll:  Jan 18 ‚Äì Sep 20 \\nSenior Data Scientist                                                                                                         Optimum infosystem Payroll:   Jun 17 ‚ÄìDec 17 \\n\\uf0b7 Received ‚Äúmost innovative employee‚Äù award in the company. Also nominated for Young Maybanker award globally \\n\\uf0b7 ML Model Research and Development: Research the latest advancements in the ML space and experiment the same \\n\\uf0b7 Data Identification and Gathering: Lead the identification of Business metrics used for ML model building and project \\nmanage collection of the same from different sources \\n\\uf0b7 Data Preprocessing: Better understand data and explain to team members the Business side of data. Contribute to best',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Lead Data Scientist_IIT Kharagpur_Urmanpreet Singh.pdf',\n",
       "   'chunk_id': 'CV_chunk_107'}},\n",
       " {'text': 'manage collection of the same from different sources \\n\\uf0b7 Data Preprocessing: Better understand data and explain to team members the Business side of data. Contribute to best \\nimpute the data, treat outliers and do Feature Engineering out of raw data \\n\\uf0b7 Model Development: Develop and implement ML models using the best Frameworks in an iterative way \\n\\uf0b7 Product Design and Use case development: Design Machine Learning solutions to solve business problems. Further \\nincorporate Machine Learning solution into software products and platforms for easy configurability and deployment.   \\n\\uf0b7 Scoping and Prioritization: Write user stories, prioritize features and develop wireframe development in an Agile method \\n\\uf0b7 Stakeholder Communication: Interact directly with high level Business stakeholders, Data owners, Subject Matter experts \\nand act as a bridge between Business and purely technical staff',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Lead Data Scientist_IIT Kharagpur_Urmanpreet Singh.pdf',\n",
       "   'chunk_id': 'CV_chunk_108'}},\n",
       " {'text': '\\uf0b7 Stakeholder Communication: Interact directly with high level Business stakeholders, Data owners, Subject Matter experts \\nand act as a bridge between Business and purely technical staff \\n\\uf0b7 Present to stakeholders, the model development progress, incorporate their feedback and address their concerns  \\n\\uf0b7 Mentoring: Guide junior team members on Business and Technology side and assign them relevant projects  \\nProduct Highlight Business Problem Impact Methodology \\nDigital Lending Fully machine learning driven loan approval/rejection \\ndecision for small SME loans \\n20-30% Loan growth/ \\n20-30% FTE saving  \\nXGBoost, Neural \\nNetwork, Clustering \\nInstant Loan:  \\nLimit Setting \\nDecide loan amount real-time based on future income \\nof customer \\n20-30%  \\nLoan growth \\nDeep Learning, Time \\nSeries Analysis \\nPortfolio Optimization \\nof Loan book \\nOptimization of Maybank loan portfolio across \\ndimensions such as product type and industry \\napprox. 16 basis point \\nreturn improvement',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Lead Data Scientist_IIT Kharagpur_Urmanpreet Singh.pdf',\n",
       "   'chunk_id': 'CV_chunk_109'}},\n",
       " {'text': 'Series Analysis \\nPortfolio Optimization \\nof Loan book \\nOptimization of Maybank loan portfolio across \\ndimensions such as product type and industry \\napprox. 16 basis point \\nreturn improvement \\nMarkowitz mean variance \\nOptimization \\nProperty Price \\nForecasting \\nForecasting property price to keep risk of mortgage \\nloans under control \\nup to 100% loan \\nrecovery \\nTime series forecasting \\nusing ARIMA \\nLoan Securitization Analytical tool for risk analysts: segregate risky loans at \\nany given time incoming data  \\nNPA reduction to \\npossibly less than 1% K-proto Clustering  \\nDetection of \\nConcentration risk \\nAnalytical tool: Real time detection of Concentration \\nCredit risk across various measures and dimensions \\nup to 1.2% less \\neconomic capital Interactive Visualization \\nFeature Selection Model explainability: Top Features to predict NPA Research work Genetic Algorithm \\nPropensity Model Prioritize prospects for targeting by Marketing Function Business Growth Ensemble of Xgboost \\nNeed based',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Lead Data Scientist_IIT Kharagpur_Urmanpreet Singh.pdf',\n",
       "   'chunk_id': 'CV_chunk_110'}},\n",
       " {'text': 'Propensity Model Prioritize prospects for targeting by Marketing Function Business Growth Ensemble of Xgboost \\nNeed based \\nSegmentation \\nIdentify Specific need category of prospects for \\noptimizing marketing efforts \\nSaving in marketing \\nBudget \\nMulti Class \\nClassification \\n \\nInfosys Limited, Mangalore  \\nSystems Engineer (BFSI Domain)  Sep 13 ‚Äì Jun 15 \\n\\uf0b7 Analyzed business problems- gathered new requirements from the client and interpreted them as computational problems \\n\\uf0b7 Collaborated with a team of 4 to build enhancements in the existing Claims processing module using Java and SQL \\n\\uf0b7 Recommended workflow process improvement using predictive analytics leading to reduction in claims lead time  \\nO T H E R  E X P E R I E N C E \\nDRDO- Center for Artificial Intelligence and Robotics                                  B. Tech summer internship| May 12 ‚Äì Jul 12 \\nBoston Consulting Group - Center for Knowledge and Analytics                     MBA summer internship  | May 16 ‚Äì Jul 16',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Lead Data Scientist_IIT Kharagpur_Urmanpreet Singh.pdf',\n",
       "   'chunk_id': 'CV_chunk_111'}},\n",
       " {'text': 'MANAVI ARORA \\nChandigarh, India | +91 7986843043 | manaviarora01@gmail.com | LinkedIn | Github | Portfolio | LeetCode \\nEDUCATION \\nUIET CHANDIGARH | PANJAB UNIVERSITY Chandigarh, IND \\nBachelor of Engineering         2022 - 2026 \\nMajor in Information Technology \\nCumulative GPA: 9.42/10 \\n   CLASS XII | CBSE                         Bhiwani Public School \\n    Percentage: 94%           2021-2022 \\n \\n   CLASS X | ICSE                     Bhiwani Public School \\n    Percentage: 95.8%           2019-2020 \\nEXPERIENCE \\n \\n1. Software Developer Engineer Intern | Bluestock FinTech                      Aug 2024 - Oct 2024 \\n‚Ä¢ Created 10+ client-facing features with React.js and Node.js and reduced deployment time via CI/CD automation. \\n‚Ä¢ Improved performance of key features by 20% through software testing, utilizing VS Code‚Äôs debugging tools. \\n2. Frontend Developer Intern | Chinar Fabrics                        Jun 2024 - July 2024',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\MANAVI_ARORA_CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_112'}},\n",
       " {'text': '2. Frontend Developer Intern | Chinar Fabrics                        Jun 2024 - July 2024 \\n‚Ä¢ Developed 20+ modular React.js components, reused to accelerate UI delivery by 40% and enhance maintainability. \\n‚Ä¢ Led 15+ code reviews, accelerating feature delivery speed by 20% and ensuring 100% test and documentation coverage. \\nMAJOR PROJECTS \\n1. Cab Booking Application: [Link] \\n‚Ä¢  Live Tracking: Integrated real-time route tracking for trip transparency and user trust, increasing user engagement by 50%. \\n‚Ä¢  Power BI Dashboard: Architected a Power BI dashboard delivering real-time insights, optimizing operations by 35%. \\n2.   Real-time Chat Application: [Link] \\n‚Ä¢  Real-Time Messaging: Engineered with Socket.io, delivering live one-on-one and group chats for 2,500+ active users. \\n‚Ä¢  Interactive Features: Built engaging features ‚Äî responsive design, reactions, story uploads, and video calling ‚Äî    \\nleading to a 45% increase in average session duration.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\MANAVI_ARORA_CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_113'}},\n",
       " {'text': '‚Ä¢  Interactive Features: Built engaging features ‚Äî responsive design, reactions, story uploads, and video calling ‚Äî    \\nleading to a 45% increase in average session duration. \\n3.   AI Voice Assistant for Education: [Link] \\n‚Ä¢  AI-Powered Real-Time Voice Interaction: Implemented live voice functionality with 95% speech recognition \\naccuracy, reducing user drop-off by 30% and maximizing learning outcomes. \\n‚Ä¢  Adaptive AI Models: Applied object-oriented design patterns to craft scalable systems enabling personalized learning \\nand targeted coaching for 10,000 users across language training, interview preparation, and academic topics. \\nSKILLS \\nLanguages: C, C++, MySQL, HTML, CSS, JavaScript, Python, SQL,  \\nFrameworks/Libraries: React JS, Node JS, Express JS, MongoDB, Tailwind, Next JS, Redux, NumPy, Pandas, Matplotlib \\nTools: Visual Studio Code, Git, GitHub, Postman, Cloudinary, Razorpay, Agora, Chrome DevTools, Tableau, Power BI',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\MANAVI_ARORA_CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_114'}},\n",
       " {'text': 'Tools: Visual Studio Code, Git, GitHub, Postman, Cloudinary, Razorpay, Agora, Chrome DevTools, Tableau, Power BI \\nCore CS Concepts: Object-Oriented Design and Programming (OOPS), Data Structures and Algorithms (DSA), Database \\nManagement Systems (DBMS),  Agile Methodology, Software Development Lifecycle, Routing & Switching, Computer \\nNetworks (CN), Operating Systems (OS), Artificial Intelligence (AI), and Design and Analysis of Algorithms (DAA).  \\nACHIEVEMENTS \\nHackathons: \\n‚Ä¢ Active contributor in GDSC Solutions Challenge \\n‚Ä¢ Selected among top 5,000 teams out of 25,000+ teams in Build with India Hackathon. \\nLeadership: Served as Head Organizer, leading 20+ volunteers and translating technical event requirements into clear action \\nplans, improving communication and achieving a 25% increase in event engagement. \\nCompetitive Programming: Solved 600+ problems on LeetCode and earned 13+ prestigious badges, showcasing proficiency in \\ndata structures and algorithms.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\MANAVI_ARORA_CV.pdf',\n",
       "   'chunk_id': 'CV_chunk_115'}},\n",
       " {'text': 'Manish Kumar\\nPhone:+91 7056008581Email: manish-1004@outlook.comLinkedIn:https://www.linkedin.com/in/manish1004/Github: https://github.com/gods-mack/Address:VPO Sehore, Kanina, Haryana ‚Äì 123027\\nEducational Qualifications:\\nYear Degree/Certificate University/Board Marks obtained\\n2017-21\\nB.E.(Hons) Computer ScienceEngineering\\nChandigarh University,Mohali (Punjab)\\n6.7 / 10\\n2016 Senior Secondary (12th ) HBSE 69%2014 Secondary (10th ) HBSE 84%',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\manish Resume_updated_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_116'}},\n",
       " {'text': '2017-21\\nB.E.(Hons) Computer ScienceEngineering\\nChandigarh University,Mohali (Punjab)\\n6.7 / 10\\n2016 Senior Secondary (12th ) HBSE 69%2014 Secondary (10th ) HBSE 84%\\nIndustry Experience:- Software Developer, KinderPass(Apr 2021 - Present)- Integrated various payment gateways (Razorpay, Stripe, Android/iOS) in backend.- Development of push notification handler API.- Implemented full DB Schema for course architecture.- Implemented Redis cache for API response.- Optimized and reduced various API‚Äôs response time.- Development of pre-recorded video API-feature.- Development of backend API for KaiOS(Jio Phone) support.- Implemented Logger to capture backend events.- Implemented  MySQL database table with Django ORM.- Worked on production level backend Bugs (Referral process, timezone issue) and Cronjob.Tech Stack- Flask, Django, MySQL, MongoDB, Redis,Nginx, AWS',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\manish Resume_updated_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_117'}},\n",
       " {'text': '- Backend Engineer Intern, Shunya Inc.(Feb 2021 - Apr 2021)- Worked on OCR track and developed backend microservice APIs.Technical Skills:- Programming Languages:C, C++, Python, Go- Tools:GDB/PDB, MATLAB/Octave, Git, Arduino IDE- Frameworks:Django, Pandas, OpenCV, Redis, Flask-RESTfulCertifications:- Algorithms: Design and Analysis-StanfordOnline (Lagunita)- NAND2TETRIS-The Hebrew University ofJerusalem (Coursera)Projects:- EagleOS (Dec 2020 - Present): Developing a 32-bit tiny OS having support for timerIRQ,Hardware/Software Interrupt handler, paging, andkeyboard/screen device driver.- NAND2TETRIS(June 2020 - Present): Build a ModernComputer from first principles: From- NAND gate to Tetris. Implemented16-bit ALUand assembler.- Onlineauction(Nov2019):Developedbackendforanonlineauctionwebsitehavingsupportfor user authentication and real-time database usingDjango framework.- Vision (Oct 2019- Dec 2019): All-in-onecomputer visionmobileapplicationusingGoogleFirebase MLkit and Dart',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\manish Resume_updated_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_118'}},\n",
       " {'text': 'user authentication and real-time database usingDjango framework.- Vision (Oct 2019- Dec 2019): All-in-onecomputer visionmobileapplicationusingGoogleFirebase MLkit and Dart (Flutter).- Arduino object-detection (Jan 2019 - Feb 2019): Developed an object detection projectusingArduino boardandultrasonic sensor.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\manish Resume_updated_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_119'}},\n",
       " {'text': '- Car-raceminigame(July2019-July2019):Developedacarracemini-gameusingPython‚ÄôsPygame library.Technical-Interests:- Data Structure and Algorithms- Operating System- System ProgrammingExtra-Curricular Activities:- Intel Edge AI Scholarship Winner on Udacity.- 3 Star ( best Rating 1633 )CodeChefRating ( manish_sharma1).- SecuredGlobal Rank 3126 out of more than 29000 usersin CodechefJuly Challenge.- Developed a quiz application (which is globally available) for Google Assistant.- Contributed toGithub Hacktoberfest2019 online open-sourcecontribution event.- Participated in DAVINCI Hackathon and developed a firearms detection application.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\manish Resume_updated_SWE_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_120'}},\n",
       " {'text': \"This PDF file is protected \\n  \\nYou'll need a different reader in order to view this content: \\nDownload a compatible PDF reader. \\n  \\nThis PDF Document has been protected. \\nThe reader you are using does not support opening files protected by Microsoft Office\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Moid_Final_version_resume_SeniorDS_M92025.pdf',\n",
       "   'chunk_id': 'CV_chunk_121'}},\n",
       " {'text': \"\\uf095 +91 7023959521\\n\\uf0e0 namansurana108@gmail.com\\n\\uf041 Jaipur\\n\\uf0e1 https://www.linkedin.com/in/naman-surana-\\n844a3b1b4/\\nSUMMARY\\nEager to work as a Data Analyst with analytical and technical skills, as well as ability to deliver accurate projections and multiple \\nscenarios to define viable process strategies will be applied for overall profitability of the company.\\nPROJECTS\\nPROFESSIONAL EXPERIENCE\\nBusiness Analyst Dec '20-Present\\nBiz4Group LLC\\nBiz4Group is an IT company indulged in providing software and application development services using cutting-edge technologies.\\nJob Responsibility: \\nINTERNSHIPS\\nFront End Web Developer Jun '19-Jul '19\\nAvantis RegTech Pvt. Ltd\\nAvantis, a TeamLease Company, is India's leading Regulatory Technology (RegTech) solutions company enabling Ease of Doing Business for over 1,100 legal \\nentities across 28 States and 9 Union Territories. Avantis offers state of the art multi-tenant, SAAS solution on its web and mobile platforms, enabling a\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Naman Surana Resume_Data_Analyst_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_122'}},\n",
       " {'text': 'entities across 28 States and 9 Union Territories. Avantis offers state of the art multi-tenant, SAAS solution on its web and mobile platforms, enabling a \\ntransparent, accountable and efficient risk and compliance management.\\nSkills Acquired: HTML, CSS, Bootstrap\\nKEY SKILLS\\nTECHNICAL SKILLS\\nJaipur\\nPune\\nNaman Surana\\nBusiness Analyst\\nHouse Price Prediction Project1. \\nEDA and Data Visualization on Google Play Store Dataset2. \\nCase Study on Bank Telemarketing Campaign 3. \\nEDA on IMDB Movie Dataset4. \\nData Visualization using Tableau on IPL Dataset5. \\nData Analysis using SQL on IMDB movie Dataset6. \\nWriting creative proposals1. \\nRequirement gathering and analysis2. \\nProject Estimations based on client requirements3. \\nProject Management4. \\nImplement Agile Project Development Methodology5. \\nEffective Communication with stakeholders6. \\nData Analysis1. \\nData Visualization2. \\nCommunication Skills3. \\nPresentation Skills4. \\nResearch5. \\nRequirements Gathering6.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Naman Surana Resume_Data_Analyst_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_123'}},\n",
       " {'text': 'Effective Communication with stakeholders6. \\nData Analysis1. \\nData Visualization2. \\nCommunication Skills3. \\nPresentation Skills4. \\nResearch5. \\nRequirements Gathering6. \\nSRS (Software requirements specification)7. \\nProject Management8. \\nCreative Proposals9. \\nTeam Player10. \\nClient Communication11.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Naman Surana Resume_Data_Analyst_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_124'}},\n",
       " {'text': \"Data Analysis\\nData Visualization\\nBig Data and Cloud\\nVersioning Tool and OS\\nEDUCATION\\nExecutive Post-Graduate Programme in Business Analytics Sep '21-Present\\nLoyola Institute of Business Administration\\nModules Covered:\\nBachelor of Technology (Computer Science and Engineering) Jul '17-Aug '21\\nJECRC University\\nObtained CGPA of 6.73 and earned 233 Credits\\nAwards and Recognitions\\nChennai\\nJaipur\\nPython/ Exploratory Data Analysis: Pandas, Numpy1. \\nExcel2. \\nSQL3. \\nTableau1. \\nPython: Matplotlib, Seaborn2. \\nAWS 1. \\nHadoop2. \\nHive3. \\nHQL4. \\nGit and Github1. \\nLinux, Windows2. \\nPython for Business Analytics1. \\nEDA and Data visualization in Python2. \\nTableau3. \\nAdvanced SQL4. \\n Big Data and Cloud5. \\nMachine Learning for Business6. \\niHelp Award Certificate from Biz4Group - Awarded with iHelp Award for portraying immense amount of dedication towards work \\nand for being an ardent follower and preacher in practicing all values of the company\\n1. \\nThe Ultimate Team Player Award from Biz4Group2.\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Naman Surana Resume_Data_Analyst_1YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_125'}},\n",
       " {'text': \"Nitish Singh  \\nDevOps Engineer\\nnitisdev@gmail.com\\n+91-8574119120\\nlinkedin.com/in/nitisdev/\\ngithub.com/nitisdev\\nDynamic and creative engineer with experience \\nin producing robust code and working \\nproductively in dynamic environments. I enjoy \\nbuilding products that make a positive impact \\non people's lives. I am a quick-learning team \\nplayer with a focus on achieving project \\nobjectives with speed and accuracy.\\nCertificates\\nRed Hat Certified Specialist in Ansible \\nAutomation\\nRed Hat Certified System Administrator\\nSkills\\nBash\\nPython\\nLinux\\nRed Hat, Cent OS, Ubuntu\\nAnsible\\nAWS\\nEC2, ECS, EKS, Cloudformation\\nGit\\nGitHub, GitLab\\nDocker\\nKubernetes\\nJenkins\\nTerraform\\nVagrant\\nObservability\\nGrafana, Prometheus\\nEducation\\nB.Tech in Computer Science and Engineering, \\nUniversity of Petroleum & Energy Studies\\n05/2016 ‚Äì 06/2020\\nProfessional Experience\\nSoftware Engineer, Msys Technologies\\n07/2020 ‚Äì present\\nClient: Seagate  | Team: DevOps\\nBuilding and maintaining shell scripts to automate the\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Nitish_Singh_Resume_SRE_08-04-2022-16-11-13.pdf',\n",
       "   'chunk_id': 'CV_chunk_126'}},\n",
       " {'text': '05/2016 ‚Äì 06/2020\\nProfessional Experience\\nSoftware Engineer, Msys Technologies\\n07/2020 ‚Äì present\\nClient: Seagate  | Team: DevOps\\nBuilding and maintaining shell scripts to automate the \\ndeployment of the Kubernetes cluster.\\nSetup and manage Harbor registry to automate the \\nuploading of build artifacts from Jenkins pipeline and \\ndeveloped build retention policy.\\nPrimarily developed scripts to monitor production Jenkins \\nand Harbor registry VM downtime and set up alerts for \\nnetwork-attached storage disk space through Grafana \\nand Prometheus.\\nDeveloped and maintain Jenkins CI/CD pipeline to deploy \\nCORTX cluster for development teams.\\nCreate docker containers as a build environment for \\napplication RPM generation and developed sanity \\npipelines for components teams.\\nDeveloped CI/CD pipeline to deploy CORTX-RGW cluster \\nand reduced total rpm package generation build time by \\n80%.\\nCreated and manage PR build pipelines to automate \\ndeployment for every pull request of different',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Nitish_Singh_Resume_SRE_08-04-2022-16-11-13.pdf',\n",
       "   'chunk_id': 'CV_chunk_127'}},\n",
       " {'text': 'and reduced total rpm package generation build time by \\n80%.\\nCreated and manage PR build pipelines to automate \\ndeployment for every pull request of different \\ncomponents for Single node and 3 node clusters.\\nTroubleshoot deployment issues on the Kubernetes \\ncluster and support developers in the process.\\nWorking on developing CI/CD process for Ceph RADOS \\npackage generation and cluster deployment.\\nManage and test build pipelines to automate deployment \\nfor services release for Single node and 3 node clusters.\\nSoftware Engineer Intern, Msys Technologies\\n01/2020 ‚Äì 06/2020\\nPrimarily developed CLI tool in Python to deploy and \\nmanage hybrid cloud infrastructure.\\nAutomated deployment process using Ansible for CI/CD \\npipelines to provision environment.\\nAutomation of server checks using python scripts and on-\\nrun server setup using Jenkins Groovy.\\nPrimarily developed and documented alerting mechanism \\nfor high-availability Pacemaker-Corosync cluster for \\nfrontend Web-UI.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Nitish_Singh_Resume_SRE_08-04-2022-16-11-13.pdf',\n",
       "   'chunk_id': 'CV_chunk_128'}},\n",
       " {'text': 'run server setup using Jenkins Groovy.\\nPrimarily developed and documented alerting mechanism \\nfor high-availability Pacemaker-Corosync cluster for \\nfrontend Web-UI.\\nPrepared and submitted reports and other documentation \\nto assist development team members.\\nPresented several talks on logs based metrics \\naggregation and data visualization utilizing Kibana, \\nLogstash and Elasticsearch.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Nitish_Singh_Resume_SRE_08-04-2022-16-11-13.pdf',\n",
       "   'chunk_id': 'CV_chunk_129'}},\n",
       " {'text': 'Preeti Moolani \\nSenior Data Analyst, Societe Generale \\nBengaluru, KA, India | +91-9131552608 | preeti.moolani@gmail.com | LinkedIn \\n \\nSummary \\nExperienced Data Analyst with nearly 5 years of expertise across Banking, Digital Marketing, and EdTech domains. Proven \\nexpertise in building predictive models, developing NLP and LLM-based solutions, and deploying Machine learning systems at \\nscale. Skilled in translating business needs into AI solutions, automating workflows, and delivering end-to-end analytics using \\nPower Platform. Strong track record of optimizing processes, uncovering business-critical insights, and collaborating with \\nstakeholders to drive performance and strategic decision-making. \\nTechnical Skills \\nProgramming & Data: Python (Pandas, NumPy, Matplotlib, Seaborn), SQL (MySQL, SQL Server), PyTorch \\nMachine Learning & AI: Classification, Regression, Clustering, Forecasting, Feature Engineering, Predictive Modeling, NLP, \\nLLMs (GPT, LLaMA), A/B Testing',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_130'}},\n",
       " {'text': 'Machine Learning & AI: Classification, Regression, Clustering, Forecasting, Feature Engineering, Predictive Modeling, NLP, \\nLLMs (GPT, LLaMA), A/B Testing \\nPower Platform: Power Apps (Canvas & Model-Driven), Power Automate, Power Pages \\nData Visualization: Power BI (Advanced DAX, Power Query), Tableau, Looker \\nCloud & Big Data: Azure Data Factory, Azure Synapse, Azure ML Studio, Azure Functions, OneLake, REST APIs,  \\nData Storage: MongoDB, MySQL, PostgreSQL \\nTools: GitHub, Azure DevOps, JIRA \\nWork Experience \\nSociete Generale Global Solution Centre                                                                                       Bengaluru, India \\nSenior Data Analyst                                                                                                                            Apr‚Äô24- Present \\n‚Ä¢ Built an AI-powered HR virtual assistant using GPT-based LLMs, deployed on Microsoft Teams, automating',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_131'}},\n",
       " {'text': '‚Ä¢ Built an AI-powered HR virtual assistant using GPT-based LLMs, deployed on Microsoft Teams, automating \\nemployee queries on leave, policy, and payroll; reduced HR helpdesk tickets and improved query resolution speed by \\n60%. \\n‚Ä¢ Developed a ML model using Random Forest and XGBoost in Azure ML to predict employee attrition based on tenure, \\nperformance metrics, and engagement scores, achieving 84% accuracy and enabling HR to reduce high-risk exits. \\n‚Ä¢ Built a resume screening system using NLP to extract structured data (skills, education, roles) from resumes and match \\nthem against job descriptions, improving the shortlisting process and enabling efficient candidate evaluation. \\n‚Ä¢ Streamlined data pipelines using Azure Data Factory to automate ingestion and transformation of employee data from \\nmultiple sources, improving data freshness and reducing manual ETL effort. \\n‚Ä¢ Built and deployed a training recommendation engine using Azure ML and Python (Pandas, scikit-learn), combining',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_132'}},\n",
       " {'text': 'multiple sources, improving data freshness and reducing manual ETL effort. \\n‚Ä¢ Built and deployed a training recommendation engine using Azure ML and Python (Pandas, scikit-learn), combining \\ncollaborative and skill-based filtering to suggest tailored certification paths for employees, integrated with a Power BI \\ndashboard for tracking engagement and completion. \\n‚Ä¢ Designed and deployed enterprise-grade apps using Power Apps and Power Pages to digitize HR workflows, \\nintegrating Power Fx logic and backend data from Dataverse and Azure SQL. \\n‚Ä¢ Developed comprehensive HR dashboards in Power BI and Tableau, such as Evaluation, Mobility, Resignation, Career, \\nand Talent Acquisition‚Äîby working closely with CHRO, HR COO, and HR Directors across APAC, reducing report \\npreparation time by 90% and helping senior HR leaders make informed decisions. \\n‚Ä¢ Optimized SQL queries for data extraction, transformation, and performance, improving query efficiency by 40%',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_133'}},\n",
       " {'text': 'preparation time by 90% and helping senior HR leaders make informed decisions. \\n‚Ä¢ Optimized SQL queries for data extraction, transformation, and performance, improving query efficiency by 40% \\nthrough indexing, query restructuring, and optimization techniques, enhancing reporting accuracy by 30% on large HR \\ndatasets. \\nWValue Martech                Bengaluru, India \\nData Analyst Manager                Feb‚Äô24-Mar‚Äô24 \\n‚Ä¢ Simplified reporting processes for campaign performance analysis using Power Query and SQL, reducing manual \\nreport generation time by 50% and enabling marketing teams to act on insights 30% faster. \\nAdcanopus Digital Media                                                                                                         Bengaluru, India \\nLead Data Analyst               Jan‚Äô23-Feb‚Äô24 \\n‚Ä¢ Built a click fraud detection model using Logistic Regression and Isolation Forest in Python, analyzing user',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_134'}},\n",
       " {'text': 'Lead Data Analyst               Jan‚Äô23-Feb‚Äô24 \\n‚Ä¢ Built a click fraud detection model using Logistic Regression and Isolation Forest in Python, analyzing user \\nbehavior patterns, bounce rates, and device fingerprints. Integrated model results with campaign dashboards to flag \\nsuspicious traffic sources, improving ad spend efficiency by 18%. \\n‚Ä¢ Conducted A/B testing on creative strategies and landing page changes; drove 20% improvement in campaign \\nconversions through statistical impact analysis. \\n‚Ä¢ Developed Power BI dashboards to analyze key performance indicators like CTR, conversion rates, ROI, and CPA for \\nBanking and Financial Services clients, leading to 25% improvement in campaign performance. \\n‚Ä¢ Utilized Azure Data Factory to integrate and transform large datasets from multiple sources using SQL-based \\nworkflows, ensuring seamless data flow into Power BI for real-time analytics.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_135'}},\n",
       " {'text': 'BYJU‚ÄôS               Bengaluru, India \\nSenior Data Analyst               Jan‚Äô21-Dec‚Äô22 \\n‚Ä¢ Optimized student engagement analytics by designing and implementing custom Power BI dashboards, providing real-\\ntime insights that led to 35% enhancement in data accessibility and quicker decision-making for marketing strategies. \\n‚Ä¢ Collaborated with data engineers, marketing, and product teams to extract, clean, and structure large-scale student \\nengagement data from SQL databases, ensuring high data quality and consistency for dashboard development and \\nstrategic analysis. \\n \\nElegant Tech Secure (Hitachi Global)               Bhopal, India \\nAssociate Data Analyst               Aug‚Äô20-Dec‚Äô20 \\n‚Ä¢ Built SQL-based reporting to identify key sales and customer behavior trends in electronics data; insights supported a \\n15% revenue increase in targeted categories. \\nProjects \\nAI-Driven Internal Job Mobility Predictor [Python| Azure ML| Power BI| SQL]                                       Jan‚Äô25-Mar‚Äô25',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_136'}},\n",
       " {'text': '15% revenue increase in targeted categories. \\nProjects \\nAI-Driven Internal Job Mobility Predictor [Python| Azure ML| Power BI| SQL]                                       Jan‚Äô25-Mar‚Äô25 \\n‚Ä¢ Overview: Built an XGBoost model in Azure ML to predict internal job mobility using features like skills, training, tenure, \\nand performance. Integrated predictions and driver analysis into Power BI for department-level insights. \\n‚Ä¢ Outcome: Improved visibility into internal mobility by 25% and reduced open role fulfillment time by identifying eligible \\ninternal candidates faster. \\nCampaign Budget Reallocation Engine [Power BI| Power Automate]                                                    Jul‚Äô24-Sep‚Äô24 \\n‚Ä¢ Overview: Built a Power BI dashboard with Power Automate alerts to track real-time campaign spend vs performance. \\nTriggered weekly optimization prompts when CPA exceeded set thresholds. \\n‚Ä¢ Outcome: Enabled faster budget reallocation decisions, improving ROI by 14% over a quarter and reducing',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_137'}},\n",
       " {'text': 'Triggered weekly optimization prompts when CPA exceeded set thresholds. \\n‚Ä¢ Outcome: Enabled faster budget reallocation decisions, improving ROI by 14% over a quarter and reducing \\noverspending on underperforming channels by 20%. \\nEducation \\nMBA: Marketing Management                                                                                                              Jul‚Äô21‚ÄìJul‚Äô23 \\nNarsee Monjee Institute of Management Studies - Mumbai \\nGPA: 7.6 \\nB.E: Electronics and Communication Engineering                 Aug‚Äô15‚ÄìMay‚Äô19 \\nLakshmi Narain College of Technology & Science ‚Äì Bhopal \\nGPA: 8.3',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Preeti_Moolani_Resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_138'}},\n",
       " {'text': 'Anutosh Trivedi \\nData Scientist \\n        \\n+91-9145728977 | trivedianutosh26@gmail.com \\n \\nSummary \\n‚Ä¢ More than 3 years of cross functional experience in Data Analysis and Big Data tools. \\n‚Ä¢ Extensive experience in Python, Spark, Data Science and SQL programming for \\nexecution of data pre-processing and ETL operations. \\n‚Ä¢ Experience in delivering high impact business solutions through developing \\nAutomated scripts, and running Ad-hoc analysis \\n‚Ä¢ Experienced in providing Analytical and Optimized solution to complex problems \\nand challenges \\n‚Ä¢ Experienced in Data preprocessing and python scripts to create reusable functions \\n \\nWork Experience \\n‚Ä¢ Data Scientist | Gramener Technology Solutions | MARCH 2021 ‚Äì Present \\n‚Ä¢ Associate ‚Äì Big Data | Celebal Technologies | AUG 2020 ‚Äì FEB 2021 \\n‚Ä¢ Associate Engineer | Sears Holdings India | AUG 2017 - MAY 2019 \\n \\n  ML Algorithms: Linear and logistic regression, Random Forest, Decision Tree, KNN, PCA,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_Anutosh_Trivedi_21032022.pdf',\n",
       "   'chunk_id': 'CV_chunk_139'}},\n",
       " {'text': '‚Ä¢ Associate Engineer | Sears Holdings India | AUG 2017 - MAY 2019 \\n \\n  ML Algorithms: Linear and logistic regression, Random Forest, Decision Tree, KNN, PCA, \\nClustering Analysis, K-Means and K-Medoids, Agglomerative, Bagging and Boosting, Time \\nSeries Analysis, Feature Engineering, ML Lifecycle, Model Evaluation metrics.           \\n \\nTechnical Skills \\n‚Ä¢ Python, Pandas, NumPy, \\nsklearn, pulp, mlflow \\n‚Ä¢ Docker, VS code, Excel \\n‚Ä¢ MongoDB, MySQL \\n‚Ä¢ Machine Learning \\n‚Ä¢ Big Data Analytics \\n‚Ä¢ PySpark - Databricks \\n \\nSoftware \\nAnalytical Languages: Python \\n  \\nVery good \\nCloud Platform \\n \\n \\n Good \\nEDA \\n  Good \\nBig Data tools \\n  Good \\nSQL   \\n  Very good \\n \\nNon-technical Skills \\n‚Ä¢ Problem Solving and \\nAnalytical Thinking \\n‚Ä¢ Risk-taking, Decision \\nmaking and Time \\nManagement \\n‚Ä¢ Constructive Criticism \\nand Critical Thinking \\n‚Ä¢ Creativity and \\nLogical Reasoning \\n \\nDomain: \\n‚Ä¢ E-Commerce \\n‚Ä¢ Logistics \\n‚Ä¢ Pharmaceuticals \\n \\nAcademia \\n‚Ä¢ B. Tech in Electronics \\nand Communication \\nEngineering - 2016',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_Anutosh_Trivedi_21032022.pdf',\n",
       "   'chunk_id': 'CV_chunk_140'}},\n",
       " {'text': 'and Critical Thinking \\n‚Ä¢ Creativity and \\nLogical Reasoning \\n \\nDomain: \\n‚Ä¢ E-Commerce \\n‚Ä¢ Logistics \\n‚Ä¢ Pharmaceuticals \\n \\nAcademia \\n‚Ä¢ B. Tech in Electronics \\nand Communication \\nEngineering - 2016 \\n‚Ä¢ PGP in Data Science \\nEngineering - 2020 \\n \\nRelevant Projects and Responsibilities: \\nDubai Tourism: \\n‚Ä¢ Performed end to end exploratory Data Analysis on the large volume of raw data \\nin the form of csv files using pandas.   \\n‚Ä¢ Developed the python script to automate data preprocessing phase which led \\nto achieve simplified and efficient way to perform EDA on cleaned data. \\n‚Ä¢ Marketing Spend Optimization for Dubai Shopping Festival using custom \\nfunctions along with pulp library. \\nAlcon: \\n‚Ä¢ Developed python script to fetch data from MongoDB to pandas using Json \\nnormalization and parallel processing. \\n‚Ä¢ Developed different key metrices through data transformation skills which helped \\nin identifying various business impacting areas.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_Anutosh_Trivedi_21032022.pdf',\n",
       "   'chunk_id': 'CV_chunk_141'}},\n",
       " {'text': 'normalization and parallel processing. \\n‚Ä¢ Developed different key metrices through data transformation skills which helped \\nin identifying various business impacting areas.  \\n‚Ä¢ Created script to generate weekly automated reports on account misuse by ECPs \\nwhich reduced time utilized from a day to just 2 hours. \\n‚Ä¢ Clustering of ECPs and US States on different revenue and orders trend metrics. \\nWonder Cement: \\n‚Ä¢ Developed optimal solutions for converting complex Stored procedures in Spark  \\nafter optimization using the knack of SQL and PySpark in Azure DataBricks. \\n‚Ä¢ Written custom UDFs in PySpark and worked on solving data inconsistency \\nissues through debugging skills. \\nAEML: \\n‚Ä¢ Developed several scripts in python to fetch data from Facebook Graph APIs and \\npush the data in Sql Server to be used further for Sentimental  Analysis. \\n‚Ä¢ Developed Python scripts to fetch Social Media data from Twitter APIs and push the \\ndata in Sql Server after batch processing the huge data. \\n IMPACT:',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_Anutosh_Trivedi_21032022.pdf',\n",
       "   'chunk_id': 'CV_chunk_142'}},\n",
       " {'text': '‚Ä¢ Developed Python scripts to fetch Social Media data from Twitter APIs and push the \\ndata in Sql Server after batch processing the huge data. \\n IMPACT: \\n‚Ä¢ Extracted data from multiple sources using MySQL queries and combining them \\nfor extensive data analysis (EDA). \\n‚Ä¢ Web-scraping in Python to fetch metadata of the products from the product URLs \\nfor predictive model building \\n‚Ä¢ Developed Python scripts as a part of utility functions for re-usability, automation, \\nand optimization of the code in the data pre-processing phase \\n‚Ä¢ Optimized script to increase model accuracy by 5% by reducing data redundancy',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_Anutosh_Trivedi_21032022.pdf',\n",
       "   'chunk_id': 'CV_chunk_143'}},\n",
       " {'text': \"AWARDS  AND RE COGNI T I ON\\nRecognised for exceptional achievement in Q2'22.\\nCompleted NSE Certification on Financial Market,\\nInvestment Analysis and Portfolio Management\\nModule (2018).\\nAwarded for outstanding performance in Q4'20 and\\nQ1'21.\\ndevanshig291@gmail.com\\nCONT ACT\\nT OOL S  AND S KI L L S\\nR, SQL, Scala, Pig, Hive, MS\\nOffice\\nDatabricks, R Shiny,\\nQuicksight, Tableau\\nWorked on econometric\\nmodels, panel and time series\\ndata.\\nDEVANSHI GUPTA\\nE DUCAT I ON\\nM.A. Economics (2017-2019):\\nMadras School of Economics \\nCGPA - 8.33/10\\nB.A. (H) Economics (2014-2017)\\nUniversity of Delhi \\n76%\\nSenior Secondary (2014)\\nDAV Public School Faridabad\\n95%\\nHigh School (2012)\\nDAV Public School Faridabad\\nCGPA - 10/10\\nP E RSONAL  P ROF I L E\\nHighly motivated analytics\\nprofessional with robust problem\\nsolving skills and a proven record\\nof delivering impactful solutions\\nto drive business growth,\\nCollaborative team player with a\\ncontinuous learning mindset.\\nWORK E XP E RI E NCE\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_DevanshiGupta_1.pdf',\n",
       "   'chunk_id': 'CV_chunk_144'}},\n",
       " {'text': 'solving skills and a proven record\\nof delivering impactful solutions\\nto drive business growth,\\nCollaborative team player with a\\ncontinuous learning mindset.\\nWORK E XP E RI E NCE\\nDeveloped a cost cutting, effort based pricing strategy\\nfor last mile delivery agents.\\nAutomate P&L statement to track costs, revenues, and\\nkey metrics on a daily basis for comprehensive analysis\\nof business.\\nAnalysing revenue-impacting weight-related gaps and\\nproposing corrective measures..\\nIdentified fraudulent activities among delivery agents\\nand suggesting ways to minimise them. \\nForecasting ETA of a package, taking into account the\\nnetwork delays, to enhance customer experience\\nAnalyse and optimise last-mile operations to reduce\\nstress and enhance service levels at facilities \\nMonitoring business performance and provide\\nactionable insights for continuous improvement.\\n[24]7.ai | Analytics Consultant\\nJuly 2019- Sept 2021\\n+91-9717755669\\nE XT RA CURRI CUL AR\\nP ROJ E CT S  AND I NT E RNS HI P S',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_DevanshiGupta_1.pdf',\n",
       "   'chunk_id': 'CV_chunk_145'}},\n",
       " {'text': \"actionable insights for continuous improvement.\\n[24]7.ai | Analytics Consultant\\nJuly 2019- Sept 2021\\n+91-9717755669\\nE XT RA CURRI CUL AR\\nP ROJ E CT S  AND I NT E RNS HI P S\\nPresented 'Analysis of Inflation Expectations Survey of  \\nHouseholds in India' at RBI Staff College, Chennai.  The\\nmonetary stance suggested in the paper aligned with\\nthe decision of RBI Monetary Policy Committee.  \\nSubmitted research paper on 'Role of Fintech to\\nrevolutionise access to finance in India'  as a part of RBI\\nMonetary Policy Challenge 2018. The paper was awarded\\nas the best regional entry.\\nInterned at CodeCrunch Techlabs (Mar'18- May'18) as a\\nContent Creator to research on the emergence of\\ncryptocurrencies, understanding their functionality and\\nthe future state of the digital currencies.Core team member of Sports  \\nCommittee, 24]7.ai\\nMember, Research Cell, Madras\\nSchool of Economics\\nPresident at Department of\\nEconomics, Maitryi College (DU)\\nTreasurer at Department of\\nEconomics, Maitryi College (DU)\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_DevanshiGupta_1.pdf',\n",
       "   'chunk_id': 'CV_chunk_146'}},\n",
       " {'text': \"Committee, 24]7.ai\\nMember, Research Cell, Madras\\nSchool of Economics\\nPresident at Department of\\nEconomics, Maitryi College (DU)\\nTreasurer at Department of\\nEconomics, Maitryi College (DU)\\nwww.linkedin.com/in/devanshi-\\ngupta\\nDelhivery | Senior Analyst\\nOptimising call flows by analysing caller behaviour for\\nmore engagement and better customer experience.\\nBuilt NLP models using internal tools and ML\\ntechniques to identify root intent for quick onboarding\\nof new client in the same domain.\\nBuilt predictive models using logistic regression to\\nanticipate customers' likelihood and optimise chat\\nfunnel to maximise conversion rate.\\nRegular reporting of key customer metrics, analysing\\ndips and provide insights to drive customer satisfaction.\\nOct 2021 - present\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_DevanshiGupta_1.pdf',\n",
       "   'chunk_id': 'CV_chunk_147'}},\n",
       " {'text': 'IbtesamAhmed\\nibtesamahmex@gmail.com|+91-9630598405|https://www.linkedin.com/in/ibtesam-ahmed/|https://github.com/Ibtastic\\nEDUCA TION BachelorofEngineeringinComputerScience 2015‚Äì2019UniversityInstituteofTechnology,RGPV,C G P A ‚Äì 8 . 0 6 \\n EXPERIENCE \\n SeniorDataScientist August2021‚ÄìPresentRebelFoods,Mumbai,India',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ds_ibtesam_ahmed.pdf',\n",
       "   'chunk_id': 'CV_chunk_148'}},\n",
       " {'text': 'PredictEstimatedTimeofArrival:UsedfeatureengineeringandtreebasedmodelstopredicttheETAwhenacustomerordersfoodfromtheappanddynamicallyupdatesit.DesignedanABtestthatshowedthemodeldecreasedRMSEby5%andreducedWIMO(Whereismyorder)by10%.AlsousedinterpretationmodelslikeLIMEandShapleyandledcallstohelpbusinessteamsunderstandblack-boxmodelsbetter.OptimizedanddeployedthemodelinproductionusingAWSECS,Docker,Redis,FlaskandElasticsearch.RetentionOptimisation:DevelopedanMLbasedstrategythatpredictswhetheracustomerwillorderinthenextmonthbasedonfactorslikedayssincelastorder,CXofthelastorder,engagementontheappetcandthensendtherightcommunicationtothecustomerswhowon‚Äôt.ABtestedthisagainstarulebasedlogicrunbythebusinessteam.Increasedretentionby20%whilealsoincreasingprofitsby10%.PushNotificationPersonalization:PersonalizePNstocustomersbypersonalizingdifferentcomponentsliketherestaurant,discountandtime.Leadmultipleproposalcallstopitchthisideatothebusinessteam.Leadandmentoredteammembersinproblemformulation,dataex',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ds_ibtesam_ahmed.pdf',\n",
       "   'chunk_id': 'CV_chunk_149'}},\n",
       " {'text': 'onalizePNstocustomersbypersonalizingdifferentcomponentsliketherestaurant,discountandtime.Leadmultipleproposalcallstopitchthisideatothebusinessteam.Leadandmentoredteammembersinproblemformulation,dataextractionanddecidingthemetrics.RanmultipleABtestswithiterations.Thisledto10%increaseinCTRand5%increaseinconversion.CustomercontactsForecasting-DevelopedanddeployedanMLmodelwhichforecaststhecountofcontactsforagivendayatanhourlevel,acrosscontactscategories.Thisforecastisakeyinputforworkforceplanningandoptimization.Ithelpedinincreasingworkforcethroughputby5%aswellasreducingavgwait-timeby9%.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ds_ibtesam_ahmed.pdf',\n",
       "   'chunk_id': 'CV_chunk_150'}},\n",
       " {'text': 'DataScientist June2019‚ÄìJuly2021ForkMediaGroup,Mumbai,IndiaMultilingualTextClassifier:WorkedendtoendinbuildingamultilingualtextclassifierusingNaturalLanguageProcessingspecificallywordembeddingsandstatisticalmodelingtechniqueforclassification.ItisbuiltusingPysparkonAzureDatabricks.ItincreasedtheCTRonadsby20%.BrandSafetyImageClassification:UsedComputervisiontodevelopanimageclassificationmodelforflaggingNSFWimagesinnewsarticlesusingInceptionV3andobjectdetectiontechniques.Itincreasedaccuracyby20%andrecallby30%.MentoredinternstounderstandMLpipelines,problemsandframeworksbetter.PROJECTS ( M o r e a t h t t p s : / / w w w . k a g g l e . c o m / i b t e s a m a / n o t e b o o k s ) \\n -',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ds_ibtesam_ahmed.pdf',\n",
       "   'chunk_id': 'CV_chunk_151'}},\n",
       " {'text': '-\\nJigsawToxicCommentClassificationChallenge:Landedintop5%ontheleaderboard‚ÄìusedXLMRobertaandtechniqueslikepretrainingMLM,LabelSmoothing,MultiSampleDropoutsetc.MelanomaClassificationwithAttention:UsedAttentionMechanismwithVGG-16tomakethemodelfocusonimportantpartsoftheimagescontainingmelanomacancer.Italsoaddsinterpretabilityanddecreasesthenumberofparameters.MachineLearningInterpretability:GaveatalkaboutthisonKaggleDaysMeetup.Alsoimplementedanotebook,exploringmodel-agnosticinterpretationmethodsandlibrarieslikePDPplots,LIMEandSHAPfortext,tabularandimagedata.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ds_ibtesam_ahmed.pdf',\n",
       "   'chunk_id': 'CV_chunk_152'}},\n",
       " {'text': \"SKILLS \\n Programming: Python,SQL,NoSQLLibraries: Tensorflow,Pytorch,Scikit-learn,HuggingFace,Statsmodels,Gensim,PysparkTools: Tableau,AWS,Azure,MongoDB,Docker,GitNon-TechnicalSkills Communication,Leadership,SelfStarter TeamWork,Strategicthinking\\nACHIEVEMENTS - KaggleNotebooksMaster.Ifallinthetop0.1%ofKagglersWorldwide.- MynotebookonGenerativeAdversarialNetworksispublishedbyO'reillyMedia.- HavegivenmultipletalksonDataScienceandKaggleinconferencesanduniversities.- FreelancedasatechnicalwriterforcompanieslikePaperspaceandProjectPro.\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ds_ibtesam_ahmed.pdf',\n",
       "   'chunk_id': 'CV_chunk_153'}},\n",
       " {'text': 'PROFESSIONAL EXPERIENCE \\n  Moid Hassan \\nhas.moid@gmail.com  | +9178145-77236 \\nData Science professional with 5+ years of experience in helping organizations to harness the power of data and make \\ninformed decisions with an inquisitive mind and technical abilities. Manages entire lifecycle of data science solutions which \\nincludes designing project pipelines, conducting experiments, building machine learning models and evaluating them.  Good \\nunderstanding of ML algorithms and hands-on experience in building production-ready scalable ML/AI products from scratch. \\n \\nData Scientist II ‚Äì Cloud + AI Nov ‚Äò21 ‚Äì Present \\n‚Ä¢ Developed a scalable time series forecasting model using Temporal Fusion Transformers (TFT) as a challenger model \\nagainst the FB Prophet model in production. For 200k-300k time series TFT model was able to give 30% lift in metrics \\nsuch as Mean Absol ute Error ( MAE) and Symmetric Mean Absolute Percenta ge Error ( SMAPE) across 7 days of',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_154'}},\n",
       " {'text': 'such as Mean Absol ute Error ( MAE) and Symmetric Mean Absolute Percenta ge Error ( SMAPE) across 7 days of \\nforecasted horizon. The challenger model cut the training and inference time to 1/3 of the  production model on the \\nsame compute cluster. The challenger model was developed using Pytorch framework. Presented the findings to a peer \\nreview grou p of 20 + colleagues and the solution was adop ted by 3 t eams. This model is cu rrently being used for \\nAnomaly detection. \\n \\n‚Ä¢ Created a service of causal attribution. The goal was to bu ild a servi ce using which the end user can build  a causal \\nattribution model without w riting much code, using REST APIs in batch m ode. Input data contract was establis hed, \\ndocumentation for the service , and a demo jupyter notebook was provided for the end -user. The service uses meta-\\nlearners models for attribution, lightgbm based classifier and is hosted on Azure. This service was launched under the',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_155'}},\n",
       " {'text': 'learners models for attribution, lightgbm based classifier and is hosted on Azure. This service was launched under the \\ncharter of Machine Learning as a Serv ice which can be leveraged by colleagues in other teams and customers of the \\ncompany.  \\n \\nData Scientist II ‚Äì Enterprise Data Platforms Sep ‚Äò16 ‚Äì Nov‚Äô21 \\n‚Ä¢ Developed and deployed multiple categories of predictive ML models (qualification, credit card propensity, uplift, \\ninterest-based, small business models) to help the partners to identify the right marketing actions for the right \\naudiences via digital channels (Display, Search, Affiliate) \\no Credit Worthiness model (probability to default on credit card) to target approvable prospects which led to \\n10%+ improvement in new application approval rates for Media channel.  \\no Collaborated with marketing, analytics, and platform teams to design, implement, and measure multiple \\ncampaigns.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_156'}},\n",
       " {'text': '10%+ improvement in new application approval rates for Media channel.  \\no Collaborated with marketing, analytics, and platform teams to design, implement, and measure multiple \\ncampaigns. \\no A/B testing was used to measure the performance of the campaigns along with t racking the business KPI like \\nnew application approval rates and model KPI such as F1-score. \\n \\n‚Ä¢ Built and deployed multiple predictive classification and regression ML models  like customer churn, customer \\nretain and acquisition using XGBoost to generate prac tical business insights and target lists for clients/advertisers \\nusing AXP closed loop data and purchase -based signals, enhanced by second - and third -party data. Performance \\nmetrics like F1 -score or MCC were used. These models aided in the generation of $1 2MM+ revenue for American \\nExpress. \\n \\n‚Ä¢ Build and deployed an Anomaly Detection product to provide scalable enterprise -wide data quality solution to',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_157'}},\n",
       " {'text': 'Express. \\n \\n‚Ä¢ Build and deployed an Anomaly Detection product to provide scalable enterprise -wide data quality solution to \\nroutinely validate data and generate alerts for data users. Techniques to handle Time Series data such as Proph et \\nmodel and EWMA, Unsupervised Learning techniques like Isolation Forest and statistical control charts methods such \\nas Nelson rules were used to create an Ensemble model with version control in place using Github.  \\n \\n‚Ä¢ Developed a Web Page categorization pipeline leveraging 100MM+ web pages to retrieve useful structured \\ninformation based on page content analysis to generate insights for partners/clients providing them a 360-degree view \\nof spend and browse behavior.  \\no Web scrapping of URLs, data extraction from scrapped webpages.  \\no Document embeddings were created of extracted data using techniques like Doc2Vec. \\no Clustering of document embeddings into topics using Hierarchical Clustering (Divisive K -Means) and Google \\nNLP API.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_158'}},\n",
       " {'text': 'o Clustering of document embeddings into topics using Hierarchical Clustering (Divisive K -Means) and Google \\nNLP API. \\no This capability was implemented in a distributed manner to handle more than 100 million URLs. \\no The intelligence of this capability as features in multiple predictive models led to 8% improvement in \\nperformance. \\n‚Ä¢ The intelligence from this capability was also used as insights for partners providing them a 360-degree view of spend \\nand browse behavior. \\n \\n \\nMicrosoft (Noida) \\nAmerican Express (Gurgaon)',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_159'}},\n",
       " {'text': '‚Ä¢ Performed a deep dive exploratory data analysis of various internal and external complex, high volume data from \\nvarying sources to enhance feature engineering in multiple Machine Learning mode ls. \\no Conducted statistical analysis based on stakeholder‚Äôs requirements which included gathering and processing of \\ndata from various sources and delivering the insights for informed business decisions. \\no Sources like bureau, demographic, spend, firmographic, mobile, social were onboarded on a 3rd party platform \\nto perform the analysis. \\no Created an ETL/ELT pipeline which onboarded all the 2nd/3rd party data prospect data. The pipeline provided \\nmultiple reports via dashboard/visualization about the quality of data hen ce increasing the trust of data \\nscientists, data analysts and engineers around the data. The pipeline was adopted by 150+ users across 3 \\nbusiness units. \\no Created a rule-based analytical tool Data Schema Inference which assigns a data type (numerical, categorical,',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_160'}},\n",
       " {'text': 'business units. \\no Created a rule-based analytical tool Data Schema Inference which assigns a data type (numerical, categorical, \\ndate, ID) by looking at the variable values of these data sources.  \\no The algorithm is implemented in Hadoop Streaming (Map-Reduce) and Pyspark making it distributed and \\nscalable. \\no Reduced the time taken by an analyst to evaluate the data source from 2 days to 3-4 hours. \\n \\n‚Ä¢ Enhanced an Auto-ML product democratizing the ML model development process, empowering everyone, no matter \\ntheir expertise, to identify an end-to-end machine learning pipeline for any target metric.  \\no Users could run the models using a UI. \\no The pipeline includes feature engineering, feature extrapolation, feature selection, hyperparameter tuning \\nusing Bayesian model building and validation.  \\no The capability reduced the efforts of creating a ML model from 1 day to 2 -3 hours.  \\no The capability till now has served almost 1000+ models and 100+ internal/external users.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_161'}},\n",
       " {'text': 'o The capability reduced the efforts of creating a ML model from 1 day to 2 -3 hours.  \\no The capability till now has served almost 1000+ models and 100+ internal/external users.  \\no Used Python and Spark. \\n \\n‚Ä¢ Successfully built the backend of organization wide Machine Learning Hackathon with 1000+ employees similar to \\nKaggle. \\n \\n  Recognition and Responsibilities  \\n‚Ä¢ Received ‚ÄúStar Performer‚Äù, American Express, March 2020. \\n‚Ä¢ Awarded ‚ÄòAnalyst of the Quarter‚Äô in Q2 2017, Q3 2018, Q2 2019. \\n‚Ä¢ Guided the team of 50+ people to successfully shift from SAS to Spark and Hadoop Streaming by hosting multiple \\ninfo-share sessions across Gurgaon, Bangalore, Phoenix and New York locations.  \\n \\nSKILLS \\n                          TECHNICAL SKILLS                          SOFT SKILLS \\nMachine Learning, Deep Learning, NLP \\nProgramming Languages and Libraries ‚Äì Python, C++, SQL, \\nTensorFlow and Pytorch \\nDistributed Frameworks ‚Äì Pyspark, Hadoop Map-Reduce, Hive',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_162'}},\n",
       " {'text': 'Machine Learning, Deep Learning, NLP \\nProgramming Languages and Libraries ‚Äì Python, C++, SQL, \\nTensorFlow and Pytorch \\nDistributed Frameworks ‚Äì Pyspark, Hadoop Map-Reduce, Hive \\nCloud Technologies ‚Äì Azure VM, Az ure Storage, A zure \\nFunctions \\nVersion Control ‚Äì Git and Github   \\nProblem solving and creative thinking.        \\nInsight generation and storytelling. \\nPartner management and collaboration. \\n \\n                  EDUCATION \\n                Institute                         Degree CGPA Year \\nPanjab University (UIET) Bachelor of Engineering  8.1 2016 \\n              CERTIFICATIONS  \\n‚Ä¢ Deep Learning Specialization | Andrew Ng | Coursera \\n‚Ä¢ Big Data Environment: HDFS, MapReduce and Spark | Yandex | Coursera',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Resume_MoidHassan_DataScientist_M52022.pdf',\n",
       "   'chunk_id': 'CV_chunk_163'}},\n",
       " {'text': 'DATA SCIENTIST 2 |\\nKAGGLE 2X EXPERT\\nRAVINDER\\nKUMAR\\xa0\\nTANWAR\\nrtanwar616@gmail.com\\nhttps://www.kaggle.com\\n/ravijoe\\n+918847095358\\nhttps://www.linkedin.com\\n/in/ravi-tanwar-12bb3811a/\\nhttps://github.com/ravijoe/\\n\\uf0e0\\n\\uf0ac\\n\\uf095\\n\\uf041 Bengaluru,Karnataka\\n\\uf0e1\\n\\uf09b\\nSkills\\nPROGRAMMING\\nPython\\nFlask\\nPyTorch\\nNumpy\\nSciPy\\nMatplotlib\\nSeaborn\\nScikit-Learn\\nDocker\\nAPI\\nData Structures\\nAUTOMATION / WEB SCRAPING\\nSelenium\\nScrapy\\nBeautifulSoup\\nSTATISTICS AND PROBABILITY\\nStatistics\\nProbability Basics\\nRandom Variables\\nProbability Distributions\\nHypothesis Testing\\nModelling\\nPRODUCT METRICS\\nA/B Testing\\nMACHINE LEARNING/DEEP LEARNING\\nK Nearest Neighbors\\nRandom Forest\\nLinear and Logistic Regression\\nNLP\\nDATA VISUALIZATION\\nTableau\\nPowerBI\\nPlotly\\nCLOUD COMPUTING\\nAWS\\nDatabricks\\nBIG DATA\\nDatabricks\\nPySpark\\nEducation\\nPEC University Of Technology May 2015 to Apr. 2019\\nBachelor Of Engineering - Computer Science\\nSpringboard Data Science Career\\nTrack\\nAug. 2019 to Feb.\\n2020\\nCompleted a 6 month Data Science Career Track that',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_164'}},\n",
       " {'text': 'Bachelor Of Engineering - Computer Science\\nSpringboard Data Science Career\\nTrack\\nAug. 2019 to Feb.\\n2020\\nCompleted a 6 month Data Science Career Track that\\nincludes 650+ hours of hands-on curriculum, with 1:1\\nindustry mentor oversight, and completion of 2 capstone\\nprojects. Mastering skills in Python, SQL, Data Analysis and\\nVisualization, Hypothesis Testing, and Machine Learning.\\nhttps:/ /www.linkedin.com/in/ravi-tanwar-12bb3811a\\n/detail/overlay-view/urn:li:fsd_proÔøΩleTreasuryMedia:\\n(ACoAAB22TbMBRg2Zo_J4IhcvP6ogMdec7Vd2bFo,1593288157466)/\\nEmployment\\nDun & Bradstreet Bengaluru\\nData Scientist II Aug. 2021 to Current\\n-Responsible for delivering Data Engineering and Data Science workÔøΩows.\\n-Working on automating ML Solutions using Python, PySpark and Databricks.\\n-Built models like Scorecard, Credit Limit and Reject Inference for Finance , Risk domain.\\n-Reduced manual eÔøΩort by 50%\\nGrazitti Interactive (SearchUnify) Panchkula\\nData Scientist Dec. 2019 to Aug. 2021',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_165'}},\n",
       " {'text': '-Reduced manual eÔøΩort by 50%\\nGrazitti Interactive (SearchUnify) Panchkula\\nData Scientist Dec. 2019 to Aug. 2021\\n‚Ä¢ Started as a part of Analytics team . Worked in Data Fetching(using APIs) , Pre-Processing(using Python) and Reporting(using Tableau and Microsoft Power\\nBI)\\xa0\\n‚Ä¢ Mediums used include Google Analytics and Microsoft Dynamics365.\\n‚Ä¢ Currently leading NLP department of the team.\\n‚Ä¢ Was responsible for mentoring two juniors who completed POCs on Anomaly Detection and Next Sentence Prediction within 2 month timeframe\\n‚Ä¢ Responsible for hiring new Data Scientists for the team .\\n‚Ä¢ Worked on Unsupervised Text clustering methods, Embedding techniques like TÔøΩdf,Word2Vec,LSTSMs,Transformers,BERT,Longformers etc.\\n‚Ä¢ Worked on Recommendation System for recommending similar queries from a dataset size >1 million rows and reduced the memory used by previous\\nalgorithm by 2 folds',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_166'}},\n",
       " {'text': '‚Ä¢ Worked on Recommendation System for recommending similar queries from a dataset size >1 million rows and reduced the memory used by previous\\nalgorithm by 2 folds\\n‚Ä¢ Currently working on Anomaly Detection and Imbalanced ClassiÔøΩcation for detecting Escalated user tickets with Model giving 90+ Recall along with\\nproper Business understanding.\\nCode Garage Tech Mohali\\nPython Developer May 2019 to July 2019\\n‚Ä¢ Worked on Web Scraping of Dynamic Websites using Scrapy , BeautifulSoup , Selenium in Python .Some websites included one of the largest places for\\nwine collection and one of the largest State-wise school database for India\\xa0 .\\n‚Ä¢ Used python for automating tasks which included but not limited to quick conversion of word ÔøΩles into pdfs ,automated liking and deleting of posts on\\nFacebook.\\nCouncil of ScientiÔ¨Åc and Industrial Research-Central ScientiÔ¨Åc Instruments Organisation Chandigarh\\nIntern Jan. 2018 to June 2018',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_167'}},\n",
       " {'text': 'Facebook.\\nCouncil of ScientiÔ¨Åc and Industrial Research-Central ScientiÔ¨Åc Instruments Organisation Chandigarh\\nIntern Jan. 2018 to June 2018\\n‚Ä¢ Worked in the Biomedical Instrumentation unit at CSIR-CSIO Chandigarh and\\xa0\\n‚Ä¢ Took part in working toward Development Of Rehabilitation Tools using Unity3D. Positive feedback from doctors at GMCH ,Sector 32 was received\\nProjects\\nSong Popularity Prediction Aug. 2019 to Oct. 2019\\n‚Ä¢ Data-set was picked from Kaggle , which contained 250k songs collected using Spotify API.\\n‚Ä¢ Features included were acousticness , danceability, loudness ,energy etc.\\n‚Ä¢ Target variable was popularity between 0-100\\xa0\\n‚Ä¢ Model used was Random Forest .\\xa0\\n‚Ä¢ Shared the results with YouTube musicians and got positive feedback\\nMovie Recommendation System Sept. 2019 to Jan. 2020\\n‚Ä¢ Used TMDB dataset from Kaggle containing 5000 movies .\\n‚Ä¢ Demographic and Content based Techniques were used.\\n‚Ä¢ Cosine Similarity was used for ÔøΩnding similar movies .',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_168'}},\n",
       " {'text': \"‚Ä¢ Used TMDB dataset from Kaggle containing 5000 movies .\\n‚Ä¢ Demographic and Content based Techniques were used.\\n‚Ä¢ Cosine Similarity was used for ÔøΩnding similar movies .\\n‚Ä¢ Successfully deployed the model on Python Flask Framework\\n‚Ä¢ Posted the demo video on Linkedin and got 200+ likes and 10000+ views\\nNews and Stats + Dashboarding using Twilio and Tableau\\n‚Ä¢ Made an alert system for latest covid-19 news and stats .\\xa0\\n‚Ä¢ Used BeautifulSoup to scrap data from https:/ /www.worldometers.info/coronavirus/.\\xa0\\n‚Ä¢ Following the completion of data fetching the stats and news are sent to user's Whatsapp number using Twilio API.\\n‚Ä¢ Used the built product for self use and notiÔøΩcations ; got decent results\\nWeb App for identifying Fake News using NLP\\n‚Ä¢ Made a classiÔøΩcation system for identifying\\xa0 Fake News among a given list of news .\\xa0\\n‚Ä¢ After model training and testing ;deployed the same model using\\xa0 Flask Web Framework in Python.\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_169'}},\n",
       " {'text': \"‚Ä¢ Made a classiÔøΩcation system for identifying\\xa0 Fake News among a given list of news .\\xa0\\n‚Ä¢ After model training and testing ;deployed the same model using\\xa0 Flask Web Framework in Python.\\xa0\\n‚Ä¢ User can input a given text as News and get prediction telling whether news is fake or not .\\nSpeech Sentiment Analysis\\n‚Ä¢ Worked on a Voice Recognition system that provides the Sentiment for the speech.\\n‚Ä¢ Used PyAudio module to convert text to speech.\\n‚Ä¢ Converted Speech was then fed into VaderSentiment Analysis package which is\\xa0 in NLTK library.\\n‚Ä¢ Suggested the idea to present company and got approval for the same since Voice based methods methods improve security.\\nAwards\\nDatacamp ¬∑ Data Scientist with Python Track Dec. 2019\\nCourseraIntroduction to Data Science in Python - University Of Michigan\\nHackerrank ¬∑ 4 Hackerrank Skill CertiÔ¨Åcates\\nhttps:/ /www.hackerrank.com/certiÔøΩcates/01b077a48963\\nActivities\\nWrote article on 'Grammar Checking using BERT' which got featured by Analytics India Magazine\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_170'}},\n",
       " {'text': \"https:/ /www.hackerrank.com/certiÔøΩcates/01b077a48963\\nActivities\\nWrote article on 'Grammar Checking using BERT' which got featured by Analytics India Magazine\\nhttps:/ /www.linkedin.com/posts/ravi-tanwar-12bb3811a_how-to-use-bert-transformer-for-grammar-checking-activity-6704088266787971072-Dnmu\\nMade 10k+ followers on Linkedin by Posting Data Science related content\\nhttps:/ /www.linkedin.com/mynetwork/invite-connect/connections/\\nAchieved trending status for 3 posts related to #datascience\\nCollaboration with Ken Jee and Andrew Mao\\nCollaboration with Ken Jee(ranked as one of the top Data Scientists in the World) and Andrew Mao(ranked as one of the top Data Scientists in the World) for their YouTube\\nchannels https:/ /www.youtube.com/channel/UC23emuGbNM7twofQIrEgPBQ\",\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\resume_ravi_tanwar_DS_3YOE.pdf',\n",
       "   'chunk_id': 'CV_chunk_171'}},\n",
       " {'text': 'Rishika Mishra\\nData Professional\\n+91 70044 16502 | rishikamishra0404@gmail.com | linkedin.com/in/rishikamishra\\nAbout\\nData professional with 2+ years of experience in data engineering, analytics, and workÔ¨Çow automation.\\nProven ability to design scalable pipelines, reduce manual eÔ¨Äorts, and improve data accuracy across\\nÔ¨Ånancial systems. Adept in Python, SQL, SnowÔ¨Çake, AutoSys, and SAP BO, with a consistent record\\nof improving operational eÔ¨Éciency and enabling data-driven decision-making.\\nWork Experience\\nData Engineer (Consultant at Morgan Stanley) Wissen Technology, Bangalore, India\\n2023 ‚Äì Present\\n‚Ä¢ Built 20+ automated pipelines in Python and SQL, reducing manual data processing time by 70%.\\n‚Ä¢ Optimized SnowÔ¨Çake queries, resulting in a 15% reduction in warehouse costs and improved per-\\nformance.\\n‚Ä¢ Maintained 99.9% uptime for batch job execution using AutoSys, Unix scripts, and real-time alert-\\ning.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Rishika_resume_tab.pdf',\n",
       "   'chunk_id': 'CV_chunk_172'}},\n",
       " {'text': 'formance.\\n‚Ä¢ Maintained 99.9% uptime for batch job execution using AutoSys, Unix scripts, and real-time alert-\\ning.\\n‚Ä¢ Delivered 15+ Ô¨Ånance and compliance reports monthly via SAP BO, enabling faster decision-\\nmaking across teams.\\n‚Ä¢ Led migration of legacy stored procedures to a SnowÔ¨Çake DAL, improving maintainability and\\ncutting runtime by 35%.\\nAssociate Data Analyst Hexaware Technologies, Chennai, India\\n2021 ‚Äì 2023\\n‚Ä¢ Developed SQL/Python-based dashboards that improved reporting eÔ¨Éciency by 25%.\\n‚Ä¢ Increased data accuracy to 98.5% by implementing validation rules and transformation logic.\\n‚Ä¢ Automated Excel and SAP BO reporting workÔ¨Çows, saving 8+ hours/week of manual eÔ¨Äort.\\n‚Ä¢ DeÔ¨Åned and tracked key KPIs in partnership with business stakeholders, leading to improved\\nvisibility and alignment.\\nEducation\\nMCA (Computer Applications) Birla Institute of Technology, Ranchi\\n2018 ‚Äì 2021\\nB.Sc. Mathematics (Hons) RVS College\\n2015 ‚Äì 2018\\nSkills\\nLanguages: Python, SQL, Shell Scripting, Git',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Rishika_resume_tab.pdf',\n",
       "   'chunk_id': 'CV_chunk_173'}},\n",
       " {'text': 'Education\\nMCA (Computer Applications) Birla Institute of Technology, Ranchi\\n2018 ‚Äì 2021\\nB.Sc. Mathematics (Hons) RVS College\\n2015 ‚Äì 2018\\nSkills\\nLanguages: Python, SQL, Shell Scripting, Git\\nReporting/BI: SAP BO (BOXI), Tableau, Excel (Advanced), KPI Reporting\\nLibraries: NumPy, Pandas, Matplotlib\\nPlatforms: SnowÔ¨Çake, DB2\\nWorkÔ¨Çow: AutoSys, Unix, CI/CD, Monitoring\\nFocus Areas: ETL, Data Modeling, Validation, Stakeholder Reporting, Statistics',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Rishika_resume_tab.pdf',\n",
       "   'chunk_id': 'CV_chunk_174'}},\n",
       " {'text': 'Saad Ahmad Mob. +91 9424533704\\nsaada22q@isid.ac.in\\nsaad1912@gmail.com\\nCourse College/University Year CGPA/%\\nM.S. Quantitative Economics Indian Statistical Institute, Delhi 2022-24 71.1\\nUndergrad IIITDM, Jabalpur 2014-18 7.8\\nWEBSITES\\n‚Ä¢ LinkedIn - https://www.linkedin.com/in/saad-ahmad-53150a240/\\nACHIEVEMENTS\\n‚Ä¢ AIR 1 : IIT-JAM (2022)\\n‚Ä¢ AIR 7 : ISI-MSQE Entrance (2022)\\n‚Ä¢ AIR 19 : GATE (2022)\\nWORK EXPERIENCE\\n‚Ä¢ Senior Analyst | Capgemini [Aug‚Äô18-Aug‚Äô21]\\n‚ó¶ Provided technical consultations to clients and end-users within the production team, ensuring effective communication\\nand problem-solving.\\n‚ó¶ Conducted data analysis, synthesis, and deployment on the production server, optimizing processes and enhancing\\nefficiency\\n‚ó¶ Leveraged SQL, WebMethods Integration Server, and Microsoft BizTalk for seamless data integration and manage-\\nment\\n‚ó¶ Offered comprehensive backend support to a diverse client base of over 50 customers, ensuring uninterrupted service\\ndelivery.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_175'}},\n",
       " {'text': 'ment\\n‚ó¶ Offered comprehensive backend support to a diverse client base of over 50 customers, ensuring uninterrupted service\\ndelivery.\\n‚ó¶ Conducted critical debugging analysis, resulting in the acceleration and enhancement of transaction volume and quality\\nfor clients\\n‚ó¶ Played a key role in the mentorship and on-boarding of junior team members, providing guidance and support to\\nfacilitate their integration and understanding of team dynamics and processes.\\nPUBLICATIONS\\n‚Ä¢ Environmental sound classification using optimum allocation sampling based empirical mode decomposition ( Physica A:\\nStatistical Mechanics and its Applications Volume 537, 1 January 2020, 122613)\\nSKILLS & INTERESTS\\n‚Ä¢ Programming Languages: Python , SQL, Stata\\n‚Ä¢ Interests: Econometrics, Statistics, Machine Learning, Deep Learning, Time Series\\n‚Ä¢ Tools & Libraries: Pandas, Numpy, Scikit-learn, Matplotlib, Seaborn, webMethods IS, Microsoft Biztalk Server, MS Office\\nPROJECTS',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_176'}},\n",
       " {'text': '‚Ä¢ Tools & Libraries: Pandas, Numpy, Scikit-learn, Matplotlib, Seaborn, webMethods IS, Microsoft Biztalk Server, MS Office\\nPROJECTS\\n‚Ä¢ Built Deep Neural Network from Scratch for Image Classification | Coursera [Feb‚Äô24-Mar‚Äô24]\\n‚ó¶ Developed a custom neural network model from scratch for image classification.\\n‚ó¶ Implemented parameter initialization (Random Initialization and He Initialization) for both 2-layer and L-layer net-\\nworks\\n‚ó¶ Constructed forward propagation with linear and activation functions (ReLU, Sigmoid)\\n‚ó¶ Implemented backward propagation with gradients of activation functions (ReLU, Sigmoid)\\n‚ó¶ Successfully trained neural network on cat/non-cat image dataset with improved accuracy:\\n* Unregularized model with gradient descent: 80 percent accuracy, up from72 percent using Logistic Regression.\\n* Regularized model with L2-Regularization and Dropout: 93 percent and 95 percent accuracy respectively on test\\ndata',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_177'}},\n",
       " {'text': '* Regularized model with L2-Regularization and Dropout: 93 percent and 95 percent accuracy respectively on test\\ndata\\n‚ó¶ Further enhanced model performance by implementing optimization techniques from scratch:',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_178'}},\n",
       " {'text': '* Gradient Descent with Momentum: Achieved 71 percent accuracy on a Flower Image Dataset\\n* Adam optimization: Achieved 94 percent accuracy on the same Flower Image Dataset.\\n(Skills: Neural Networks, Regularization, Optimization, Python)\\n‚Ä¢ Bankruptcy in Poland | Worldquant University [Feb‚Äô24-Feb‚Äô24]\\n‚ó¶ Led the development of Random Forest and Gradient Boosting models for corporate bankruptcy risk prediction\\n(Skills: Exploratory Data Analysis, Random Forest, Feature Engineering, Gradient Boosting)\\n‚Ä¢ Customer Segmentation in USA | Worldquant University [Jan‚Äô24-Feb‚Äô24]\\n‚ó¶ Led the development of a k-means clustering model to segment US consumers.\\n(Skills: Exploratory Data Analysis, Principal Component Analysis, k-means clustering)\\n‚Ä¢ Air Quality in Dar es Salaam | Worldquant University [Dec‚Äô23-Jan‚Äô24]\\n‚ó¶ Led the development of an ARMA time-series model to forecast particulate matter levels in Kenya',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_179'}},\n",
       " {'text': '‚Ä¢ Air Quality in Dar es Salaam | Worldquant University [Dec‚Äô23-Jan‚Äô24]\\n‚ó¶ Led the development of an ARMA time-series model to forecast particulate matter levels in Kenya\\n(Skills: Exploratory Data Analysis, Time Series Analysis, Hyperparameter Tuning, Feature Engineering)\\n‚Ä¢ Apartment Sales in Mexico City | Worldquant University [Nov‚Äô23-Dec‚Äô23]\\n‚ó¶ Led the development of a sophisticated Linear Regression model to forecast apartment prices within the Mexico real\\nestate market\\n(Skills: Exploratory Data Analysis, Linear Regression, Ridge Regression, OneHot Encoder, Pipelines )\\n‚Ä¢ Applied Data Science Capstone Project | Coursera [Jun‚Äô23-Jul‚Äô23]\\n‚ó¶ Developed data pipeline using web scraping for Falcon 9 rocket data, and built machine learning models to predict\\nFalcon 9 first-stage landing success.\\n‚ó¶ Test Accuracy of Logistic Regression : 0.833\\n‚ó¶ Test Accuracy of SVM : 0.842\\n‚ó¶ Test Accuracy of Decision Tree : 0.944\\n‚ó¶ Test Accuracy of KNN : 0.833',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_180'}},\n",
       " {'text': 'Falcon 9 first-stage landing success.\\n‚ó¶ Test Accuracy of Logistic Regression : 0.833\\n‚ó¶ Test Accuracy of SVM : 0.842\\n‚ó¶ Test Accuracy of Decision Tree : 0.944\\n‚ó¶ Test Accuracy of KNN : 0.833\\n(Skills: Exploratory Data Analysis, Logistic Regression, SVM, Decision Tree, KNN )\\n‚Ä¢ Impact of Attitude towards Dowry on Highest Female Education Attained in Indian Household‚Äî Dr. Abhiroop Mukhopad-\\nhyay| ISI Delhi [Mar‚Äô23-May‚Äô23]\\n‚ó¶ Created Attitude Towards Dowry Index (ATDI) using 22 variables in IHDS-2 dataset using PCA for dimensionality\\nreduction.\\n‚ó¶ Achieved R-squared of 0.4299 with a statistically significant positive ATDI coefficient.\\n(Skills: Exploratory Data Analysis, Principal Component Analysis, Linear Regression )\\nCERTIFICATION COURSES\\n‚Ä¢ Deep Learning Specialization (Ongoing) (Mar‚Äô24-Present) [Standford Online | DeepLearning.AI]\\n‚Ä¢ Machine Learning Specialization (Jul‚Äô23-Aug‚Äô23) [Standford Online | DeepLearning.AI]\\n‚Ä¢ Data Science Professional Certificate (Jun‚Äô23-Jul‚Äô23) [IBM]',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_181'}},\n",
       " {'text': '‚Ä¢ Machine Learning Specialization (Jul‚Äô23-Aug‚Äô23) [Standford Online | DeepLearning.AI]\\n‚Ä¢ Data Science Professional Certificate (Jun‚Äô23-Jul‚Äô23) [IBM]\\n‚Ä¢ Data Analyst Professional Certificate (May‚Äô23-Jun‚Äô23) [IBM]\\nRELEV ANT COURSEWORK\\n‚Ä¢ Probability and Statistics\\n‚Ä¢ Econometrics\\n‚Ä¢ Applied Econometrics\\n‚Ä¢ Mathematical Methods\\n‚Ä¢ Optimization Techniques\\n‚Ä¢ Time Series Analysis\\nPOSITIONS OF RESPONSIBILITY\\n‚Ä¢ Coordinator | IIITDMJ Football Club [Aug‚Äô16-Jul‚Äô17]\\n‚ó¶ Organized Inter and Intra college level football competitions, as well as managed regular activities of the football club\\nEXTRACURRICULAR ACTIVITIES\\n‚Ä¢ Football - Captained college team to First position at Gusto ‚Äô17 and ‚Äô18.\\n‚Ä¢ Quizzing - Won Sweden India Nobel Memorial Quiz‚Äô16 at IIM Indore. Won various Inter College level quiz competitions.\\n‚Ä¢ Reading/Writing - Published articles in college and school Annual Magazines. Like to read fiction and non-fiction books.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Saad_Ahmad_Res.pdf',\n",
       "   'chunk_id': 'CV_chunk_182'}},\n",
       " {'text': 'SAURABH KUMAR SHARMA  \\n+91 9711219543 | saurabh280294@gmail.com | 34-D, RBI Colony, Shalimar Bagh, Delhi 110088 \\n \\nS K I L L S \\nPython | PySpark | SQL | Econometrics | Modelling | Adobe Workspace | Hive | MS Office | SAS \\n \\nP R O F E S S I O N A L  E X P E R I E N C E \\nA M  - D a ta  S c ie n t is t | New applications, Intl Markets ‚Äì American Express, Gurugram                                                                            \\n10 /2 0 21  ‚Äì P r e s e n t \\n‚ñ™ Lead the development of Proxy Income model to predict income of card applications in India \\n‚Ä¢ Techniques used ‚Äì Boosting algorithm, SAS platform \\n‚Ä¢ Potential benefit ‚Äì $0.74Mn (2-year P re-Tax Income estimate  ‚Äì net credit los s \\nsaved+incremental billings) \\n‚Ä¢ An application universe of 600,000 credit card applications \\n \\nS e n io r  D a ta  S c i e n ti s t | Tesco UK ‚Äì Digital channel (Onsite) ‚Äì dunnhumby, Gurugram                                                                            \\n06 /2 0 1 9 ‚Äì 1 0 /2 0 21',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_183'}},\n",
       " {'text': 'S e n io r  D a ta  S c i e n ti s t | Tesco UK ‚Äì Digital channel (Onsite) ‚Äì dunnhumby, Gurugram                                                                            \\n06 /2 0 1 9 ‚Äì 1 0 /2 0 21 \\n‚ñ™ Created following features in Customer 360 project ‚Äì  \\n‚Ä¢ Online loyalty ‚Äì whether a customer is an avid online shopper \\n‚Ä¢ Spend on promotion ‚Äì how much customer spend is in promotional activities \\n‚Ä¢ Health score of shoppers ‚Äì whether a customer follows theoretically healthy lifestyle \\n‚Ä¢ Baby flag ‚Äì whether the customer is a parent based on their transaction data with Tesco UK \\n‚ñ™ Presented a project based on retrospective analysis of heuristic based audience v/s ML techniques-\\nbased audience to key stakeholders. Technique ‚Äì Measured RoAS for brand buyer audience v/s \\nan audience based upon k-means clustering \\n‚ñ™ Delivered 59 targeting files for personalized marketing campaigns on Google Ad Manager. Targeting \\ninvolves bespoke elements ‚Äì',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_184'}},\n",
       " {'text': 'an audience based upon k-means clustering \\n‚ñ™ Delivered 59 targeting files for personalized marketing campaigns on Google Ad Manager. Targeting \\ninvolves bespoke elements ‚Äì  \\n‚Ä¢ Bespoke science-based audience ‚Äì e.g. Propensity modelling for baby products with in -time \\nvalidation \\n‚Ä¢ Product relevancy for customers ‚Äì e.g. product affinity segmentation \\n‚Ä¢ AB testing of new product experimenter science with business rules \\n‚ñ™ Delivered 170 targeted campaigns‚Äô evaluations. Scope of evaluation covers ‚Äì  \\n‚Ä¢ Uplift due to campaign \\n‚Ä¢ Customer profiling \\n‚Ä¢ Conversion \\n‚Ä¢ Return on Ad Spend (ROAS) \\n‚ñ™ Set up automated dashboard on Adobe Workspace using custom metrics and segmentations \\n‚ñ™ Worked with Adobe Omniture reports to develop ad-hoc solutions for instance ‚Äì Tesco.com traffic \\nbehavior post 2nd round of lockdowns in UK \\n‚ñ™ Developed a targeted sampling solution on new tech stack (Python, PySpark). Introduced',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_185'}},\n",
       " {'text': 'behavior post 2nd round of lockdowns in UK \\n‚ñ™ Developed a targeted sampling solution on new tech stack (Python, PySpark). Introduced \\nResampling (t-test for good test and control match) and ANCOVA based uplift calculation  in \\ncampaign evaluation using scikit and statsmodels library \\n‚ñ™ Created health segmentation from scratch to profile customer into healthy, unhealthy lifestyle based \\non their basket and macro nutrients component in products using Logistic Regression \\n‚ñ™ Collaborated on project to measure long term impact of media on customers ‚Äì  \\n‚Ä¢ Customer were divided into different control groups for 3 different channels as ‚Äì fully held back \\ncontrol, single channel exposure and interaction effect  across channel',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_186'}},\n",
       " {'text': '‚Ä¢ Then incremental revenue per cust omer was measured and tested for significance using \\nANOVA using statsmodels library \\n‚ñ™ Consulting and UAT support to Global Products team.  Results ‚Äì launch of first campaign \\nevaluation product \\n \\nA n a ly s t ‚Äì In te r n | Heinen‚Äôs, USA ‚Äì dunnhumby India, Gurugram (erstwhile Gurgaon)                                                                 \\n05 /2 0 18  ‚Äì 07 /201 8 \\n‚ñ™ Developed an offer pool solution for retailer based on Machine Learning model ( Decision Trees + \\nRandom Forest Classifier). Results - 7% increment in offer redemption based on ANCOVA based \\nuplift measurement \\n‚ñ™ Recognized as the best intern in grad batch of 2019 \\n \\n \\nE n g li s h  T e a c h e r  -  V o lu n te e r |  T h e  W o r d s w o r th  P r o j e c t, D e lh i                                                                   \\n06 /2 0 1 6 ‚Äì 07 /2017  \\n‚ñ™ Worked with 6 kids and taught them to make small paragraphs in English about their everyday actions',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_187'}},\n",
       " {'text': '06 /2 0 1 6 ‚Äì 07 /2017  \\n‚ñ™ Worked with 6 kids and taught them to make small paragraphs in English about their everyday actions \\n \\nA C A D E M I C  P R O J E C T S \\n‚ñ™ Capstone Project ‚Äì A study of factors contributing to Air pollution in Delhi. End-to-end project which \\nincluded ‚Äì  \\n‚Ä¢ data cleaning and preparation \\n‚Ä¢ EDA ‚Äì Trend analysis, Correlation matrix, stationarity test \\n‚Ä¢ Final modelling ‚Äì dummy variables, heteroscedasticity robust Multi variable regression model \\nwith BLUE estimator s and post regression validation (heteroscedasticity, autocorrelation, \\nmulticollinearity) \\n‚ñ™ Econometrics Project ‚Äì A study of socio -economic factors impacting sporting performance of \\ncountries \\n‚Ä¢ Cross-country study over 2012 and 2016 Olympics. Incorporating panel regression measure \\nand Logistic Model \\n‚ñ™ Statistics Project ‚Äì Analysis of 30 pharmacy firms, using HT testing, ANOVA and independence test \\ncomplete with post regression validation \\n \\nE D U C A T I ON',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_188'}},\n",
       " {'text': 'and Logistic Model \\n‚ñ™ Statistics Project ‚Äì Analysis of 30 pharmacy firms, using HT testing, ANOVA and independence test \\ncomplete with post regression validation \\n \\nE D U C A T I ON \\nM a s t e r  o f B u s in e s s  E c o n o m ic s |  Department of Business Economics, University of Delhi                                                              \\nY e a r ‚Äì 2 0 1 9  |  F ir s t  D iv i s io n  (T o p  5  o u t  o f  5 5  s tu d e n t s ) \\nB a c h e lo r  o f  T e c h no lo g y  (ME )  |  Northern India Engineering College, GGSIPU, Delhi                                                              \\nY e a r  ‚Äì 2 0 1 6  |  F ir s t  D iv i s io n \\nS e n io r  S e c o n d a r y  | Rajkiya Pratibha Vikas Vidyalaya, CBSE, Delhi                                                                   \\nY e a r  ‚Äì 2 0 1 1  |  F ir s t  D iv i s io n',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\SAURABH_ANALYST_APR22.pdf',\n",
       "   'chunk_id': 'CV_chunk_189'}},\n",
       " {'text': 'Shahina  Athar  a.zaminshahina@gmail.com  |  P o r t f o l i o . c o m  |  L i n k e d I n  |  +91-7569638940    \\n \\nExperience',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_190'}},\n",
       " {'text': 'User  Experience  Designer  Airbus,  Bangalore  |  September  2022  -  November  2024   ‚óè  Led  the  design  and  development  of  user  interfaces  for  the  Resource  Management  System,  a  web-based  Enterprise  Solution  Application ,  using  Figma  to  craft  visually  consistent  and  user-friendly  layouts  based  on  human-centered  design  principles.  ‚óè  Conducted  User  Research  and  usability  testing  to  gather  actionable  insights  and  validate  design  decisions  based  on  Human-Centered  Design  principles.  ‚óè  Led  the  creation  of  wireframes ,  high-Ô¨Ådelity  prototypes ,  and  interactive  UI  components  using  Figma  to  deliver  compelling,  scalable  designs.  ‚óè  Created  detailed  personas  and  user  Ô¨Çows  to  guide  product  decisions  and  improve  user  experience  (UX) .  ‚óè  Applied  design  thinking  methodologies  to  ideate,  iterate,  and  solve  complex  design  challenges.  ‚óè  Led  the  creation  and  maintenance  of  scalable  design  systems',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_191'}},\n",
       " {'text': '(UX) .  ‚óè  Applied  design  thinking  methodologies  to  ideate,  iterate,  and  solve  complex  design  challenges.  ‚óè  Led  the  creation  and  maintenance  of  scalable  design  systems  for  consistent  UI  across  products.  ‚óè  Translated  complex  datasets  and  user  needs  into  clean,  intuitive,  and  accessible  user  interfaces  guided  by  Human-Centered  Design  methodologies.  ‚óè  Collaborated  with  product  managers ,  developers ,  and  stakeholders  to  align  design  with  business  objectives.  ‚óè  Utilized  analytics  tools  and  A/B  testing  to  measure  and  improve  design  performance.  ‚óè  Collaborated  with  front-end  developers  to  ensure  accurate  implementation  of  design  speciÔ¨Åcations  created  in  Figma ,  providing  ongoing  support  during  development.  ‚óè  Product  Experience  Enhancement :  Applied  human-centered  design  thinking  to  optimize  usability  and  continuously  reÔ¨Åne  the  product  experience.  ‚óè  Product  Strategy  &  Design',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_192'}},\n",
       " {'text': '‚óè  Product  Experience  Enhancement :  Applied  human-centered  design  thinking  to  optimize  usability  and  continuously  reÔ¨Åne  the  product  experience.  ‚óè  Product  Strategy  &  Design  Methodology:  Collaborating  with  cross-functional  teams  to  craft  design  strategies  that  align  with  both  business  goals  and  user  needs,  ensuring  that  product  solutions  drive  measurable  outcomes.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_193'}},\n",
       " {'text': 'Data  Analyst  Virtusa,  Hyderabad  |  January  2020  -  August  2022   ‚óè  Collaborated  with  stakeholders  to  understand  business  goals  and  data  requirements.  ‚óè  Analyzed  large  datasets  to  uncover  trends,  patterns,  and  actionable  insights  to  support  business  decisions.  ‚óè  Created  interactive  Dashboards  and  Reports  using  Tools  like  Tableau  &  Power  BI.  ‚óè  Conducted  exploratory  data  analysis  (EDA)  to  understand  data  trends.  ‚óè  Extracted  data  using  SQL  queries  and  data  connectors.   ‚óè  Collaborated  with  cross-functional  teams  to  understand  data  needs  and  deliver  actionable  insights.   ‚óè  Support  decision-making  by  sharing  insights  with  teams  and  helping  improve  business  strategies.   ‚óè  Translated  data  Ô¨Åndings  into  clear,  concise  presentations  for  non-technical  audiences.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_194'}},\n",
       " {'text': 'Digital  Marketing  Executive  Sanbrains,  Hyderabad  |  April  2019  -  October  2019    ‚óè  Conducted  keyword  research  and  optimized  website  content  to  improve  organic  search  rankings.   ‚óè  Build  high-quality  backlinks  through  outreach,  guest  posting,  and  directory  submissions.   ‚óè  Collaborated  with  content  teams  to  ensure  SEO  best  practices  were  integrated  into  all  web  content.  \\n  \\nDigital  Marketing  Executive  PinnaclesPro,  Hyderabad  |  June  2018  -  December  2018    ‚óè  Optimized  on-page  elements,  including  meta  tags,  headers,  and  content  for  better  search  visibility.   ‚óè  Create  logos,  banners,  posters,  and  social  media  graphics  using  Photoshop  ‚óè  Conducted  keyword  research  and  optimized  website  content  to  improve  organic  \\nsearch\\n \\nrankings.\\n   \\n \\nProfessional  Summary',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_195'}},\n",
       " {'text': 'Professional  Summary   \\nDynamic,  detail-oriented,  and  AI-powered  UX  Designer  with  over   5+  years  of  IT  industry  experience,  driving  innovation  through  Human-Centered  Design ,  data-informed  decisions,  and  a  strong  foundation  in  Design  Thinking .  Adept  at  leading  the  full  UX  lifecycle‚Äîconducting  in-depth  User  Research ,  deÔ¨Åning  problems,  ideating  solutions,  and  delivering  intuitive,  accessible,  and  aesthetically  compelling  user  interfaces.  ProÔ¨Åcient  in  Figma  for  high-impact  UI  design,  component  management,  and  collaboration.  Known  for  translating  complex  business  needs  into  seamless  user  journeys,  backed  by  research,  usability  testing,  and  iterative  improvements.  A  strong  advocate  of  Human-Centered  Design ,  combining  empathy,  creativity,  and  strategy  to  drive  meaningful  digital  experiences.  Experienced  in  applying  Design  Thinking  and  User  Research  across  agile  product  teams.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_196'}},\n",
       " {'text': 'Education  \\nS.B.  Jain  College,  Nagpur  B.E.  ‚Äì  Electronics  &  Telecommunication  Engineering  |  2012  ‚Äì  2016  \\nSkills  \\nDesign ¬∑  User  Experience  Design  ¬∑  Interaction  Design  ¬∑  User  Interface  Design   ¬∑  Wireframes  ¬∑  Prototyping  ¬∑  Low-High  Fidelity  Mockups  ¬∑  User-Center  Design   ¬∑  Design  Strategy  ¬∑  Design  Thinking  ¬∑  Design  Strategy  ¬∑  Responsive  Web  Design   ¬∑  Heuristic  Evaluation  \\n Research  ¬∑  User  Interviews  ¬∑  User  Research  ¬∑  Usability  Testing  ¬∑  User  Ô¨Çow    ¬∑  Information  Architecture  ¬∑  Quantitative  Analysis  ¬∑  Qualitative  Analysis    ¬∑  Competitor  Analysis  ¬∑  Surveys  ¬∑  Persona  ¬∑  Card  Sorting  ¬∑  A/B  Testing   ¬∑  Field  Studies  ¬∑  Thematic  Analysis  ¬∑  Human-Center  Design   \\nSoftware   Figma  ¬∑  Adobe  XD  ¬∑  Miro  ¬∑  Maze  ¬∑  Balsamiq  ¬∑  Zeplin  ¬∑  Photoshop  ¬∑  Hotjar    ¬∑Adobe  Illustrator   ¬∑  Adobe  Creative  Suite  ¬∑  ChatGpt  ¬∑  Visily',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_197'}},\n",
       " {'text': 'Software   Figma  ¬∑  Adobe  XD  ¬∑  Miro  ¬∑  Maze  ¬∑  Balsamiq  ¬∑  Zeplin  ¬∑  Photoshop  ¬∑  Hotjar    ¬∑Adobe  Illustrator   ¬∑  Adobe  Creative  Suite  ¬∑  ChatGpt  ¬∑  Visily   \\nVisual  Design   ¬∑  Design  Systems  &  Components  ¬∑  Logo  making  ¬∑  Typography  ¬∑  Iconography     ¬∑  Google‚Äôs  Material  Design  Guidelines  ¬∑  Pixel-Perfect  Design  ¬∑   Illustration    \\nOther  Skills ¬∑  Critical  thinking  ¬∑  Cross-functional  Collaboration  and  Communication   ¬∑  Adatability  ¬∑  Storytelling  ¬∑  Problem-Solving  ¬∑  Time  management   \\n \\nAwards \\nChampion  of  Growth,  Professionals  Success  Club  |  2025   CertiÔ¨Åcate  of  Merit  ‚Äì  CAMBRIDGE  COUNCIL  |  2017   CertiÔ¨Åcate  of  Merit  in  Logistics  Excellence  |  2016   National  Level  CertiÔ¨Åcate  Tech-Shindig  6.0  |  2014   National  level  CertiÔ¨Åcate,  FEETA  presents,  CRANK  |  2014    \\nCertiÔ¨Åcations \\nDESIGN  RULES:  Principles  for  Great  UI  Design  ‚Äì  Udemy   Learn  Figma  ‚Äì  UI/UX  Design  Essential  Training  -  Udemy',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Shahina_Updated_Resume..pdf',\n",
       "   'chunk_id': 'CV_chunk_198'}},\n",
       " {'text': 'UDAY SHARMA \\n+91-9805278069 | udaysharmaz1003@gmail.com | Linkedin/Uday Sharma | Nahan, Himachal Pradesh \\nEducation  \\nBachelor of Technology, Computer Science                                   2019 ‚Äì 2023  \\nPanjab University | Chandigarh, India                         CGPA: 8.48 \\n \\nExperience \\n \\nAssociate Software Engineer            08/2023 ‚Äì Present            \\nOSI Systems | Hyderabad, Telangana  \\n` \\n\\uf0b7 Designed and enhanced core features for 3 enterprise client releases, and supported migration to secure, resilient microservices, following a data \\ndriven architecture. \\n\\uf0b7 Refactored legacy code and resolved 40+ production issues; Improved SCA & Junit code coverage from 60% to 90% \\n\\uf0b7 Implemented Log Aggregations and Kibana Dashboards in the application. \\n\\uf0b7 Automated scheduled tasks using Spring Scheduler, enhancing system reliability and availability. \\n\\uf0b7 Designed and developed features to support a multi-tenant architecture, enabling isolated data for 7+ clients.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Uday_Sharma_SWE.pdf',\n",
       "   'chunk_id': 'CV_chunk_199'}},\n",
       " {'text': '\\uf0b7 Designed and developed features to support a multi-tenant architecture, enabling isolated data for 7+ clients. \\n\\uf0b7 Implemented Caching using Redis & Messaging Queues (RabbitMQ, Kafka) for asynchronous microservices communication. \\n\\uf0b7 Collaborated with Automation & QA testing team to resolve performance and load-related bugs. \\n\\uf0b7 Contributed to production-grade CI/CD pipelines using Jenkins and Git, supporting multiple deployment pipelines. \\n\\uf0b7 Collaborated in Agile teams using JIRA, Git, TDD, created detailed technical documentations using Enterprise Architect and participated in peer \\ncode reviews. \\n\\uf0b7 Managed deliverables using Agile tools like JIRA, Slack, and Azure DevOps \\n\\uf0b7 Adhered to OWASP Top10 guidelines to harden microservice security and participated in internal secure coding reviews. \\nSoftware Developer Intern             01/2023 ‚Äì 07/2023          \\nChicmic | Mohali, Punjab \\n\\uf0b7 Led the design and implementation of multiple projects assigned to the intern team.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Uday_Sharma_SWE.pdf',\n",
       "   'chunk_id': 'CV_chunk_200'}},\n",
       " {'text': 'Software Developer Intern             01/2023 ‚Äì 07/2023          \\nChicmic | Mohali, Punjab \\n\\uf0b7 Led the design and implementation of multiple projects assigned to the intern team. \\n\\uf0b7 Developed backend modules in Java with features including CRUD, ORM, Auth, Exceptional handling, Payments, Multithreading, al so worked with \\nUI, Android & Cloud teams. \\n\\uf0b7 Utilized SonarQube, JUnit, and Mockito to ensure high-quality, testable, and maintainable code. \\n\\uf0b7 Maintained API documentation using Swagger and used Postman for Validating and testing APIs. \\n\\uf0b7 Applied Agile and Linux-based methodologies to improve solutions. \\n \\nResearch and Development Intern           06/2021 ‚Äì 10/2021 \\nPowerhouse AI | Singapore (Remote) \\n \\n\\uf0b7 Collaborated with Tech and Business team to designed UI and UX wireframes and present Pitch deck to stakeholders; Used python and early stage \\napplication of NLP, AI to analyse data, extract insights, and optimize reporting performance.  \\n \\n \\n \\nSKILLS',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Uday_Sharma_SWE.pdf',\n",
       "   'chunk_id': 'CV_chunk_201'}},\n",
       " {'text': 'application of NLP, AI to analyse data, extract insights, and optimize reporting performance.  \\n \\n \\n \\nSKILLS \\n\\uf0b7 Languages: Java, SQL, HTML, Javascript \\n\\uf0b7 Libraries, Framework: Spring boot, Hibernate, Angular Js, GRPC, REST, SOAP \\n\\uf0b7 Tools and Software: Linux, Redis, RabbitMQ, WebSockets, Git, JIRA, Docker, AWS, PgAdmin \\n\\uf0b7 Database: MySQL, PostgreSQL \\n\\uf0b7 Subjects: Data Structures and Algorithms, OOPS, Agile Methodology, Operating System, RDBMS, SDLC, Leadership, Problem Solving, \\nCommunication Skills, Analytical Skills \\nProjects  \\n \\n \\n \\nTakeAway | Spring boot, Postgresql, JS, Apache \\n\\uf0b7 Constructed a Food ordering Android app, enabling users to order from canteen and pay via digital wallet. \\n\\uf0b7 Managed real-time concurrent orders and achieved 80+ active user engagement. \\neNaukri | Spring boot, Postgresql, Web Socket, JS \\n \\n \\n\\uf0b7 Built a Job portal for multi-role users to post, search, and apply for jobs. \\n\\uf0b7 Implemented chat, search filters, authentication, and notification features.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Uday_Sharma_SWE.pdf',\n",
       "   'chunk_id': 'CV_chunk_202'}},\n",
       " {'text': '\\uf0b7 Built a Job portal for multi-role users to post, search, and apply for jobs. \\n\\uf0b7 Implemented chat, search filters, authentication, and notification features. \\n \\nOther Achievements  \\n\\uf0b7 Certified in Agile Methodologies & Practises. \\n\\uf0b7 Former Head, Technical Group, Panjab University. \\n\\uf0b7 Core Team Member, Training and Placement Cell. \\n\\uf0b7 Gold & Silver Medallist, District & State Rifle Shooting Championship.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Uday_Sharma_SWE.pdf',\n",
       "   'chunk_id': 'CV_chunk_203'}},\n",
       " {'text': 'VIDUSHI GOEL\\nSUMMARY\\nDynamic and forward-thinking AI Consultant with 1 year of experience building innovative, real-world\\nsolutions using Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and agentic\\nframeworks. Skilled in developing intelligent systems using LangChain, LangGraph, and Open Interpreter,\\nwith a focus on solving complex problems in supply chain analytics. Adept at building robust backend\\narchitectures with Python, FastAPI, and Azure. Previously supported the Factory Planning module on the\\nBlue Yonder platform for 4 large integrated steel plants.\\nPROFESSIONAL EXPERIENCE  (1 year)\\n1. End-to-End Generative AI Agent Frameworks:\\nDeveloped a scalable data analysis agent using LangChain, LangGraph, and Open Interpreter, data\\nretrieval, and analysis across enterprise datasets.\\nIntegrated Azure SQL via SSMS with MongoDB memory checkpointing, enabling agent continuity\\nand boosting execution speed by ~35%.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\VidushiGoel-resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_204'}},\n",
       " {'text': 'retrieval, and analysis across enterprise datasets.\\nIntegrated Azure SQL via SSMS with MongoDB memory checkpointing, enabling agent continuity\\nand boosting execution speed by ~35%.\\nEngineered a ReAct-based decision agent to enhance multi-step reasoning.\\nImplemented the Model-Context Protocol (MCP) using fastmcp, enabling modular tool-calling and\\ndynamic orchestration between agent and external systems.\\nAssociate Consultant - Data Science & Supply Chain\\n4. Blue Yonder Factory Planning ‚Äì Post-Implementation Support:\\nSupported the Factory Planning module on the Blue Yonder (JDA) platform for 4 large integrated\\nsteel plants. Assisted in analyzing system requirements, validating alignment with Blue Yonder\\ncapabilities, and ensuring smooth functional integration during the post-implementation phase.\\n Email: vidushigoel3108@gmail.com | 9319931622 \\n LinkedIn: http://www.linkedin.com/in/vidushi-goel-393a43256 \\n2.Advanced Document Intelligence & Retrieval Systems:',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\VidushiGoel-resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_205'}},\n",
       " {'text': 'Email: vidushigoel3108@gmail.com | 9319931622 \\n LinkedIn: http://www.linkedin.com/in/vidushi-goel-393a43256 \\n2.Advanced Document Intelligence & Retrieval Systems:\\nBuilt an intelligent document retrieval system using LightRAG, designing a graph-based retriever for\\nhigh-accuracy semantic search across unstructured data.\\nCreated custom embeddings and integrated Azure Postgres + pgvector, reducing retrieval latency\\nby ~40% and improving semantic accuracy by ~30%.\\nImplemented cosine similarity-based ranking, enhancing document relevance and precision.\\n3. High-Concurrency API Development & Optimization:\\nDelivered robust backend services using FastAPI with parallelized REST endpoints, increasing\\nthroughput and concurrency handling by ~50%.\\nEnabled JWT-based secure authentication, ensuring data privacy and controlled access to API\\nendpoints.\\nUtilized Azure Load Testing and Postman test coverage, ensuring 99.9% API uptime and\\nperformance stability under scale.\\nErnst & Young India LLP (1 year)',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\VidushiGoel-resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_206'}},\n",
       " {'text': 'endpoints.\\nUtilized Azure Load Testing and Postman test coverage, ensuring 99.9% API uptime and\\nperformance stability under scale.\\nErnst & Young India LLP (1 year)\\nJuly 2024 - Present\\nMumbai-MH\\nPROJECTS\\nMultimodal Retrieval System ‚Äì Built PDF-based RAG system using image & text embeddings via\\nLlamaIndex and PostgreSQL.\\nImplemented a multi-agent decision automation system using AutoGen to optimize order fulfillment\\nworkflows. Integrated domain-specific agents (Order Management, Inventory, Capacity, Tradeoff,\\nRe-Optimization) with rule-based logic and structured data reasoning to automate rush order\\nevaluation and supply chain planning.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\VidushiGoel-resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_207'}},\n",
       " {'text': 'EDUCATION\\nUniversity of Delhi\\n Dec 2022 - June 2024\\nBachelor of Mathematics (Hons.) - (9.35 CGPA)  July 2019 - May 2022\\nMaster of Operational Research - (8.12 CGPA)\\nNew Delhi\\nNew DelhiUniversity of Delhi\\nCERTIFICATIONS\\nEY Bronze Badge in AI Engineering\\nCertified in MSSQL\\nCertified in Microsoft Excel\\nCertified in Power BI\\nSupply Chain - Beginner\\nGenerative AI LLMs (Azure OpenAI), Retrieval-Augmented Generation (RAG), vector databases\\n(pgvector, MongoDB), custom embeddings, LangGraph, Azure, MCP.\\nAgentic\\nFrameworks\\nLangChain, LangGraph, Open Interpreter, dynamic agents, multi-tool\\norchestration, memory checkpointing, React-based decision agents.\\nData Science &\\nML\\nPython (Pandas, NumPy, Scikit-learn), statistical modeling,Machine learning,\\nclustering, evaluation metrics (F1, ROC, etc.).\\nOptimization Linear & Integer Programming, Warehouse Location, Source Mix Optimization,\\nTraveling Salesman Problem (TSP) using OR-Tools, PuLP, SciPy,Excel Solver.\\nAPI & Backend\\nSystems',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\VidushiGoel-resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_208'}},\n",
       " {'text': 'Optimization Linear & Integer Programming, Warehouse Location, Source Mix Optimization,\\nTraveling Salesman Problem (TSP) using OR-Tools, PuLP, SciPy,Excel Solver.\\nAPI & Backend\\nSystems\\nFastAPI (scalable REST APIs), JWT-based authentication, Azure, Postman (test\\nautomation), high-concurrency architecture.\\nKEY SKILLS\\nSource Mix Optimization for Cost Minimization- Formulated and solved a linear programming model\\nusing Excel Solver to minimize transportation costs across a multi-factory, multi-depot network for a\\nMade-to-Stock manufacturing setup.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\VidushiGoel-resume.pdf',\n",
       "   'chunk_id': 'CV_chunk_209'}},\n",
       " {'text': 'New Delhi, India 9968377674 | \\nsablok.vishu2@gmail.com \\nSKILLS \\n‚ñ™ Generative AI & LLMs: RAG, LLM Fine-Tuning (PEFT, SFT), OpenAI GPT, Llama-3, BERT, Prompt Engineering \\n‚ñ™ Machine Learning & Frameworks: PyTorch, TensorFlow, Hugging Face, Scikit-learn, XGBoost, SARIMA, Prophet \\n‚ñ™ Cloud & MLOps: AWS (SageMaker, EC2, S3), Docker, CI/CD, Model Deployment, Latency Optimization \\n‚ñ™ Big Data & Programming: Python, PySpark (Spark), SQL, REST APIs (Flask), Pandas, NumPy \\n‚ñ™ Data Engineering: Data Preprocessing, Feature Engineering, Pyspark, Databricks \\n‚ñ™ Leadership: Cross-Functional Team Leadership, Project Management, Mentorship \\n \\nEXPERIENCE \\nData Scientist | NAB, Gurgaon, India Jan 2024 ‚Äì Present \\n\\uf0b7 Email and Ticket query resolution with RAG. \\n‚ñ™ Orchestrated RAG pipelines with OpenAI and LangChain to streamline email and ticket query resolution, reducing \\nTAT through optimized human involvement. Utilized the database and SOP policies for effective query handling',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Vishu_Sablok_Data_Science_LLM_6year-1.pdf',\n",
       "   'chunk_id': 'CV_chunk_210'}},\n",
       " {'text': 'TAT through optimized human involvement. Utilized the database and SOP policies for effective query handling  \\n‚ñ™ Mentored junior data scientists in prompt engineering (Chain of Thought, Few-Shot) and dynamic retrieval \\nstrategies. \\n‚ñ™ Led a team of 4 engineers to design agentic AI workflows for automated email resolution, reducing turnaround \\ntime by 40%. \\n‚ñ™ Fine-tuned Llama-3 using PEFT/SFT, improving domain-specific accuracy by 25%. \\n‚ñ™ Mentored junior data scientists in prompt engineering(Chain of Thought, Few-Shot) and dynamic retrieval strategies \\n \\nData Scientist |Bridgenext, Gurgaon, India Aug 2022 ‚Äì Jan 2024 \\n\\uf0b7 Predictive Analytics for Customer Retention and Acquisition \\n‚ñ™ Managed end -to-end ML projects: Developed a churn prediction model ( XGBoost) achieving 85% accuracy, \\ndriving $1M revenue uplift. \\n‚ñ™ Engineered features from raw customer data using PySpark, optimizing model performance for real -time \\npredictions via Dockerized Flask APIs.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Vishu_Sablok_Data_Science_LLM_6year-1.pdf',\n",
       "   'chunk_id': 'CV_chunk_211'}},\n",
       " {'text': 'driving $1M revenue uplift. \\n‚ñ™ Engineered features from raw customer data using PySpark, optimizing model performance for real -time \\npredictions via Dockerized Flask APIs. \\n‚ñ™ Deployed seamlessly on AWS ec2 instance using Docker and Kubernates on Batch Streaming. the model \\nhas been instrumental in boosting customer base by 20%, increasing revenue  by $150,000 and significantly \\nimproving retention rates by 30% \\n \\nData Scientist |Accenture, Gurgaon, India Dec 2018 ‚Äì July 2022 \\n\\uf0b7 Customer Sentiment Analysis and Visualization for Retail Business Insights. \\n‚ñ™ Led cross-functional teams to deploy BERT-based sentiment analysis (PyABSA) in Databricks, boosting sales \\nby $20K. \\n‚ñ™ Designed Tableau dashboards to visualize feature importance and sentiment trends, enabling data-driven \\nproduct improvements. \\n‚ñ™ Mentored 3 junior analysts in NLP techniques, improving team efficiency by 15%.',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Vishu_Sablok_Data_Science_LLM_6year-1.pdf',\n",
       "   'chunk_id': 'CV_chunk_212'}},\n",
       " {'text': 'product improvements. \\n‚ñ™ Mentored 3 junior analysts in NLP techniques, improving team efficiency by 15%. \\n‚ñ™ Applied time series forecasting techniques such as moving averages, Fb Prophet and SARIMA to predict future \\nsubscriber growth. \\nKEY PROJECTS \\n\\uf0b7 Automated Drift Detection Framework \\n‚Ä¢ Processed 10M+ customer records using PySpark for clustering analysis, enabling targeted retention \\nstrategies. \\n‚Ä¢ Reduced computation time by 40% through distributed computing optimizations \\n \\nEDUCATION  \\nB. E. Electronics Electrical Engineering | Guru Gobind Singh Indraprastha \\nUniversity, New Delhi, India \\n \\n \\nAugust 2014 ‚Äì July 2018 \\nVISHU SABLOK',\n",
       "  'meta': {'source': '..\\\\data\\\\CV\\\\Vishu_Sablok_Data_Science_LLM_6year-1.pdf',\n",
       "   'chunk_id': 'CV_chunk_213'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_chunks['CV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ba21af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Building FAISS index for category: CV\n",
      "‚úÖ FAISS index for 'CV' saved to 'faiss_indexes\\CV' (139 vectors)\n",
      "\n",
      "[INFO] Building FAISS index for category: FINANCIAL\n",
      "‚úÖ FAISS index for 'FINANCIAL' saved to 'faiss_indexes\\FINANCIAL' (117 vectors)\n",
      "\n",
      "[INFO] Building FAISS index for category: REIMBURSEMENT\n",
      "‚úÖ FAISS index for 'REIMBURSEMENT' saved to 'faiss_indexes\\REIMBURSEMENT' (22 vectors)\n",
      "\n",
      "[INFO] Building FAISS index for category: SPECS\n",
      "‚úÖ FAISS index for 'SPECS' saved to 'faiss_indexes\\SPECS' (585 vectors)\n",
      "\n",
      "‚úÖ All FAISS indexes built successfully.\n",
      "\n",
      "--- FAISS Index Summary ---\n",
      "CV: 139 vectors\n",
      "FINANCIAL: 117 vectors\n",
      "REIMBURSEMENT: 22 vectors\n",
      "SPECS: 585 vectors\n",
      "‚úÖ Cell 3B executed successfully ‚Äî FAISS indexes created and saved.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3B. Generate Embeddings & Build FAISS Indexes\n",
    "# \n",
    "# This cell:\n",
    "# - Uses the `build_all_faiss_indexes()` function\n",
    "# - Creates a FAISS index for each category\n",
    "# - Displays summary information about the created indexes\n",
    "\n",
    "# %%\n",
    "faiss_indexes = build_all_faiss_indexes(grouped_chunks)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n--- FAISS Index Summary ---\")\n",
    "for category, index in faiss_indexes.items():\n",
    "    print(f\"{category}: {index.index.ntotal} vectors\")\n",
    "\n",
    "print(\"‚úÖ Cell 3B executed successfully ‚Äî FAISS indexes created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af60e492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CV': <langchain_community.vectorstores.faiss.FAISS at 0x14aa95ef910>,\n",
       " 'FINANCIAL': <langchain_community.vectorstores.faiss.FAISS at 0x14aa95ece10>,\n",
       " 'REIMBURSEMENT': <langchain_community.vectorstores.faiss.FAISS at 0x147c5f00810>,\n",
       " 'SPECS': <langchain_community.vectorstores.faiss.FAISS at 0x146b485bb50>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb6cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employment\n",
      "Dun & Bradstreet Bengaluru\n",
      "Data Scientist II Aug. 2021 to Current\n",
      "-Responsible for delivering Data Engineering and Data Science workÔøΩows.\n",
      "-Working on automating ML Solutions using Python, P\n",
      "=================\n",
      "DATA SCIENTIST 2 |\n",
      "KAGGLE 2X EXPERT\n",
      "RAVINDER\n",
      "KUMAR¬†\n",
      "TANWAR\n",
      "rtanwar616@gmail.com\n",
      "https://www.kaggle.com\n",
      "/ravijoe\n",
      "+918847095358\n",
      "https://www.linkedin.com\n",
      "/in/ravi-tanwar-12bb3811a/\n",
      "https://github.com/rav\n",
      "=================\n",
      "Data Science Math Skills\n",
      " (06/2020 - 07/2020)\n",
      " \n",
      "OÔ¨Äered by Duke University through Coursera \n",
      "Getting Started with AWS Machine Learning\n",
      " (10/2020 - 11/2020)\n",
      " \n",
      "OÔ¨Äered by Amazon through Coursera \n",
      "Using Py\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "cv_index = faiss_indexes[\"CV\"]\n",
    "results = cv_index.similarity_search(\"Data Scientist at Microsoft\", k=3)\n",
    "#print(results)\n",
    "for r in results:\n",
    "    print(r.page_content[:200])\n",
    "    print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82fabd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM MODEL_PATH - models\\mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4A. LLM Configuration & Loader Function\n",
    "# Define all configuration parameters and helper functions\n",
    "# to load a quantized Mistral-7B GGUF model using llama-cpp-python with CUDA acceleration.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# --- Global configuration for the quantized model ---\n",
    "MODEL_DIR = r\"models\"\n",
    "MODEL_FILE = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"  # adjust if needed\n",
    "#mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n",
    "print(f\"LLM MODEL_PATH - {MODEL_PATH}\")\n",
    "\n",
    "# Model parameters\n",
    "N_GPU_LAYERS = -1       # use all layers on GPU\n",
    "N_CTX = 8192+8192           # max context window\n",
    "N_BATCH = 512\n",
    "TEMPERATURE = 0.1\n",
    "MAX_TOKENS = 512\n",
    "VERBOSE = True\n",
    "\n",
    "# --- Helper function to load the quantized model ---\n",
    "def load_llm(model_path: str = MODEL_PATH):\n",
    "    \"\"\"\n",
    "    Load the quantized Mistral-7B-Instruct model using llama-cpp-python with CUDA acceleration.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Loading model from: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at: {model_path}\")\n",
    "\n",
    "    llm = LlamaCpp(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=N_GPU_LAYERS,\n",
    "        n_ctx=N_CTX,\n",
    "        n_batch=N_BATCH,\n",
    "        f16_kv=True,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Model loaded successfully with CUDA acceleration enabled.\")\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f281589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model from: models\\mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 6766 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models\\mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  4095.05 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 16384\n",
      "llama_context: n_ctx_per_seq = 16384\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 16384 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB ( 16384 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      CUDA0 compute buffer size =  1092.01 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    44.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully with CUDA acceleration enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4B. Load the Quantized Model and Test\n",
    "# Actually load the Mistral-7B GGUF model and run a simple test prompt\n",
    "# to confirm it's working with CUDA.\n",
    "\n",
    "# %%\n",
    "# Load the LLM using the helper defined above\n",
    "llm = load_llm(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3f5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Test Prompt: Where is Nice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     488.03 ms /     5 tokens (   97.61 ms per token,    10.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11615.50 ms /   511 runs   (   22.73 ms per token,    43.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   14516.15 ms /   516 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ LLM Response:\n",
      "  Nice is a city located in the southeastern part of France, on the French Riviera. It is situated between the eastern end of the Baie des Anges (Bay of Angels) and the western tip of Cap de Nice (Nice Cape). The city center is about 12 kilometers (7 miles) west of the Italian border. Nice is the second-largest French city on the Mediterranean coast, after Marseille. It is also the fifth-most populous urban area in France, with a population of over 340,000 inhabitants in the metropolitan area. Nice is known for its beautiful beaches, crystal-clear waters, and mild Mediterranean climate. The city is also famous for its vibrant cultural scene, rich history, and stunning architecture. Some of the most popular tourist attractions in Nice include the Old Town (Vieille Ville), Colline du Ch√¢teau (Castle Hill), Promenade des Anglais (English Promenade), Mus√©e Matisse (Matisse Museum), and Mus√©e Marc Chagall (Chagall Museum). Nice is also home to many beautiful parks, gardens, and public spaces, such as the Jardin Albert Ier (Albert I Garden), Jardin Cagnes (Cagnes Garden), Parc Phoenix (Phoenix Park), and Parc du Mont Boron (Mont Boron Park). Nice is also known for its delicious local cuisine, which is heavily influenced by Italian and Proven√ßal culinary traditions. Some of the most popular dishes from Nice include socca (a savory chickpea pancake), pissaladi√®re (an onion tart topped with anchovies or olives), bouillabaisse (a hearty fish stew originating in the port city of Marseille, but also popular in Nice), and salade ni√ßoise (a classic salad made with tomatoes, olives, hard-boiled eggs, anchovies, and capers, all dressed with a vinaigrette). Nice is also famous for its delicious pastries, such as the famous \"pastis\" (a sweet almond cake), \"financiers\" (small cakes made with almond meal, sugar, and butter, baked in small rectangular molds and named after the Parisian financial district where they were first popularized), and \"macarons\" (small round\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity check ---\n",
    "test_prompt = \"Where is Nice.\"\n",
    "print(\"\\nüß† Test Prompt:\", test_prompt)\n",
    "\n",
    "response = llm.invoke(test_prompt)\n",
    "print(\"\\nüí¨ LLM Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df97c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5A. Retrieval-Augmented Generation (RAG) Setup\n",
    "# Define helper functions that connect the FAISS vectorstores with the quantized Mistral LLM.\n",
    "# Each category (CV, FINANCIAL, REIMBURSEMENT, SPECS) will have its own retriever.\n",
    "\n",
    "# %%\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# %\n",
    "number_of_retrievals = 5\n",
    "\n",
    "# --- Prompt template for RAG responses ---\n",
    "RAG_TEMPLATE = \"\"\"You are an intelligent assistant that answers questions based on the provided context.\n",
    "If the answer cannot be found in the context, say \"The answer is not available in the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(template=RAG_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# --- Helper function to create RAG QA chain ---\n",
    "def create_rag_chain(llm, vectorstore):\n",
    "    \"\"\"\n",
    "    Create a RetrievalQA chain for a given FAISS vectorstore and the loaded LLM.\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": number_of_retrievals})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": qa_prompt},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "# --- Build QA chains for all available categories ---\n",
    "def build_all_rag_chains(llm, vectorstores_dict):\n",
    "    \"\"\"\n",
    "    Create and store RAG QA chains for each category (CV, FINANCIAL, REIMBURSEMENT, SPECS).\n",
    "    Returns a dictionary of QA chains keyed by category name.\n",
    "    \"\"\"\n",
    "    rag_chains = {}\n",
    "    for category, vs in vectorstores_dict.items():\n",
    "        print(f\"üîó Building RAG chain for: {category}\")\n",
    "        rag_chains[category] = create_rag_chain(llm, vs)\n",
    "    print(\"‚úÖ All RAG chains initialized.\")\n",
    "    return rag_chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "971f7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5A. Enhanced RAG Setup (Return Sources)\n",
    "# Modified version of RAG setup to:\n",
    "# 1. Return source documents and context\n",
    "# 2. Use a stricter prompt to reduce hallucination\n",
    "\n",
    "# %%\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --- Stricter prompt template to reduce hallucination ---\n",
    "RAG_TEMPLATE = \"\"\"You are an intelligent assistant that answers questions based only on the provided context.\n",
    "If the answer cannot be found in the context, respond with:\n",
    "\"The answer is not available in the provided documents.\"\n",
    "\n",
    "Use only the facts from the context and do not add your own assumptions.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (with factual reasoning):\"\"\"\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"You are a precise and intelligent assistant that answers questions. Use ONLY the following retrieved context to answer the question. If you are not certain or answer is not in the provided Context, say \"I don't know based on the provided context.\"\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (with factual reasoning):\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(template=RAG_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# --- Helper function to create RAG QA chain with sources ---\n",
    "def create_rag_chain_with_sources(llm, vectorstore):\n",
    "    \"\"\"\n",
    "    Create a RetrievalQA chain that returns source documents and reduces hallucination.\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": qa_prompt},\n",
    "        return_source_documents=True,   # important for sources\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "# --- Build all category-wise QA chains ---\n",
    "def build_all_rag_chains_with_sources(llm, vectorstores_dict):\n",
    "    \"\"\"\n",
    "    Build RAG chains for all categories that return both answers and source documents.\n",
    "    \"\"\"\n",
    "    rag_chains = {}\n",
    "    for category, vs in vectorstores_dict.items():\n",
    "        print(f\"üîó Building RAG chain with sources for: {category}\")\n",
    "        rag_chains[category] = create_rag_chain_with_sources(llm, vs)\n",
    "    print(\"‚úÖ All RAG chains (with source return) initialized.\")\n",
    "    return rag_chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e72e64a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Building RAG chain with sources for: CV\n",
      "üîó Building RAG chain with sources for: FINANCIAL\n",
      "üîó Building RAG chain with sources for: REIMBURSEMENT\n",
      "üîó Building RAG chain with sources for: SPECS\n",
      "‚úÖ All RAG chains (with source return) initialized.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5B. Execute RAG Queries\n",
    "# Use the quantized Mistral model + FAISS retrievers to answer questions from your local documents.\n",
    "\n",
    "# %%\n",
    "# Build all category-wise RAG pipelines\n",
    "#rag_chains = build_all_rag_chains(llm, faiss_indexes)\n",
    "rag_chains = build_all_rag_chains_with_sources(llm, faiss_indexes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58310ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÇÔ∏è Category: CV\n",
      "\n",
      "‚ùì Q1: What programming languages does Moid Hassan know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 58 prefix-match hit, remaining 2734 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =    2304.29 ms /  2734 tokens (    0.84 ms per token,  1186.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =     922.31 ms /    38 runs   (   24.27 ms per token,    41.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    3294.91 ms /  2772 tokens\n",
      "llama_perf_context_print:    graphs reused =         36\n",
      "Llama.generate: 58 prefix-match hit, remaining 2245 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Moid Hassan is proficient in several programming languages, including Python, C, C++, SQL, Kotlin, Java, HTML, CSS, JavaScript, and TypeScript.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q2: Where is Moid Hassan currently working?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =    1629.91 ms /  2245 tokens (    0.73 ms per token,  1377.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     376.79 ms /    16 runs   (   23.55 ms per token,    42.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    2028.27 ms /  2261 tokens\n",
      "llama_perf_context_print:    graphs reused =         14\n",
      "Llama.generate: 1374 prefix-match hit, remaining 773 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Moid Hassan is currently working as a Vice President at Morgan Stanley.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q3: Where has Moid Hassan worked in the past?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     621.69 ms /   773 tokens (    0.80 ms per token,  1243.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.90 ms /    47 runs   (   23.70 ms per token,    42.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1819.40 ms /   820 tokens\n",
      "llama_perf_context_print:    graphs reused =         45\n",
      "Llama.generate: 58 prefix-match hit, remaining 2110 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Moid Hassan has worked for Argusoft India Ltd. in the past. This information can be deduced from the context provided, which mentions that Moid Hassan has previously worked for Argusoft India Ltd.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üóÇÔ∏è Category: FINANCIAL\n",
      "\n",
      "‚ùì Q1: What is the total dividend income in August 2025?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =    1525.63 ms /  2110 tokens (    0.72 ms per token,  1383.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2956.72 ms /   125 runs   (   23.65 ms per token,    42.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    4713.10 ms /  2235 tokens\n",
      "llama_perf_context_print:    graphs reused =        120\n",
      "Llama.generate: 58 prefix-match hit, remaining 1454 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "The table does not provide the total dividend income in August 2025. The table only shows the estimated monthly cash flow for various securities, including stocks, bonds, and mutual funds. The table also includes other relevant information, such as transaction costs, taxes withheld, and core fund activity. However, the table does not provide a comprehensive breakdown of dividend income by month for all securities held in an investment portfolio over a given period of time. Therefore, it is not possible to determine the total dividend income in August 2025 based on the information provided in the table.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q2: What was the Debit in DoubleTree by Hilton in Bangalore?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =    1019.17 ms /  1454 tokens (    0.70 ms per token,  1426.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     915.76 ms /    39 runs   (   23.48 ms per token,    42.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    2009.00 ms /  1493 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n",
      "Llama.generate: 58 prefix-match hit, remaining 1321 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "The provided context does not contain any information about a debit or debt in DoubleTree by Hilton in Bangalore. Therefore, I cannot provide an answer based on the given context.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üóÇÔ∏è Category: REIMBURSEMENT\n",
      "\n",
      "‚ùì Q1: What documents are required for travel reimbursement?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     922.69 ms /  1321 tokens (    0.70 ms per token,  1431.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2208.55 ms /    95 runs   (   23.25 ms per token,    43.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3304.46 ms /  1416 tokens\n",
      "llama_perf_context_print:    graphs reused =         91\n",
      "Llama.generate: 58 prefix-match hit, remaining 769 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Based on the context provided, there is no explicit mention of documents required for travel reimbursement. However, the context does mention that important traveler information regarding Check-in times, Insurance, Health & Vaccinations, USA entry requirement, Pricing & Taxes can be found on the Travel Itinerary. Therefore, it is recommended to refer to the Travel Itinerary for any specific document requirements related to travel reimbursement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üóÇÔ∏è Category: SPECS\n",
      "\n",
      "‚ùì Q1: What is the screen size of Surface Pro 9?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     535.46 ms /   769 tokens (    0.70 ms per token,  1436.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1892.49 ms /    83 runs   (   22.80 ms per token,    43.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    2558.31 ms /   852 tokens\n",
      "llama_perf_context_print:    graphs reused =         79\n",
      "Llama.generate: 59 prefix-match hit, remaining 1321 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "The context provided does not mention the screen size of Surface Pro 9. However, it does provide the technical specifications for Surface Pro X, which includes its display screen size and resolution. Based on this information, I cannot definitively answer your question about the screen size of Surface Pro 9 based on the provided context alone.\n",
      "\n",
      "I don't know based on the provided context.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q2: Which GPU Surface Laptop Studio 2 has?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     935.78 ms /  1321 tokens (    0.71 ms per token,  1411.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1642.06 ms /    71 runs   (   23.13 ms per token,    43.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    2690.14 ms /  1392 tokens\n",
      "llama_perf_context_print:    graphs reused =         68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Surface Laptop Studio 2 comes with NVIDIA¬Æ GeForce RTX‚Ñ¢ 3060 Laptop GPU. This GPU is built with the latest RT Cores, Tensor Cores, and streaming multiprocessors. Surface Laptop Studio 2 has double the graphics performance than Surface Studio 2.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example queries for testing\n",
    "sample_queries = {\n",
    "    \"CV\": [\"What programming languages does Moid Hassan know?\",\n",
    "    \"Where is Moid Hassan currently working?\",\"Where has Moid Hassan worked in the past?\"],\n",
    "    \"FINANCIAL\": [\"What is the total dividend income in August 2025?\",\n",
    "    \"What was the Debit in DoubleTree by Hilton in Bangalore?\"],\n",
    "    \"REIMBURSEMENT\": [\"What documents are required for travel reimbursement?\"],\n",
    "    \"SPECS\": [\"What is the screen size of Surface Pro 9?\", \"Which GPU Surface Laptop Studio 2 has?\"],\n",
    "}\n",
    "\n",
    "# --- Run all queries ---\n",
    "for category, questions in sample_queries.items():\n",
    "    print(f\"\\nüóÇÔ∏è Category: {category}\")\n",
    "    qa_chain = rag_chains[category]\n",
    "\n",
    "    for i, question in enumerate(questions, start=1):\n",
    "        print(f\"\\n‚ùì Q{i}: {question}\")\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        print(f\"üß† Answer:\\n{result['result']}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de5aa94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÇÔ∏è Category: SPECS\n",
      "\n",
      "‚ùì Q1: Suggest a laptop which comes with a dedicated GPU not intergrated GPU. Give me the name of the laptop with GPU name as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 58 prefix-match hit, remaining 1136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =    1134.02 ms /  1136 tokens (    1.00 ms per token,  1001.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1078.44 ms /    47 runs   (   22.95 ms per token,    43.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    2285.40 ms /  1183 tokens\n",
      "llama_perf_context_print:    graphs reused =         45\n",
      "Llama.generate: 58 prefix-match hit, remaining 769 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Based on the provided context, a laptop that comes with a dedicated GPU is the Surface Laptop 15‚Äù 7th Edition. The dedicated GPU for this laptop is the Qualcomm¬Æ Adreno‚Ñ¢ GPU.\n",
      "\n",
      "üìÑ Sources:\n",
      "   1. Source: Unknown\n",
      "      Snippet: Processor 13th Gen Intel Core‚Ñ¢ i7-13700H Processor Built on the Intel Evo‚Ñ¢ platform Intel Gen3 Movidius 3700VC VPU AI Accelerator Graphics Graphics options: NVIDIAGeForce RTX‚Ñ¢ 4050 Laptop GPU with 6GB GDDR6 vRAM 2130 MHz boost clock speed, 80W maximum graphics power NVIDIA GeForce RTX‚Ñ¢ 4060 Laptop G...\n",
      "   2. Source: Unknown\n",
      "      Snippet: Surface Laptop 13.8‚Äù  7th Edition    Surface Laptop 15‚Äù  7th Edition  Processor        Snapdragon¬Æ X Plus   Snapdragon¬Æ X Elite    Snapdragon¬Æ X Elite  Neural Processing Unit (NPU) Qualcomm¬Æ Hexagon‚Ñ¢ with 45 TOPS  Graphics Qualcomm¬Æ Adreno‚Ñ¢ GPU  Memory and Storage1 Memory options:  16GB, 32GB, 64GB ...\n",
      "   3. Source: Unknown\n",
      "      Snippet: Surface Laptop 13.8‚Äù  7th Edition    Surface Laptop 15‚Äù  7th Edition  Processor        Snapdragon¬Æ X Plus   Snapdragon¬Æ X Elite    Snapdragon¬Æ X Elite  Neural Processing Unit (NPU) Qualcomm¬Æ Hexagon‚Ñ¢ with 45 TOPS  Graphics Qualcomm¬Æ Adreno‚Ñ¢ GPU  Memory and Storage1 Memory options:  16GB, 32GB, 64GB ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q2: What is the screen size of Surface Pro 9?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     528.34 ms /   769 tokens (    0.69 ms per token,  1455.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1391.84 ms /    61 runs   (   22.82 ms per token,    43.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    2012.52 ms /   830 tokens\n",
      "llama_perf_context_print:    graphs reused =         58\n",
      "Llama.generate: 59 prefix-match hit, remaining 895 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "The context provided does not include the screen size of Surface Pro 9. However, it does mention that the Surface Pro 9 is compatible with Surface Pro 9. Since the context does not provide the answer to the question, I don't know based on the provided context.\n",
      "\n",
      "üìÑ Sources:\n",
      "   1. Source: Unknown\n",
      "      Snippet: Surface Pro X technical specs Dimensions 11.3‚Äù x 8.2‚Äù x 0.28‚Äù (287 mm x 208 mm x 7.3 mm) Display Screen: 13‚Äù PixelSense‚Ñ¢ Display Resolution: 2880x1920 (267 PPI) Aspect ratio: 3:2 Touch: 10 point multi-touch Memory 8GB or 16GB LPDDR4x RAM Processor Microsoft SQ¬Æ 1 Microsoft SQ¬Æ 2 Security Firmware TP...\n",
      "   2. Source: Unknown\n",
      "      Snippet: Surface Pro Flex Keyboard  Compatibility33 Surface Pro (11th Edition)  Surface Pro 10 For Business  Surface Pro 9  Surface Pro 8  Size and Weight Length: 11.38 inches (289 mm)  Width: 8.71 inches (221 mm)  Height: 0.21 inches (5.25 mm)  Weight: 0.75 lbs (340 g)   Battery life  Up to 41 hours of cont...\n",
      "   3. Source: Unknown\n",
      "      Snippet: Surface Pro Flex Keyboard  Compatibility33 Surface Pro (11th Edition)  Surface Pro 10 For Business  Surface Pro 9  Surface Pro 8  Size and Weight Length: 11.38 inches (289 mm)  Width: 8.71 inches (221 mm)  Height: 0.21 inches (5.25 mm)  Weight: 0.75 lbs (340 g)   Battery life  Up to 41 hours of cont...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q3: What is the maximum RAM and storage for Surface Laptop 7 Copilot+PC Snapdragon?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     588.13 ms /   895 tokens (    0.66 ms per token,  1521.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4721.26 ms /   209 runs   (   22.59 ms per token,    44.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    5710.85 ms /  1104 tokens\n",
      "llama_perf_context_print:    graphs reused =        201\n",
      "Llama.generate: 62 prefix-match hit, remaining 1318 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Based on the provided context, the Surface Laptop 7 Copilot+PC Snapdragon has two different models: one with a 13.8-inch display and another with a 15-inch display.\n",
      "\n",
      "The 13.8-inch model comes with up to 32GB LPDDR5x RAM and up to 1TB Gen 4 SSD storage.\n",
      "\n",
      "On the other hand, the 15-inch model comes with up to 16GB or 32GB LPDDR5x RAM and up to 512GB Gen 4 SSD storage.\n",
      "\n",
      "Therefore, the maximum RAM for Surface Laptop 7 Copilot+PC Snapdragon is 32GB for both models. The maximum storage for the 13.8-inch model is 1TB, while for the 15-inch model, it is 512GB.\n",
      "\n",
      "üìÑ Sources:\n",
      "   1. Source: Unknown\n",
      "      Snippet: Surface Laptop 13.8‚Äù  7th Edition    Surface Laptop 15‚Äù  7th Edition  Processor        Snapdragon¬Æ X Plus   Snapdragon¬Æ X Elite    Snapdragon¬Æ X Elite  Neural Processing Unit (NPU) Qualcomm¬Æ Hexagon‚Ñ¢ with 45 TOPS  Graphics Qualcomm¬Æ Adreno‚Ñ¢ GPU  Memory and Storage1 Memory options:  16GB, 32GB, 64GB ...\n",
      "   2. Source: Unknown\n",
      "      Snippet: Surface Laptop 13.8‚Äù  7th Edition    Surface Laptop 15‚Äù  7th Edition  Processor        Snapdragon¬Æ X Plus   Snapdragon¬Æ X Elite    Snapdragon¬Æ X Elite  Neural Processing Unit (NPU) Qualcomm¬Æ Hexagon‚Ñ¢ with 45 TOPS  Graphics Qualcomm¬Æ Adreno‚Ñ¢ GPU  Memory and Storage1 Memory options:  16GB, 32GB, 64GB ...\n",
      "   3. Source: Unknown\n",
      "      Snippet: Tech specs Processor Surface Laptop 13.8 inch:Snapdragon¬Æ X Plus (10 Core)Snapdragon¬Æ X Elite¬†(12 Core) Surface Laptop 15 inch:Snapdragon¬Æ X Elite¬†(12 Core) NPU Qualcomm¬Æ Hexagon‚Ñ¢ with 45 trillion operations per second Graphics Qualcomm¬Æ Adreno‚Ñ¢ GPU Memory and storage Memory options:16GB or 32GB LPD...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ùì Q4: Which GPU Surface Laptop Studio 2 has?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     488.28 ms\n",
      "llama_perf_context_print: prompt eval time =     932.03 ms /  1318 tokens (    0.71 ms per token,  1414.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1648.09 ms /    71 runs   (   23.21 ms per token,    43.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    2696.92 ms /  1389 tokens\n",
      "llama_perf_context_print:    graphs reused =         68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Answer:\n",
      "\n",
      "Surface Laptop Studio 2 comes with NVIDIA¬Æ GeForce RTX‚Ñ¢ 3060 Laptop GPU. This GPU is built with the latest RT Cores, Tensor Cores, and streaming multiprocessors. Surface Laptop Studio 2 has double the graphics performance than Surface Studio 2.\n",
      "\n",
      "üìÑ Sources:\n",
      "   1. Source: Unknown\n",
      "      Snippet: Surface Laptop Studio 2  Fact sheet | September 2023    Meet Surface Laptop Studio 2, a laptop like no other.   Surface Laptop Studio 2  brings together the versatility to create and the power to perform ,  combining cutting-edge design with incredible performance to power the most demanding apps.  ...\n",
      "   2. Source: Unknown\n",
      "      Snippet: Surface Studio 2+  Fact Sheet  October 2022    Meet Surface Studio 2+, Stand-out design, fluid productivity   Find fuel for inspiration with professional-grade performance on a sleek, versatile all-in-one device that  commands attention. Dive into brilliant color, blazing-fast graphics, and immersiv...\n",
      "   3. Source: Unknown\n",
      "      Snippet: Sign in Animation On SurfaceComputersÓúç Computers for BusinessÓúç AccessoriesÓúç Shop nowÓúç Support SHOP DEALS All MicrosoftÓúç Search Óú° CartÓûø ÓûøPay your way, with flexible  payment options at checkout . Learn moreÓù¨ Surface Laptop Studio 2 Versatility to create, power to perform Build your device Óúæ Platinum ...\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define multiple questions per category\n",
    "sample_queries = {\n",
    "    #\"CV\": \n",
    "    #[\n",
    "    #    \"Who has worked in both American Express and Microsoft?\"\n",
    "    #],\n",
    "    #\"FINANCIAL\": \n",
    "    #[\n",
    "    #    \"What is the total dividend income in August 2025?\",\n",
    "    #    \"What was the Debit in DoubleTree by Hilton in Bangalore?\"\n",
    "    #],\n",
    "    #\"REIMBURSEMENT\": \n",
    "    #[\n",
    "    #    \"What documents are required for travel reimbursement?\"\n",
    "    #],\n",
    "    \"SPECS\": \n",
    "    [\n",
    "        \"Suggest a laptop which comes with a dedicated GPU not intergrated GPU. Give me the name of the laptop with GPU name as well.\",\n",
    "        \"What is the screen size of Surface Pro 9?\",\n",
    "        \"What is the maximum RAM and storage for Surface Laptop 7 Copilot+PC Snapdragon?\",\n",
    "        \"Which GPU Surface Laptop Studio 2 has?\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# --- Run all queries and show answers + sources ---\n",
    "for category, questions in sample_queries.items():\n",
    "    print(f\"\\nüóÇÔ∏è Category: {category}\")\n",
    "    qa_chain = rag_chains[category]\n",
    "\n",
    "    for i, question in enumerate(questions, start=1):\n",
    "        print(f\"\\n‚ùì Q{i}: {question}\")\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        print(f\"üß† Answer:\\n{result['result']}\\n\")\n",
    "\n",
    "        # Show source documents used for this answer\n",
    "        print(\"üìÑ Sources:\")\n",
    "        for j, doc in enumerate(result.get(\"source_documents\", []), start=1):\n",
    "            print(f\"   {j}. Source: {getattr(doc.metadata, 'source', 'Unknown')}\")\n",
    "            snippet = doc.page_content[:300].replace(\"\\n\", \" \")\n",
    "            print(f\"      Snippet: {snippet}...\")\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b5073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_gpu)",
   "language": "python",
   "name": "rag_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
