{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f872935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps to install llama-cpp-python on Windows with CUDA support\n",
    "# 1. Install Visual Studio 2022 with \"Desktop development with C++\" workload\n",
    "# 2. Install CUDA Toolkit from NVIDIA's website if not already installed\n",
    "# 3. Install latest version of cmake\n",
    "# 4. Open x64 Native Tools Command Prompt for VS 2022\n",
    "# 5. Run the following command:\n",
    "    # pip install llama-index\n",
    "    # pip install llama-index-llms-llama-cpp\n",
    "    # pip install sentence-transformers\n",
    "    # pip install chromadb\n",
    "    # pip install faiss-cpu\n",
    "    # pip install pypdf\n",
    "    # pip install docx2txt\n",
    "    # pip install python-pptx\n",
    "    # set FORCE_CMAKE=1\n",
    "    # set CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89\n",
    "    # pip install llama-cpp-python --force-reinstall --no-cache-dir --verbose --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125\n",
    "# 6. Verify installation by running python shell and executing:\n",
    "    # from llama_cpp import Llama\n",
    "    # llm = Llama(model_path=r\"C:\\Users\\moidhassan\\Downloads\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", n_gpu_layers=-1)\n",
    "# 7. If you encounter issues, refer to the GitHub repository: https://github.com/abetlen/llama-cpp-python\n",
    "# Note: Adjust the model_path to point to your downloaded GGUF model file.\n",
    "# 8. For CUDA support, ensure your GPU is compatible and the correct CUDA version is installed.\n",
    "# 9. Test with a simple script to ensure everything is working fine.\n",
    "# Note: Some installations may require restarting the terminal or IDE to recognize new environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "325f3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Imports & Configuration\n",
    "# All necessary libraries and configuration constants.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "from textwrap import dedent\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.llama_cpp import LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdb026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:09:31 - Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Constants ---\n",
    "\n",
    "# 1. Paths\n",
    "# Adjust these paths to match your folder structure.\n",
    "CV_PATH = \"../data/CV\"\n",
    "FINANCIAL_PATH = \"../data/financial\"\n",
    "SPECS_PATH = \"../data/specs\"\n",
    "REIMBURSEMENT_PATH = \"../data/reimbursement\"\n",
    "DATA_PATHS_MAP = {\n",
    "    \"CV\": CV_PATH,\n",
    "    \"FINANCIAL\": FINANCIAL_PATH,\n",
    "    \"SPECS\": SPECS_PATH,\n",
    "    \"REIMBURSEMENT\": REIMBURSEMENT_PATH\n",
    "}\n",
    "MODEL_PATH = \"./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "CHROMA_DB_PATH = \"./chroma_db_v2\"\n",
    "COLLECTION_NAME = \"local_rag_demo_v2\"\n",
    "\n",
    "# 2. Embedding Model\n",
    "# \"all-MiniLM-L6-v2\" is small (~90MB), fast, and high-quality.\n",
    "EMBED_MODEL_NAME_1 = \"all-MiniLM-L6-v2\"\n",
    "EMBED_MODEL_NAME_2 = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# 3. Chunking Parameters\n",
    "# Using a semantic splitter to keep sentences intact.\n",
    "CHUNK_SIZE = 1024      # Target size of each chunk (in characters)\n",
    "CHUNK_OVERLAP = 192     # Overlap between chunks to maintain context\n",
    "\n",
    "# 4. LLM Parameters\n",
    "# These are settings for the Mistral 7B model.\n",
    "LLM_N_CTX = 8192*3       # Context window size (Mistral has a large one)\n",
    "LLM_N_GPU_LAYERS = 40  # Layers to offload to GPU (adjust based on your VRAM)\n",
    "LLM_TEMPERATURE = 0.2  # Low temperature for factual, less \"creative\" answers\n",
    "LLM_MAX_NEW_TOKENS = 1024 # Max tokens to generate in an answer\n",
    "\n",
    "# 5. Retrieval Parameters\n",
    "TOP_K_RESULTS = 3      # Number of context chunks to retrieve\n",
    "\n",
    "# 6. Helper Function\n",
    "def log(message):\n",
    "    \"\"\"Helper function for formatted logging.\"\"\"\n",
    "    print(f\"[INFO] {time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\")\n",
    "\n",
    "log(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53d4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.1: Load Documents\n",
    "# **Goal:** Load documents from each directory path separately and group them. This allows us to create a dedicated index for \"CVs\", \"Financials\", etc. (Multi-Index RAG).\n",
    "\n",
    "# %%\n",
    "def load_documents(data_paths_map):\n",
    "    \"\"\"\n",
    "    Loads documents from multiple directories and groups them by source name.\n",
    "    \n",
    "    Args:\n",
    "        data_paths_map (dict): A dictionary mapping index names (e.g., \"CV\")\n",
    "                               to their directory paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the index names and values are\n",
    "              lists of LlamaIndex Document objects.\n",
    "    \"\"\"\n",
    "    log(\"Starting to load documents for multiple indexes...\")\n",
    "    grouped_documents = {}\n",
    "\n",
    "    # Iterate through the map: (Index Name) -> (Directory Path)\n",
    "    for name, path in data_paths_map.items():\n",
    "        if not Path(path).exists():\n",
    "            log(f\"Warning: Path for '{name}' does not exist, skipping: {path}\")\n",
    "            continue\n",
    "\n",
    "        # SimpleDirectoryReader reads all files in the directory\n",
    "        reader = SimpleDirectoryReader(path, recursive=True)\n",
    "        try:\n",
    "            documents = reader.load_data()\n",
    "            grouped_documents[name] = documents\n",
    "            log(f\"Successfully loaded {len(documents)} documents for index: '{name}'\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error loading data from {path} for '{name}': {e}\")\n",
    "\n",
    "    total_docs = sum(len(docs) for docs in grouped_documents.values())\n",
    "    log(f\"‚úÖ Total documents loaded for {len(grouped_documents)} indexes: {total_docs}\")\n",
    "\n",
    "    if not grouped_documents:\n",
    "        log(\"Error: No documents were loaded. Please check your DATA_PATHS_MAP.\")\n",
    "\n",
    "    return grouped_documents\n",
    "    \"\"\"\n",
    "    log(\"Starting to load documents for multiple indexes...\")\n",
    "    grouped_documents = {}\n",
    "    \n",
    "    # Iterate through the map: (Index Name) -> (Directory Path)\n",
    "    for name, path in data_paths_map.items():\n",
    "        if not Path(path).exists():\n",
    "            log(f\"Warning: Path for '{name}' does not exist, skipping: {path}\")\n",
    "            continue\n",
    "        \n",
    "        # SimpleDirectoryReader reads all files in the directory\n",
    "        reader = SimpleDirectoryReader(path, recursive=True)\n",
    "        try:\n",
    "            documents = reader.load_data()\n",
    "            grouped_documents[name] = documents\n",
    "            log(f\"Successfully loaded {len(documents)} documents for index: '{name}'\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error loading data from {path} for '{name}': {e}\")\n",
    "            \n",
    "    total_docs = sum(len(docs) for docs in grouped_documents.values())\n",
    "    log(f\"‚úÖ Total documents loaded for {len(grouped_documents)} indexes: {total_docs}\")\n",
    "    \n",
    "    if not grouped_documents:\n",
    "        log(\"Error: No documents were loaded. Please check your DATA_PATHS_MAP.\")\n",
    "        \n",
    "    return grouped_documents\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485638f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:09:33 - Starting to load documents for multiple indexes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 13:09:43,325 - WARNING - Ignoring wrong pointing object 41 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:09:47 - Successfully loaded 51 documents for index: 'CV'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 13:10:18,991 - WARNING - invalid pdf header: b'\\xac\\xed\\x00\\x05u'\n",
      "2025-10-29 13:10:18,999 - WARNING - incorrect startxref pointer(1)\n",
      "2025-10-29 13:10:19,061 - WARNING - parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:10:27 - Successfully loaded 98 documents for index: 'FINANCIAL'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 13:10:52,900 - WARNING - Ignoring wrong pointing object 41 0 (offset 0)\n",
      "2025-10-29 13:11:20,177 - WARNING - parsing for Object Streams\n",
      "2025-10-29 13:11:20,656 - WARNING - parsing for Object Streams\n",
      "2025-10-29 13:11:24,801 - WARNING - parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:11:28 - Successfully loaded 843 documents for index: 'SPECS'\n",
      "[INFO] 2025-10-29 13:11:29 - Successfully loaded 15 documents for index: 'REIMBURSEMENT'\n",
      "[INFO] 2025-10-29 13:11:29 - ‚úÖ Total documents loaded for 4 indexes: 1007\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.1 (Modified for Multi-Index) ---\n",
    "# This will scan your directories and populate the 'grouped_documents' dictionary.\n",
    "grouped_documents = load_documents(DATA_PATHS_MAP)\n",
    "if not grouped_documents:\n",
    "    log(\"Error: No documents were loaded. Please check your DATA_PATHS_MAP.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eafb456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:18:16 - Index 'CV': Raw files = 37, Loaded documents = 51\n",
      "[INFO] 2025-10-29 13:18:16 - Index 'FINANCIAL': Raw files = 8, Loaded documents = 98\n",
      "[INFO] 2025-10-29 13:18:16 - Index 'SPECS': Raw files = 89, Loaded documents = 843\n",
      "[INFO] 2025-10-29 13:18:16 - Index 'REIMBURSEMENT': Raw files = 10, Loaded documents = 15\n"
     ]
    }
   ],
   "source": [
    "# checking how many documents were loaded per index\n",
    "#for index_name, docs in grouped_documents.items():\n",
    "#    log(f\"Index '{index_name}' has {len(docs)} documents loaded.\")\n",
    "\n",
    "# checking how many documents were split into multiple chunks\n",
    "#for index_name, docs in grouped_documents.items():\n",
    "#    chunked_count = sum(1 for doc in docs if len(doc.get_text()) > CHUNK_SIZE)\n",
    "#    log(f\"Index '{index_name}' has {chunked_count} documents that were chunked into multiple pieces.\")\n",
    "\n",
    "# checking how many raw documents were in DATA_PATHS_MAP and how many were loaded into grouped_documents per category\n",
    "for index_name, path in DATA_PATHS_MAP.items():\n",
    "    raw_count = len(list(Path(path).rglob('*.*'))) if Path(path).exists() else 0\n",
    "    loaded_count = len(grouped_documents.get(index_name, []))\n",
    "    log(f\"Index '{index_name}': Raw files = {raw_count}, Loaded documents = {loaded_count}\")\n",
    "    #chunked_count = sum(1 for doc in grouped_documents.get(index_name, []) if len(doc.text) > CHUNK_SIZE)\n",
    "    #log(f\"Index '{index_name}': Chunked documents = {chunked_count}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741ff14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:18:19 - Sample document from index 'CV':\n",
      "ABHISHEK  RANJAN                  +91  9040140733   \n",
      " Data  Scientist  |  AI  Engineer                     13eee079@gmail.com  \n",
      "\n",
      "ABOUT  ME  \n",
      "Data  Scientist  with  3.9   years  of  experience,  with  expertise  in  Machine  Learning,  Deep  Learning,  Generative  AI,  NLP,  \n",
      "LLMs,\n",
      "\n",
      "and\n",
      "\n",
      "RAG.\n",
      "\n",
      "Skilled\n",
      "\n",
      "in\n",
      "\n",
      "fine-tuning\n",
      "\n",
      "Hugging\n",
      "\n",
      "Face\n",
      "\n",
      "models,\n",
      "\n",
      "building\n",
      "\n",
      "AI-driven\n",
      "\n",
      "solutions,\n",
      "\n",
      "and\n",
      "\n",
      "deploying\n",
      "\n",
      "scalable\n",
      "\n",
      "architectures\n",
      "\n",
      "using\n",
      "\n",
      "LangChain\n",
      "\n",
      "and\n",
      "\n",
      "OpenAI\n",
      "\n",
      "API.\n",
      "\n",
      "Passionate\n",
      "\n",
      "about\n",
      "\n",
      "AI...\n",
      "\n",
      "\n",
      "\n",
      "[INFO] 2025-10-29 13:18:19 - Sample document from index 'FINANCIAL':\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "[INFO] 2025-10-29 13:18:19 - Sample document from index 'SPECS':\n",
      "Surface USB-C¬Æ Travel Hub\n",
      "All the connections, wherever you are\n",
      "Pitch\n",
      "Turn your laptop into an on-the-go productivity companion with \n",
      "this elegant, multi-port travel adapter. Designed for on-the-go \n",
      "professionals, it gives you five ways to stay productive anywhere. \n",
      "Connect to the internet, project content onto a big screen, charge \n",
      "accessories, and more. Compatible with modern PCs and all \n",
      "Surface laptops with a USB-C¬Æ port. Supports compatible \n",
      "accessory charging. Does not support laptop charg...\n",
      "\n",
      "\n",
      "\n",
      "[INFO] 2025-10-29 13:18:19 - Sample document from index 'REIMBURSEMENT':\n",
      "moid hassan\n",
      "RD17471298637220809\n",
      "Ramaswamy\n",
      "KA04AA9727\n",
      "Car\n",
      "May 13th 2025, 3:25 PM\n",
      "Booking History\n",
      "Customer Name\n",
      "Ride ID\n",
      "Driver name\n",
      "Vehicle Number\n",
      "Mode of Vehicle\n",
      "Time of Ride\n",
      "Selected Price\n",
      "‚Çπ  1181\n",
      "Cluster_bellanduru 6 Lpg Gas Station, 3, Outer Ring Rd, near Citrus Hotel, Bellandur,\n",
      "Bengaluru, Karnataka 560103, India\n",
      "This document is issued on request by the passenger. Rapido does not collect any fee/commission from passengers and shall not issue tax\n",
      "invoices to the passengers under this segment....\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a snippet from one of the first documents loaded\n",
    "for index_name, docs in grouped_documents.items():\n",
    "    if docs:\n",
    "        log(f\"Sample document from index '{index_name}':\\n{dedent(docs[0].text[:500])}...\\n\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edacc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.2: Load Embedding Model\n",
    "# We define the function to load the `sentence-transformer` model and then call it.\n",
    "\n",
    "# %%\n",
    "def load_embedding_model(model_name, cuda_flag=False):\n",
    "    \"\"\"\n",
    "    Loads the SentenceTransformer embedding model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model from Hugging Face.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    log(f\"Loading embedding model: {model_name}...\")\n",
    "    # The first time you run this, it will download the model.\n",
    "    if cuda_flag:\n",
    "        log(\"Using CUDA for embedding model.\")\n",
    "        embed_model = SentenceTransformer(model_name, device='cuda')\n",
    "    else:\n",
    "        embed_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Example: Test the model\n",
    "    sample_vec = embed_model.encode(\"This is a test sentence.\")\n",
    "    log(f\"‚úÖ Embedding model loaded. Vector size: {len(sample_vec)}\")\n",
    "    return embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b58694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 13:18:24,428 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: BAAI/bge-small-en-v1.5...\n",
      "[INFO] 2025-10-29 13:18:24 - Loading embedding model: BAAI/bge-small-en-v1.5...\n",
      "[INFO] 2025-10-29 13:18:24 - Using CUDA for embedding model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:18:29 - ‚úÖ Embedding model loaded. Vector size: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.2 ---\n",
    "# This will download the model if you don't have it cached.\n",
    "print(f\"Loading embedding model: {EMBED_MODEL_NAME_2}...\")\n",
    "embed_model = load_embedding_model(EMBED_MODEL_NAME_2, cuda_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb7c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.3: Chunk and Embed Documents\n",
    "# **Goal:** Convert raw documents into semantically meaningful chunks and generate high-quality embedding vectors for each chunk using the loaded model.\n",
    "\n",
    "# %%\n",
    "def chunk_and_embed(documents, embed_model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Chunks documents and generates embeddings for each chunk.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of LlamaIndex Document objects for a single index group.\n",
    "        embed_model (SentenceTransformer): The embedding model.\n",
    "        batch_size (int): Number of chunks to process in each embedding batch.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated IDs, chunk text, and embeddings \n",
    "              for the input documents.\n",
    "    \"\"\"\n",
    "    log(\"Starting document chunking and embedding...\")\n",
    "    \n",
    "    if not documents:\n",
    "        log(\"No documents to chunk. Skipping...\")\n",
    "        return {'ids': [], 'chunks': [], 'embeddings': []}\n",
    "\n",
    "    # This is the correct, semantic way to chunk text.\n",
    "    # It splits on sentences and respects word boundaries.\n",
    "    text_splitter = SentenceSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    \n",
    "    # LlamaIndex's SentenceSplitter returns \"Node\" objects.\n",
    "    nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    log(f\"Split {len(documents)} source documents into {len(nodes)} chunks.\")\n",
    "    \n",
    "    log(\"Generating embeddings for all chunks...\")\n",
    "    # 1. Get all the text content first\n",
    "    all_chunk_text = [node.get_content() for node in nodes]\n",
    "    \n",
    "    # 2. Generate embeddings in a single, efficient batch\n",
    "    # This is *much* faster than embedding chunks one by one.\n",
    "    embeddings = embed_model.encode(all_chunk_text, show_progress_bar=True, batch_size=batch_size)\n",
    "    \n",
    "    # 3. Create the IDs list\n",
    "    ids = [f\"chunk_{i}\" for i in range(len(nodes))]\n",
    "    \n",
    "    log(f\"‚úÖ Generated {len(embeddings)} embeddings.\")\n",
    "    return {\n",
    "        'ids': ids, \n",
    "        'chunks': all_chunk_text, \n",
    "        'embeddings': embeddings.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a107a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:23:10 - Processing group: CV (51 documents)\n",
      "[INFO] 2025-10-29 13:23:10 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 13:23:10 - Split 51 source documents into 61 chunks.\n",
      "[INFO] 2025-10-29 13:23:10 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:23:11 - ‚úÖ Generated 61 embeddings.\n",
      "[INFO] 2025-10-29 13:23:11 - Processing group: FINANCIAL (98 documents)\n",
      "[INFO] 2025-10-29 13:23:11 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 13:23:11 - Split 98 source documents into 101 chunks.\n",
      "[INFO] 2025-10-29 13:23:11 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:23:12 - ‚úÖ Generated 101 embeddings.\n",
      "[INFO] 2025-10-29 13:23:12 - Processing group: SPECS (843 documents)\n",
      "[INFO] 2025-10-29 13:23:12 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 13:23:27 - Split 843 source documents into 2665 chunks.\n",
      "[INFO] 2025-10-29 13:23:27 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:26<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:23:53 - ‚úÖ Generated 2665 embeddings.\n",
      "[INFO] 2025-10-29 13:23:53 - Processing group: REIMBURSEMENT (15 documents)\n",
      "[INFO] 2025-10-29 13:23:53 - Starting document chunking and embedding...\n",
      "[INFO] 2025-10-29 13:23:53 - Split 15 source documents into 15 chunks.\n",
      "[INFO] 2025-10-29 13:23:53 - Generating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-10-29 13:23:53 - ‚úÖ Generated 15 embeddings.\n",
      "[INFO] 2025-10-29 13:23:53 - ‚úÖ Completed chunking and embedding for 4 groups.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Run Step 4.3 (Modified for Multi-Index) ---\n",
    "# This will chunk and embed the documents for all groups.\n",
    "# The results are stored in a dictionary matching the group names.\n",
    "embedded_groups = {}\n",
    "batch_size = 128  # You can adjust this based on your GPU memory\n",
    "\n",
    "for name, docs in grouped_documents.items():\n",
    "    log(f\"Processing group: {name} ({len(docs)} documents)\")\n",
    "    \n",
    "    # We call the function and get the processed data back\n",
    "    processed_data = chunk_and_embed(docs, embed_model, batch_size)\n",
    "    \n",
    "    embedded_groups[name] = processed_data\n",
    "\n",
    "log(f\"‚úÖ Completed chunking and embedding for {len(embedded_groups)} groups.\")\n",
    "# You can now inspect 'embedded_groups' to see IDs, chunks, and embeddings for each index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adeeab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 13:44:35,651 - INFO - Loading faiss with AVX2 support.\n",
      "2025-10-29 13:44:35,745 - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Step 4.4: Initialize Vector Store (Modified for Multi-Index)\n",
    "# **Goal:** Initialize and populate a FAISS index for each document group. Uses GPU acceleration automatically if available.\n",
    "\n",
    "# %%\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def initialize_vector_stores(embedded_groups):\n",
    "    \"\"\"\n",
    "    Initializes and populates a FAISS index for each document group.\n",
    "    Uses GPU acceleration automatically if available.\n",
    "\n",
    "    Args:\n",
    "        embedded_groups (dict): Dictionary from embedding step, containing:\n",
    "            - ids: list of unique IDs\n",
    "            - chunks: list of document text chunks\n",
    "            - embeddings: list or np.array of vector embeddings\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping group names to FAISS index + metadata:\n",
    "              {\n",
    "                group_name: {\n",
    "                    \"index\": faiss.Index,\n",
    "                    \"ids\": [...],\n",
    "                    \"chunks\": [...]\n",
    "                }\n",
    "              }\n",
    "    \"\"\"\n",
    "    log(\"üîç Initializing FAISS Vector Stores...\")\n",
    "    all_collections = {}\n",
    "\n",
    "    # Detect GPU availability\n",
    "    use_gpu = False\n",
    "    try:\n",
    "        ngpu = faiss.get_num_gpus()\n",
    "        if ngpu > 0:\n",
    "            use_gpu = True\n",
    "            log(f\"‚ö° Detected {ngpu} GPU(s) ‚Äî will use FAISS GPU acceleration.\")\n",
    "        else:\n",
    "            log(\"üí° No GPU FAISS available ‚Äî running on CPU.\")\n",
    "    except Exception as e:\n",
    "        log(f\"‚ö†Ô∏è GPU detection failed, defaulting to CPU: {e}\")\n",
    "\n",
    "    for name, data in embedded_groups.items():\n",
    "        ids = data.get(\"ids\", [])\n",
    "        document_chunks = data.get(\"chunks\", [])\n",
    "        embeddings = data.get(\"embeddings\", [])\n",
    "\n",
    "        collection_name = f\"rag_docs_{name.lower()}\"\n",
    "        log(f\"Creating FAISS index for '{collection_name}'\")\n",
    "\n",
    "        if not ids or len(ids) != len(embeddings):\n",
    "            log(f\"‚ö†Ô∏è Skipping '{collection_name}' (empty or invalid data).\")\n",
    "            continue\n",
    "\n",
    "        # Convert embeddings to float32 numpy\n",
    "        embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "\n",
    "        # Create FAISS index (inner product = cosine similarity)\n",
    "        dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "        # Move to GPU if available\n",
    "        if use_gpu:\n",
    "            try:\n",
    "                index = faiss.index_cpu_to_all_gpus(cpu_index)\n",
    "                log(f\"‚úÖ Using GPU FAISS index for '{collection_name}'\")\n",
    "            except Exception as e:\n",
    "                log(f\"‚ö†Ô∏è GPU transfer failed for '{collection_name}', using CPU instead: {e}\")\n",
    "                index = cpu_index\n",
    "        else:\n",
    "            index = cpu_index\n",
    "\n",
    "        # Add embeddings to index\n",
    "        index.add(embeddings)\n",
    "        log(f\"‚úÖ Stored {len(ids)} vectors in '{collection_name}'\")\n",
    "\n",
    "        # Store index + metadata\n",
    "        all_collections[name] = {\n",
    "            \"index\": index,\n",
    "            \"ids\": ids,\n",
    "            \"chunks\": document_chunks\n",
    "        }\n",
    "\n",
    "    log(\"üéØ All FAISS vector stores initialized successfully.\")\n",
    "    return all_collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78722e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_gpu)",
   "language": "python",
   "name": "rag_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
